<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

        <meta name="description" content="JetBrains AI school club">
        <meta name="author" content="Alex Avdiushenko" />
        <meta name="keywords" content="STEM Education, Artificial Intelligence">

		<title>Attention mechanism and Transformer architecture</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
		<script defer src="https://pyscript.net/latest/pyscript.js"></script>
		<py-config>
			terminal = false
		</py-config>
		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
		</style>
	</head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
				<section>
                    <section>
                        <div class="column">
                            <img src="images/jetbrains_logo_icon_145150.webp" alt="jetbrains_logo" />
                        </div>
                        <div class="column">
                            <h2>Youth AI club</h2>
                            <div class="fragment" style="margin-bottom:20px;">
                                    <div class="typesetting">
                                        <h3>Attention mechanism and Transformer architecture</h3>
                                        <br />
                                        Alex Avdiushenko<br />
                                        December 11, 2024
                                    </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">In the last episode</h3>
                        <div class="fragment" style="text-align: left">
                            <div class="typesetting">
                                <ul>
                                    <li><span class="important">Makemore</span>
                                        on bigrams level</li>
                                    <li>Prior to more robust predictions</li>
                                    <li>Multi layer perceptron</li>
                                </ul>
                            </div>
                        </div>
                    </section>
				</section>
                <section>
                    <section>
                        <h3 style="text-align: left">Disadvantages of vanilla Recurrent Neural Network
                        (reminder)</h3>
                        <p style="text-align: left">We use one hidden vector</p>
                        $$ h_t = f_W(h_{t-1}, x_t) $$
                        <p style="text-align: left">As a function \(f_W\) we set a
                            linear transformation with a non-linear component-wise "sigmoid":</p>
                        $$
                            \begin{align*}
                                h_t &= \tanh ({\color{orange}W_{hh}} h_{t-1} + {\color{orange}W_{xh}} x_t) \\
                                y_t &= {\color{orange}W_{hy}} h_t
                            \end{align*}
                        $$
                        <div class="fragment">
                            <ol>
                                <li>Input and output sequence lengths must match</li>
                                <li>"Reads" the input only from left to right, does not look ahead</li>
                                <li>Therefore, it is not suitable for machine translation, question answering tasks, and others</li>
                            </ol>
                        </div>
                        <aside class="notes">
                            Now, let's remember vanilla recurrent neural network and it's disadvantages.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">RNN for sequence synthesis (seq2seq, reminder)</h3>
                        <p> $X = (x_1, \dots, x_n)$ — input sequence</p>
                        <p> $Y = (y_1, \dots, y_m)$ — output sequence</p>
                        <p>\(\color{green}c \equiv h_n\) encodes all information about \(X\) to synthesize \(Y\)</p>
                        <div class="column">
                            <div style="text-align:center">
                                <img src="../ml_fundament/images/seq2seq.png" alt="Sequence to Sequence Model" style="width:50vw; object-fit:contain;">
                            </div>
                        </div>
                        <div class="column">
                            $$
                                \begin{align*}
                                    h_i &= f_{in}(x_i, h_{i-1}) \\
                                    {\color{green}h_t^\prime} &{\color{green}= f_{out}(h_{t-1}^\prime,y_{t-1},c)} \\
                                    y_t &= f_{y}(h_t^\prime, y_{t-1})
                                \end{align*}
                            $$
                        </div>
                        <aside class="notes">
                            Ok, next step. How can we solve these problems? For example, we can use seq2seq architecture.
                        </aside>
                    </section>
                    <section>
                        <div style="text-align:center;">
                        $$
                            \begin{align*}
                                h_i &= f_{in}(x_i, h_{i-1}) \\
                                {\color{orange}h_t^\prime} &= f_{out}(h_{t-1}^\prime, y_{t-1}, c) \\
                                y_t &= f_{y}(h_t^\prime, y_{t-1})
                            \end{align*}
                        $$
                        </div>
                        <div class="fragment">
                            <h4 style="text-align: left">Disadvantages</h4>
                            <ul>
                                <li>${\color{green}c}$ remembers the end ($h_n$) better than the start</li>
                                <li>The more $n$, the more difficult to pack all the information into vector ${\color{green}c}$</li>
                                <li>We should control the vanishing and explosions of the gradient</li>
                                <li>RNN is difficult to parallelize</li>
                            </ul>
                        </div>
                        <div class="fragment">
                            <div class="r-frame important"
                                 style="border: 1px solid #000; padding: 10px;text-align: left">
                                Question: How can you fix some of the problems above?
                            </div>
                        </div>
                        <div class="fragment">
                            <div style="border: 1px solid #00A; padding: 10px;
                            background-color: #E0F7FA;text-align: left">
                                Hint: How do people perceive information?
                            </div>
                        </div>
                        <aside class="notes">
                            Now, as is often the case, we have new, more challenging problems with seq2seq architecture.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align:center">Let's count the number of passes</h3>
                        <div style="text-align:center;">
                            <a href="https://www.youtube.com/watch?v=vJG698U2Mvo" target="_blank">Basketball!</a>
                        </div>
                        <aside class="notes">
                            Now let's have some fun and count the number of passes in the video.
                        </aside>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">RNN with an attention mechanism</h3>
                        <p style="text-align: left">\(a(h, h^\prime)\) is the similarity function of input \(h\) and output \(h^\prime\)
                            (for example, dot product or \(\exp(h^T h^\prime)\) and others)</p>
                        <p style="text-align: left">
                            \(\alpha_{ti}\) — importance of input \(i\) for output \(t\) (attention score), \(\sum\limits_i \alpha_{ti} = 1\)</p>
                        <p style="text-align: left">\(c_t\) — input context vector for output \(t\)</p>
                        <div style="text-align:center;">
                            <img src="images/seq2seq_attention.png" alt="seq2seq with attention" style="width:50%;">
                        </div>
                        <aside class="notes">
                            So, how can we use this mechanism for improving our network architecture?
                            We use function $a$, it is the similarity function of two vectors. And so on...
                        </aside>
                    </section>
                    <section>
                        $$
                            \begin{align*}
                                h_i &= f_{in}(x_i, h_{i-1}) \\
                                {\color{green}\alpha_{ti}} &= \text{norm}_i \, a(h_i, h^\prime_{t-1}), \, \text{ norm}_i(p) = \frac{p_i}{\sum\limits_j p_j} \\
                                {\color{green}c_t} &= \sum_i \alpha_{ti} h_i \\
                                h_t^\prime &= f_{out}(h^\prime_{t-1}, y_{t-1}, {\color{green}c_t}) \\
                                y_t &= f_{y}(h_t^\prime, y_{t-1}, {\color{green}c_t})
                            \end{align*}
                        $$
                        <div class="fragment">
                            <ul>
                                <li>You can enter learnable parameters in \({\color{green}\alpha}\) and \({\color{green}c_t}\)</li>
                                <li>It is possible to refuse recurrence both in \(h_i\) and in \(h_t^\prime\)</li>
                            </ul>
                        </div>
                        <div class="fragment">
                            <hr style="border: 0.4pt solid black;">
                            <p style="text-align: left;">
                                <em>Bahdanau et al.</em> Neural machine translation by jointly learning to align and translate. 2015
                            </p>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">The Main Areas for Attention Models</h3>
                        <p style="text-align: left">Converting one sequence to another, i.e. seq2seq:</p>
                        <ul class="fragment">
                            <li>Machine translation</li>
                            <li>Question answering</li>
                            <li>Text summarization</li>
                            <li>Annotation of images, videos (multimedia description)</li>
                            <li>Speech recognition</li>
                            <li>Speech synthesis</li>
                        </ul>
                        <div class="fragment" style="margin-top: 0.5cm;">
                            <p style="text-align: left">Sequence processing:</p>
                            <ul>
                                <li>Classification of text documents</li>
                                <li>Document Sentiment Analysis</li>
                            </ul>
                        </div>
                        <aside class="notes">
                            Attention models allow us to solve a lot of problems from different areas.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Attention in machine translation</h3>
                        <div style="text-align:center;">
                            <img src="images/eng_to_french.png" alt="English to French Translation with Attention" style="width:50%;">
                            <p>Attention Model Interpretability: Visualization of \(\alpha_{ti}\)</p>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Attention for annotating images</h3>
                        <div style="text-align:center;">
                            <img src="images/image_attention.png" alt="Image Attention" style="width:80%;">
                        </div>
                        <p style="text-align: left">When generating a word for an image description,
                            the visualization shows which areas of the image the model pays attention to</p>
                        <hr style="border: 0.4pt solid black;">
                        <div style="text-align: left;">
                            <em>Kelvin Xu et al.</em> Show, attend and tell: neural image caption generation with visual attention. 2016
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Vector Similarity Functions:</h3>
                        <p style="text-align: left">\(a(h, h^\prime) = h^T h^\prime\) is the scalar (inner) product</p>
                        <p class="fragment" style="text-align: left">\(a(h, h^\prime) = \exp(h^T h^\prime)\) — with
                            <code>norm</code> becomes <code>SoftMax</code></p>
                        <p class="fragment" style="text-align: left">\(a(h, h^\prime) = h^T {\color{orange}W} h^\prime\) — with the learning parameter matrix \(\color{orange}{W}\)</p>
                        <p class="fragment" style="text-align: left">\(a(h, h^\prime) = {\color{orange}w^T} \tanh ({\color{orange}U}h +
                            {\color{orange}V} h^\prime)\) is additive attention with \({\color{orange}w, U, V}\)</p>
                    </section>
                    <section>
                        <p style="margin-top: 0.5cm;text-align: left">
                            <strong>Linear vector transformations:</strong> query, key, value
                        </p>
                        <div class="column" style="width: 60%;">
                            <p style="text-align: left">\(a(h_i, h^\prime_{t-1}) = ({\color{orange}W_k}h_i)^T ({\color{orange}W_q}h^\prime_{t-1}) / \sqrt{d}\)</p>
                            <p style="text-align: left">\(\alpha_{ti} = \text{SoftMax}_i a(h_i, h^\prime_{t-1})\)</p>
                            <p style="text-align: left">\(c_t = \sum\limits_i \alpha_{ti} {\color{orange}W_v} h_i\)</p>
                            <p style="text-align: left">\({\color{orange}W_q}_{d \times dim(h^\prime)},
                                {\color{orange}W_k}_{d \times dim(h)},
                                {\color{orange}W_v} _{d \times dim(h)}\) — linear neuron weight matrices,</p>
                            <p style="text-align: left">possible simplification: \({\color{orange}W_k} \equiv {\color{orange}W_v}\)</p>
                            <br>
                            <hr style="border: 0.4pt solid black;">
                            <div style="text-align: left">
                                <em>Dichao Hu.</em> An introductory survey on attention mechanisms in NLP problems. 2018
                            </div>
                        </div>
                        <div class="column" style="width: 22%;text-align: right">
                            <img src="images/general_attention.png" alt="General Attention" style="width: 100%;">
                        </div>
                        <aside class="notes">
                            Let's discuss in detail the math inside attention.
                            The first one is Similarity Functions of two vectors.
                            Of course in the simplest case it could be the scalar (inner) product.
                            Also, you could use the trainable parameters and our favorite linear transformation layer
                            with non-linear sigmoid function and weight vector.
                            But the most common procedure is to have query, key, value parameters matrices.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Attention Formula</h3>
                        <p style="text-align: left">\(q\) is the query vector for which we want to calculate the context</p>
                        <p style="text-align: left">\(K = (k_1, \dots, k_n)\) — key vectors compared with the query</p>
                        <p style="text-align: left">\(V = (v_1, \dots, v_n)\) — value vectors forming the context</p>
                        <p style="text-align: left">\(a(k_i, q)\) — score of relevance (similarity) of key \(k_i\) to query \(q\)</p>
                        <p style="text-align: left">\(c\) is the desired context vector relevant to the query</p>
                        <div class="fragment">
                            <div style="border: 1px solid #000; padding: 10px;">
                                <strong>Attention Model</strong><br/>
                                <p style="text-align: left;">
                                    This is a 3-layer network that computes a convex combination of \(v_i\) values relevant to the query \(q\):</p>
                                \[ c = \text{Attn}(q,K,V) = \sum\limits_i v_i \, \text{SoftMax}_i \, a(k_i, q) \]
                            </div>
                        </div>
                        <aside class="notes">
                            In general case we could say that it is three-layer network,
                            and an attention mechanism chooses the best way to store information from the input to the one output vector \(c\).
                        </aside>
                    </section>
                    <section>
                            <p style="text-align: left">
                                \(c_t = \text{Attn}({\color{orange}W_q} h^\prime_{t-1},
                                {\color{orange}W_k} H, {\color{orange}W_v} H)\) is the example from the previous slide,
                                where \(H = (h_1, \dots, h_n)\) come from <span class="important">encoder</span>,
                                \(h^\prime_{t-1}\) comes from <span class="important">decoder</span></p>
                        <br>
                        <div class="fragment">
                            <p style="text-align: left"><strong>Self-attention:</strong></p>
                            <p>\(c_i = \text{Attn}({\color{orange}W_q} h_{i}, {\color{orange}W_k} H, {\color{orange}W_v} H)\)
                                is a special case when \(h^\prime \in H\)</p>
                        </div>
                        <aside class="notes">
                            And one important case needs to be highlighted here. It is called self-attention.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Multi-Head Attention</h3>
                        <p style="text-align: left"><strong>Idea:</strong>
                            \(J\) different attention models are jointly trained to highlight various aspects of the input information
                            (for example, parts of speech, syntax, idioms):</p>
                        <p style="text-align: left">\( c^j = \text{Attn}({\color{orange}W^j_q} q,
                            {\color{orange}W^j_k} H, {\color{orange}W^j_v} H), j = 1, \dots, J\)</p>
                        <div class="fragment">
                            <p style="text-align: left"><strong>Variants</strong> of aggregating the output vector:</p>
                            <ul>
                                <li>\( c = \frac{1}{J} \sum\limits_{j=1}^J c^j\) — averaging</li>
                                <li>\( c = [c^1 \dots c^J]\) — concatenation</li>
                                <li>\( c = [c^1 \dots c^J] {\color{orange}W}\) — to return to the desired dimension</li>
                            </ul>
                        </div>
                        <aside class="notes">
                            Ok, and the last generalization in this lecture — is multi-head attention.
                            Here you do all the same, but having \(J\) different attention models :)
                        </aside>
                    </section>
                </section>
                <section>
                    <section data-background-image="images/many_transformers.png">
                    </section>
                    <section>
                        <h3 style="text-align: left">Transformer for Machine Translation</h3>
                        <p style="text-align: left">It is a neural network architecture based on
                            attention and fully connected layers, <strong>without RNN.</strong>
                        Scheme of transformations in machine translation:</p>
                        <div class="fragment">
                            <ul>
                                <li>\(S = (w_1, \dots, w_n)\) — sentence from words in the input language</li>
                                <p style="text-align: left;font-size: medium">\(\downarrow\) trainable or pre-trained word vectorization \(\downarrow\)</p>
                                <li>\(X = (x_1, \dots, x_n)\) — embeddings of input sentence words</li>
                                <p style="text-align: left;font-size: medium">\(\downarrow\) encoder \(\downarrow\)</p>
                                <li>\(Z = (z_1, \dots, z_n)\) — contextual embeddings</li>
                                <p style="text-align: left;font-size: medium">\(\downarrow\) decoder \(\downarrow\)</p>
                                <li>\(Y = (y_1, \dots, y_m)\) — embeddings of output sentence words</li>
                                <p style="text-align: left;font-size: medium">\(\downarrow\) generation of words from the constructed language model \(\downarrow\)</p>
                                <li>\(\tilde S = (\tilde w_1, \dots, \tilde w_m)\) — sentence words in target language</li>
                            </ul>
                        </div>
                        <hr style="border: 0.4pt solid black;">
                        <div style="text-align: left;">
                            <em>Vaswani et al.</em> (Google) Attention is all you need. 2017
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Architecture of Transformer: Encoder</h3>
                        <div class="column" style="width: 20%;">
                            <img src="images/transformer_encoder.png" alt="Transformer Encoder" style="width: 100%;">
                        </div>
                        <div class="column" style="width: 78%;">
                            <ol>
                                <li class="fragment">Position vectors \(p_i\) are added:
                                    <p>
                                        <table style="width:100%;">
                                            <tr>
                                                <td style="width: 80%;">\(h_i = x_i + p_i, \ H = (h_1, \dots, h_n)\)</td>
                                                <td style="width: 20%; font-size: smaller;">
                                                    \(\scriptscriptstyle d = \text{dim } x_i, p_i, h_i = 512\)<br>
                                                    \(\scriptscriptstyle \text{dim } H = 512 \times n\)
                                                </td>
                                            </tr>
                                        </table>
                                    </p>
                                </li>
                                <div class="fragment">
                                    <li>Multi-head self-attention:
                                        <p>
                                            <table style="width:100%;">
                                                <tr>
                                                    <td style="width: 80%;">\(h_i^j = \text{Attn}({\color{orange}W_q^j} h_{i}, {\color{orange} W_k^j} H, {\color{orange}W_v^j} H)\)</td>
                                                    <td style="width: 20%; font-size: smaller;">
                                                        \(\scriptscriptstyle j = 1, \dots, J = 8\)<br>
                                                        \(\scriptscriptstyle \text{dim } h_i^j = 64\)<br>
                                                        \(\scriptscriptstyle \text{dim } W_q^j, W_k^j, W_v^j = 64 \times 512\)
                                                    </td>
                                                </tr>
                                            </table>
                                        </p>
                                    </li>
                                </div>
                                <div class="fragment">
                                    <li>Concatenation:
                                        <p>
                                            <table style="width:100%;">
                                                <tr>
                                                    <td style="width: 80%;">\(h_i^\prime = MH_j(h_i^j) \equiv [h_i^1 \dots h_i^J]\)</td>
                                                    <td style="width: 20%; font-size: smaller;">
                                                        \(\scriptscriptstyle \text{dim } h_i^\prime = 512\)
                                                    </td>
                                                </tr>
                                            </table>
                                        </p>
                                    </li>
                                </div>
                                <div class="fragment">
                                    <li>Skip-connection and layer normalization:
                                        <p>
                                            <table style="width:100%;">
                                                <tr>
                                                    <td style="width: 80%;">\(h_i^{\prime\prime} = LN(h_i^\prime + h_i; {\color{orange}\mu_1, \sigma_1})\)</td>
                                                    <td style="width: 20%; font-size: smaller;">
                                                        \(\scriptscriptstyle \text{dim } h_i^{\prime\prime}, \mu_1, \sigma_1 = 512\)
                                                    </td>
                                                </tr>
                                            </table>
                                        </p>
                                    </li>
                                </div>
                            </ol>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Architecture of Transformer: Encoder</h3>
                        <div class="column" style="width: 20%;">
                            <img src="images/transformer_encoder.png" alt="Transformer Encoder" style="width: 100%;">
                        </div>
                        <div class="column" style="width: 78%;">
                            <br><br><br><br>
                            <ol start="5">
                                <li>Fully connected two-layer network FFN:
                                    <p>
                                        <table style="width:100%;">
                                            <tr>
                                                <td style="width: 80%;">\(h_i^{\prime\prime\prime} = {\color{orange} W_2} \text{ReLU}({\color{orange} W_1}h_i^{\prime\prime} + {\color{orange} b_1}) + {\color{orange} b_2}\)</td>
                                                <td style="width: 20%; font-size: smaller;">
                                                    \(\scriptscriptstyle \text{dim } W_1 = 2048 \times 512\)<br>
                                                    \(\scriptscriptstyle \text{dim } W_2 = 512 \times 2048\)
                                                </td>
                                            </tr>
                                        </table>
                                    </p>
                                </li>
                                <div class="fragment">
                                    <li>Skip-connection and layer normalization:
                                        <p>
                                            <table style="width:100%;">
                                                <tr>
                                                    <td style="width: 80%;">\(z_i = LN(h_i^{\prime\prime\prime} + h_i^{\prime\prime}; {\color{orange}\mu_2, \sigma_2})\)</td>
                                                    <td style="width: 20%; font-size: smaller;">
                                                        \(\scriptscriptstyle \text{dim } z_i, \mu_2, \sigma_2 = 512\)
                                                    </td>
                                                </tr>
                                            </table>
                                        </p>
                                    </li>
                                </div>
                            </ol>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Additions and Remarks</h3>
                        <ul>
                            <li class="fragment">A lot of such blocks \(N=6, h_i \to \square \to z_i\) are connected in series</li>
                            <li class="fragment">Calculations can be easily parallelized in \(x_i\)</li>
                            <li class="fragment">It is possible to use pre-trained \(x_i\) embeddings</li>
                            <li class="fragment">It is possible to learn embeddings \(x_i\) of words \(w_i \in V\)</li>
                            <li class="fragment">Layer Normalization (LN), \(x, {\color{orange}\mu}, {\color{orange}\sigma} \in \mathbb{R}^d\):</li>
                        </ul>
                        <div class="fragment">
                            $$ LN_s(x; {\color{orange}\mu}, {\color{orange}\sigma}) =
                            {\color{orange}\mu_s} + {\color{orange}\sigma_s} \frac{x_s - \overline{x}}{\sigma_x} $$
                            $$ \overline{x} = \frac{1}{d} \sum\limits_{s=1}^d x_s, \ \sigma_x^2 = \frac{1}{d} \sum\limits_{s=1}^d (x_s - \overline{x})^2 $$
                        </div>
                        <aside class="notes">
                            Layer Normalization is quite easy — you train the mean and variance values of components of your vectors.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Positional Encoding</h3>
                        <p style="text-align: left">The positions of the words \(i\) are encoded by the vectors \(p_i\) so that:</p>
                        <ul>
                            <li>The more \(|i-j|\), the more \(\|p_i-p_j\|\)</li>
                            <li>The number of positions is not limited</li>
                        </ul>
                        <div class="fragment">
                            <p style="margin-top: 1cm;text-align: left">For example,</p>
                            \[ h_j = \text{Attn}(q_j, K, V) =
                            \sum_i (v_i + {\color{orange}w^v_{i\boxminus j}}) \text{SoftMax}_i a(k_i + {\color{orange} w^k_{i\boxminus j}}, q_j), \]
                            <p style="text-align: left">
                                where \(i\boxminus j = \max(\min(i-j, \delta), -\delta)\) is the truncated difference, \(\delta = 5..16\)
                            </p>
                        </div>
                        <br>
                        <hr style="border: 0.4pt solid black;">
                        <div class="fragment" style="text-align: left;">
                            <em>Shaw, Uszkoreit, Vaswani.</em> Self-attention with relative position representations. 2018
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Architecture of Transformer: Decoder</h3>
                            <div class="column" style="width: 28%;">
                                <img src="images/transformer_decoder.png" alt="Transformer Decoder" style="width: 100%;">
                            </div>
                            <div class="column" style="width: 68%;">
                                <p style="text-align: left">Autoregressive sequence synthesis</p>
                                <p style="text-align: left">\(y_0 = \left< \text{BOS} \right>\) — start symbol embedding;</p>
                                <p style="text-align: left"><strong>For all</strong> \(t = 1, 2, \dots\)</p>
                                <ol>
                                    <li>Masking data "from the future":
                                        <p>\(h_t = y_{t-1} + p_t; \ H_t = (h_1, \dots, h_t)\)</p>
                                    </li>
                                    <div class="fragment">
                                        <li>Multi-head self-attention:
                                            <p>\(h_t^\prime = {\color{olive} LN_{sc}} \circ MH_j \circ \text{Attn}({\color{orange}W_q^j} h_{t},
                                                {\color{orange} W_k^j} H_t, {\color{orange}W_v^j} H_t)\)</p>
                                        </li>
                                    </div>
                                    <div class="fragment">
                                        <li>Multi-head attention on the Z vectors:
                                            <p>\(h_t^{\prime\prime} = {\color{olive} LN_{sc}} \circ MH_j \circ \text{Attn}({\color{orange}\tilde W_q^j} h_{t}^\prime,
                                                {\color{orange}\tilde W_k^j} Z, {\color{orange}\tilde W_v^j} Z)\)</p>
                                        </li>
                                    </div>
                                </ol>
                            </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Architecture of Transformer Decoder</h3>
                            <div class="column" style="width: 28%;">
                                <img src="images/transformer_decoder.png" alt="Transformer Decoder" style="width: 100%;">
                            </div>
                            <div class="column" style="width: 68%;">
                                <ol start="4">
                                    <div class="fragment">
                                        <li>Two-layer neural network:
                                            <p>\(y_t = {\color{olive} LN_{sc}} \circ \text{FFN}(h_t^{\prime\prime})\)</p>
                                        </li>
                                    </div>
                                    <div class="fragment">
                                        <li>Linear predictive layer:
                                            <p>\(p(\tilde w|t) = \text{SoftMax}_{\tilde w}({\color{orange} W_y} y_t + {\color{orange} b_y})\)</p>
                                        </li>
                                    </div>
                                    <div class="fragment">
                                        <li>Generation \(\tilde w_t = \arg\max\limits_{\tilde w} p(\tilde w|t)\) <br>
                                            <strong>until</strong> \(\tilde w_t \neq \left< \text{EOS} \right>\)</li>
                                    </div>
                                </ol>
                            </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Training and Validation Criteria for Machine Translation</h3>
                        <p style="text-align: left"><strong>Criteria for learning</strong> parameters of the neural network \({\color{orange}{W}}\)
                            on the training set of sentences \(S\) with translation \(\tilde S\):</p>
                        $$ \sum\limits_{(S, \tilde S)} \sum\limits_{\tilde w_t \in \tilde S} \ln p(\tilde w_t|t, S, {\color{orange}W}) \to \max\limits_W $$
                        <div class="fragment">
                            <p style="text-align: left"><strong>Criteria for evaluating models</strong> (non-differentiable)
                                based on a sample of pairs of sentences "translation \(S\), standard \(S_0\)":</p>
                            <p style="text-align: left">BiLingual Evaluation Understudy:</p>
                            <p style="text-align: center;">
                                \(\text{BLEU} = \min \left(1, \frac{\sum \text{len}(S)}{\sum \text{len}(S_0)} \right) \times \) <br>
                                \(\underset{(S_0, S)}{\text{mean}}\left(\prod\limits_{n=1}^4 \frac{\#n\text{-gram from } S, \text{incoming in } S_0}{\#n\text{-gram in } S} \right)^{\frac14}\)
                            </p>
                        </div>
                    </section>
                </section>
                <section>
                    <section data-background-image="images/BERT_real.png">
                    </section>
                    <section>
                        <h3 style="text-align: left">BERT — Bidirectional Encoder Representations from Transformers</h3>
                        <p style="text-align: left">The BERT transformer is a decoderless encoder that can be trained to solve a wide range of NLP problems.
                            Scheme of data transformations in NLP tasks:</p>
                        <ul>
                            <li>\(S = (w_1, \dots, w_n)\) — sentence from words in the input language</li>
                            <p style="font-size: small;text-align: left">\(\downarrow\) learning embeddings with transformer \(\downarrow\)</p>
                            <li>\(X = (x_1, \dots, x_n)\) — embeddings of input sentence words</li>
                            <p style="font-size: small;text-align: left">\(\downarrow\) encoder transformer \(\downarrow\)</p>
                            <li>\(Z = (z_1, \dots, z_n)\) — contextual embeddings</li>
                            <p style="font-size: small;text-align: left">\(\downarrow\) additional training for a specific task \(\downarrow\)</p>
                            <li>\(Y\) — output text / markup / classification, etc.</li>
                        </ul>
                        <hr style="border: 0.4pt solid black;">
                        <div style="text-align: left;">
                            <em>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</em> (Google AI Language)<br>
                            BERT: pre-training of deep bidirectional transformers for language understanding. 2019.
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">MLM (Masked Language Modeling) Criterion for BERT Training</h3>
                        <p style="text-align: left">The masked language modeling criterion is built automatically from texts (self-supervised learning):</p>
                          $$ \sum\limits_{S} \sum\limits_{ i \in M(S)} \ln p(w_i|i, S, {\color{orange}W}) \to \max\limits_W, $$
                        <p style="text-align: left">where \(M(S)\) is a subset of masked tokens from \(S\),</p>
                          $$ p(w|i, S, {\color{orange}W}) = \underset{w \in V}{\text{SoftMax}}({\color{orange}W_z}z_i(S, {\color{orange}W_T}) + {\color{orange}b_z})$$
                        <p style="text-align: left">is a language model that predicts the \(i\)-th sentence token \(S\),
                            \(z_i(S, {\color{orange}W_T})\) is the contextual embedding
                            of the \(i\)-th sentence token \(S\) at the output of the transformer with parameters \({\color{orange}W_T}\),
                            \({\color{orange}W}\) — all transformer and language model parameters</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">A Few More Notes About Transformers</h3>
                        <ul>
                            <li>Fine-tuning: model \(f(Z(S, {\color{orange}W_T}), {\color{orange}W_f})\),
                                sample \(\{S\}\) and criterion \(\mathcal{L}(S, f) \to \max\)</li>
                            <li>Multitask learning: for additional training on a set of tasks \(\{t\}\),
                                models \(f_t(Z(S, {\color{orange}W_T}), {\color{orange}W_f})\),
                                samples \(\{S\}_t\) and sum of criteria \( \sum_t \lambda_t \sum_S \mathcal{L}_t(S, f_t) \to \max \)</li>
                            <li>GLUE, SuperGLUE, Russian SuperGLUE — sets of test problems for understanding natural language</li>
                            <li>Transformers are usually built not on words, but on tokens received by BPE (Byte-Pair Encoding) or SentencePiece</li>
                            <div class="fragment">
                                <li>First transformer: \(N = 6, d = 512, J = 8\), weights 65M</li>
                                <li>\(\text{BERT}_{\text{BASE}}\), GPT-1: \(N = 12, d = 768, J = 12\), weights 110M</li>
                                <li>\(\text{BERT}_{\text{LARGE}}\), GPT-2: \(N = 24, d = 1024, J = 16\), weights 340M-1000M</li>
                            </div>
                            <div class="fragment">
                                <li>GPT-3: weights 175 billion</li>
                                <li>Gopher (DeepMind): 280 billion</li>
                                <li>Turing-Megatron (Microsoft + Nvidia): 530 billion</li>
                            </div>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>
                        <ul>
                            <li>Attention models were first built into RNNs or CNNs, but they turned out to be self-sufficient</li>
                            <li>Based on them, the Transformer architecture was developed,
                                various variants of which (BERT, GPT, XLNet, ELECTRA, etc.) are currently SotA in natural language processing tasks, and not only</li>
                            <li>Multi-head self-attention (MHSA) has been proven to be equivalent to a convolutional network [Cordonnier, 2020]</li>
                        </ul>
                        <br><br>
                        <hr style="border: 0.4pt solid black;">
                        <div style="font-size: small;text-align: left">
                            <em>Vaswani et al.</em> Attention is all you need. 2017.<br>
                            <em>Dichao Hu.</em> An Introductory Survey on Attention Mechanisms in NLP Problems. 2018.<br>
                            <em>Xipeng Qiu et al.</em> Pre-trained models for natural language processing: A survey. 2020.<br>
                            <em>Cordonnier et al.</em> On the relationship between self-attention and convolutional layers. 2020.
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">What Else Can You Look at?</h3>
                        <ul>
                            <li>GPT-3 on Wikipedia: <a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank">https://en.wikipedia.org/wiki/GPT-3</a></li>
                            <li><a href="https://www.youtube.com/watch?v=zjkBMFhNj_g" target="_blank">
                                Intro to LLMs</a> by Andrej Karpathy, November 2023</li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <p style="text-align: left">
                            <strong>Regularization</strong>: to make aspects of attention as different as possible,
                            rows \(J \times n\) of matrices \(A, \alpha_{ji} = \text{SoftMax}_i a({\color{orange}W_k^j}h_i,
                            {\color{orange}W_q^j}q)\), decorrelated \((\alpha_{s}^T\alpha_{j} \to 0)\) and sparse \((\alpha_{j}^T\alpha_{j} \to 1)\):</p>
                        <p style="text-align: center;">\(\|AA^T - I \|^2 \to \min\limits_{\{{\color{orange}W_k^j}, {\color{orange}W_q^j}\}} \)</p>
                        <br><br>
                        <hr style="border: 0.4pt solid black;">
                        <div style="text-align: left;">
                            <em>Zhouhan Lin, Y.Bengio et al</em>. A structured self-attentive sentence embedding. 2017
                        </div>
                        <aside class="notes">
                            And the last thing is about regularization. We need to make our attention models as different as possible.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">NSP (Next Sentence Prediction) Criterion for BERT Training</h3>
                        <p style="text-align: left">The criterion for predicting the relationship between NSP sentences
                            is built automatically from texts (self-supervised learning):</p>
                            $$ \sum\limits_{(S, S^\prime)} \ln p\left(y_{SS^\prime}|S, S^\prime, {\color{orange}W}\right) \to \max\limits_W, $$
                        <p style="text-align: left">where \(y_{SS^\prime} = \) \(\left[S\right.\) followed by \(\left.S^\prime\right]\)
                            is the binary classification of a pair of sentences,</p>
                            $$ p(y|S, S^\prime, {\color{orange}W}) = \underset{y \in \{0,1\}}{\text{SoftMax}}\left({\color {orange}W_y} \tanh({\color{orange}W_s}z_0(S, S^\prime, {\color{orange}W_T}) + {\color{orange}b_s}) + {\color{orange}b_y}\right)$$
                        <p style="text-align: left">— probabilistic classification model for pairs \((S, S^\prime)\),
                            \(z_0(S, S^\prime, {\color{orange}W_T})\) — contextual embedding of token \(\left<\text{CLS}\right>\)
                            for a sentence pair written as \(\left<\text{CLS}\right> S \left<\text{SEP}\right> S^\prime \left<\text{SEP}\right>\)</p>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>