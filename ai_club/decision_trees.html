<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Logical Rules and Decision Trees</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

        <link rel="stylesheet" href="https://pyscript.net/releases/2024.11.1/core.css">
        <script type="module" src="https://pyscript.net/releases/2024.11.1/core.js"></script>
		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div class="column">
                            <img src="images/jetbrains_logo_icon_145150.webp" alt="jetbrains_logo" />
                        </div>
                        <div class="column">
                            <h2>Youth AI club</h2>
                            <div class="fragment" style="margin-bottom:20px;">
                                    <div class="typesetting">
                                        <h3>Logical Rules and Decision Trees</h3>
                                        <br />
                                        Alex Avdiushenko <br />
                                        October 30, 2024
                                    </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">In the last episode</h3>
                        <div class="fragment" style="text-align: left">
                            <div class="typesetting">
                                <ul>
                                    <li><span class="important">Linear models</span>:
                                        regression and classification, the concept of margin</li>
                                    <li>Stochastic Gradient Descent</li>
                                    <li><span class="important">Logistic Regression</span></li>
                                    <li>Principle of Maximum Likelihood (optional)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Logical rule</h3>

                        <p>$X^\ell = (x_i, y_i)_{i=1}^\ell \subset X \times Y$ — training sample, $y_i = y(x_i)$</p>

                        <div style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">A <em>logical rule</em> (regularity) is a predicate
                                    $R: X \to \{0, 1\}$, satisfying two requirements:</p>

                                <ol class="fragment">
                                  <li>Interpretability:</li>
                                    <ul>
                                      <li>$R$ is written in natural language</li>
                                      <li>$R$ depends on a small number of features (usually 1-3)</li>
                                    </ul>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <ol start="2">
                          <li>Informativeness with respect to one of the classes $c \in Y$:</li>
                            <ul>
                              <li>$p_c (R) = ＃\{x_i : R(x_i)=1 \text{ and } y_i =c\} \to \max$</li>
                              <li>$n_c (R) = ＃\{x_i : R(x_i)=1 \text{ and } y_i \neq c\} \to \min$</li>
                            </ul>
                        </ol>
                        <p style="text-align: left">If $R(x) = 1$, then we say "$R$ covers $x$"</p>

                        <div align="center">
                            <img src="../ml_with_python/images/logic_rule.jpg" alt="logic_rule" width='50%' style="border-radius: 5%"/>
                        </div>
                    </section>
                    <section>
                        <h4 style="text-align: left">Example from medicine</h4>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p><strong>If</strong> "age > 60" <strong>and</strong>
                                    "patient has previously had a heart attack",
                                    then do not perform surgery, the risk of negative outcome is 60%</p>
                            <br />
                            <br />
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <h4 style="text-align: left">Example from the field of scoring</h4>
                                <p><strong>If</strong> "home phone is listed on the form" <strong>and</strong>
                                    "salary > $2000" <strong>and</strong> "loan amount < $5000"
                                    then the loan can be issued, default risk is 5%</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Main steps of rules induction</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ol>
                              <li>Selection of rule family for finding regularities</li>
                              <li>Rule generation</li>
                              <li>Rule regularity selection</li>
                            </ol>

                            <p style="text-align: left"><em>Regularity</em> is a highly informative, interpretable,
                                single-class classifier with rejections.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Rule family selection</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ol>
                                  <li>Threshold condition (decision stump):</li>
                                  $$R(x) = [f_j(x) \leq {\color{orange}a_j}] \text{ or } [{\color{orange}a_j} \leq f_j(x) \leq {\color{orange}b_j}]$$
        
                                  <li><em>Conjunction</em> of threshold conditions:</li>
                                  $$R(x) = \bigwedge\limits_{j \in {\color{orange}J}} [{\color{orange}a_j} \leq f_j(x) \leq {\color{orange}b_j}]$$
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <ol start="3">
                          <li><em>Syndrome</em> — at least $d$ conditions out of $|J|$ must hold (for $d = |J|$ this is conjunction, for $d = 1$ — disjunction):</li>
                        </ol>
                          $$R(x) = \left[\sum\limits_{j \in {\color{orange}J}} [{\color{orange}a_j} \leq f_j(x) \leq
                            {\color{orange}b_j}] \geq {\color{orange}d}\right]$$
                    </section>
                    <section>
                            <div class="typesetting">
                            <ol start="4">
                              <li>Half-plane — linear threshold function:</li>
                              $$R(x) = \left[\sum\limits_{j \in {\color{orange}J}} {\color{orange}w_j} f_j(x)
                                \geq {\color{orange}w_0}\right]$$
                              
                              <li class="fragment"><em>Sphere</em> — proximity threshold function:
                              $$R(x) = [(\rho(x, {\color{orange}x_0}) \leq {\color{orange}w_0}]$$</li>

                              <li class="fragment">SCM — set covering machine [M. Marchand, 2001]:
                        $$\rho(x, {\color{orange}x_0}) = \sum\limits_{j \in {\color{orange}J}} {\color{orange}w_j}
                          |f_j(x) - f_j({\color{orange}x_0})|^{\color{orange}\gamma}$$</li>

                            </ol>
                        <p style="text-align: left" class="fragment">Parameters ${\color{orange}J, a_j, b_j, d, w_j}$
                            are tuned using the training set via optimization of the <strong>information criterion</strong>.</p>
                            </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Regularity selection based on a pair of criteria</h3>
                        $p_c \to \max, n_c \to \min$

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <div class="row">
                                    <div class="column">
                                        <p style="text-align: left">
                                        <strong>Pareto front</strong> — a set of non-improvable regularities.
                                        A point is non-improvable if there are no points to the left and below it</p>
                                        <p style="text-align: left">
                                        <strong>Problem</strong>: it would be desirable
                                        to have a single scalar criterion</p>
                                    </div>
                                    <div class="column">
                                        <img src="../ml_with_python/images/pareto-front.png" alt="pareto" width="80%" style="border-radius: 5%"/>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Complex informativeness criteria</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li>Entropy information gain criterion:</li>
                                  <p>$ \text{IGain}(p, n) = h\left(\frac{P}{\ell} \right) - \frac{p+n}{\ell} h\left(\frac{p}{p+n} \right) - \frac{\ell-p-n}{\ell} h\left(\frac{P-p}{\ell-p-n} \right) \to \max $</p>
                                  <p>$ h(q) = -q \log_2 q - (1-q) \log_2 (1 - q)$ — entropy of “two-class distribution”</p>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li style="text-align: left">Gini Impurity:</li>
                                  <p>$\text{IGini}(p, n) = \text{IGain}(p, n)$ when $h(q) = 4q(1 - q)$</p>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#fdf6e3">
<!--language=python-->
                        <script type="py-editor" config='{"packages":["numpy", "matplotlib","scikit-learn"]}'>
                            import matplotlib.pyplot as plt
                            import numpy as np

                            fig, ax = plt.subplots()

                            q = np.linspace(0.01, 0.99, num=50)
                            h_q = -q * np.log2(q) - (1 - q) * np.log2(1 - q)
                            h_q_2 = 4 * q * (1 - q)

                            ax.set_xlabel('q'), ax.set_xlim(-0.1, 1.1)
                            ax.set_ylabel('h(q)')

                            ax.plot(q, h_q, 'bo-', label='entropy')
                            ax.plot(q, h_q_2, 'go--', label='Gini impurity')
                            ax.grid(True), plt.legend(loc='best')

                            fig
                        </script>
                    </section>
                    <section data-background-color="#ffffff">
                        <img src="../ml_with_python/images/entropy_and_gini.png" alt="entropy_and_gini" width="70%">
                    </section>
                    <section>
                        <ul>
                          <li>Fisher's Exact Test:</li>
                          <p>$\text{IStat}(p, n) = -\frac{1}{\ell} \log_2 \frac{C_P^pC_N^n}{C_{P+N}^{p+n}} \to \max$
                              — the argument of logarith is simply the probability of realizing such a pair of $p, n$,
                             when choosing $(p+n)$ elements from $(P+N)$
                          </p>
                        </ul>

                        <div class="fragment" style="margin-bottom:20px;text-align: left">
                            <div class="typesetting">
                                <ul>
                                  <li>Normalized boosting criterion:</li>
                                  $$\sqrt{\frac{p}{P}} - \sqrt{\frac{n}{N}} \to \max$$
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Let's have a break!</h3>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Rules generation</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <p style="text-align: left"><strong>Input</strong>: Training sample $X^\ell$</p>
                            <p style="text-align: left"><strong>Output</strong>: Set of rules $Z$</p>

                            <ol>
                              <li>Initial set of rules $Z$</li>
                              <li><strong>repeat</strong></li>
                                <ul>
                                      <li>$Z^\prime :=$ set of local modifications of the rules $R \in Z$</li>
                                      <li>remove overly similar rules from $Z \cup Z^\prime$</li>
                                      <li>$Z :=$ most informative rules from $Z \cup Z^\prime$</li>
                                </ul>
                              <li><strong>until</strong> the rules keep improving</li>
                              <li><strong>return</strong> $Z$</li>
                            </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Local rule modifications</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Example.</strong> Family of conjunctions of threshold conditions:</p>
                                $$ R(x) = \bigwedge\limits_{j \in {\color{orange}J}} [{\color{orange}a_j} \leq f_j(x) \leq {\color{orange}b_j}]$$

                                <p style="text-align: left">Local modifications of a conjunctive rule:</p>
                                <ul>
                                  <li>varying one of the thresholds ${\color{orange}a_j}$ or ${\color{orange}b_j}$</li>
                                  <li>varying both thresholds ${\color{orange}a_j}$ and ${\color{orange}b_j}$ simultaneously</li>
                                  <li>adding feature ${\color{orange}f_j}$ to ${\color{orange}J}$ with variation of thresholds
                                      ${\color{orange} a_j, b_j}$</li>
                                  <li>removing feature ${\color{orange}f_j}$ from ${\color{orange}J}$</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li style="text-align: left">When removing a feature (pruning),
                                    informativeness is usually evaluated on a control sample (hold-out)</li>
                                    <li style="text-align: left">The same methods are suitable for optimizing
                                    the set ${\color{orange}J}$ as for feature selection</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Combined use of regularities</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul style="text-align: left">
                                  <li>Weighted Voting (linear classifier with weights $w_{yt}$):</li>
                                    <p>$a(x) = \arg\max_{y \in Y} \sum_{t=1}^{T} w_{yt} R_{yt}(x)$</p>

                                  <li>Simple Voting (majority committee):</li>
                                    <p>$a(x) = \arg\max_{y \in Y} \frac{1}{T_y}\sum_{t=1}^{T_y} R_{yt}(x)$</p>

                                  <li>Decision List (seniority committee), \(c_t \in Y \):</li>
                                  <div align="center">
                                    <img src="../ml_with_python/images/decision-list.jpg" alt="pareto" style="width: 80%;border-radius: 5%">
                                  </div>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Decision Tree</h3>
                          <p style="text-align: left">
                            A decision tree is a classification algorithm $a(x)$, defined by a tree (a connected acyclic graph).
                          </p>

                          <ul>
                            <li>$V = V_{\text{internal}} \cup V_{\text{leaf}}, v_0 \in V$ — root of the tree</li>
                            <li>$v \in V_{\text{internal}}:$ functions $f_v: X \to D_v$ and $S_v: D_v \to V, |D_v| < \infty$</li>
                            <li>$v \in V_{\text{leaf}}:$ label of the class $y_v \in Y$</li>
                          </ul>

                        <div class="row">
                            <div class="column">
                                  <p style="text-align: left">
                                    <b>Special case:</b> $D_v = {0, 1}$ — binary decision tree
                                  </p>

                                  <p style="text-align: left">
                                    <b>Example:</b> $f_v(x) = [f_j(x) \ge \theta_j]$
                                  </p>
                            </div>
                            <div class="column">
                                <img src="../ml_with_python/images/decision-tree.jpg" alt="decision-tree" width="50%" style="border-radius: 5%"/>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left"><a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">
                            Fisher's Iris</a></h3>
                        <p style="text-align: left">
                            A simple well-known dataset for classification
                        </p>

                        <div class="row">
                            <div class="column">
                                <img style="vertical-align:middle; display:inline;" src="../ml_with_python/images/iris_measurements.webp" alt="iris_measurements" width="70%">
                            </div>
                            <div class="column">
                                <p style="text-align: left">Classes:
                                  <ul>
                                    <li>Iris setosa</li>
                                    <li>Iris virginica</li>
                                    <li>Iris versicolor</li>
                                  </ul>
                                </p>
                                <p style="text-align: left">Features:</p>
                                  <ul>
                                    <li>Length/Width of the sepal</li>
                                    <li>Length/Width of the petal</li>
                                  </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" config='{"packages":["numpy", "matplotlib","scikit-learn"]}' env="shared">
                            import numpy as np

                            from sklearn.datasets import load_iris
                            X, y = load_iris(return_X_y=True)

                            features = np.array(["sepal length", "sepal width",
                                              "petal length", "petal width"])
                            labels = np.array(["setosa", "versicolor", "virginica"])

                            print(X[:3], "\n", y[:3])
                        </script>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                        import matplotlib.pyplot as plt

                        def show_legend():
                            cb = plt.colorbar()
                            loc = [0, 1, 2]
                            cb.set_ticks(loc)
                            cb.set_ticklabels(labels)
                        print("Ok!")
                        </script>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            fig = plt.figure(figsize=(18.0, 10.0))
                            plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap='autumn', edgecolors="black")
                            plt.xlabel(features[0]), plt.ylabel(features[1]), show_legend()
                            fig
                        </script>
                    </section>
                    <section data-background-color="white">
                        <img src="images/iris_dataset.png" alt="iris_dataset">
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            from sklearn.linear_model import LogisticRegression

                            clf = LogisticRegression(random_state=0, C=1.0).fit(X, y)
                            print(clf.predict(X))
                        </script>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            print(clf.predict_proba(X[:2]))
                        </script>
                        <script type="py-editor" env="shared">
                            print(clf.score(X, y))
                        </script>
                    </section>
                    <section>
                        <div class="r-frame">
                          How is the <b>score</b> calculated?
                        </div>
                        <br />
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                Answer: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression">
                                    Mean Accuracy</a>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#fdf6e3">
                        We will use a helper function that returns a grid
                        for further nice visualizations:
                        <script type="py-editor" env="shared">
                            def get_grid(data):
                                x_min, x_max = data[:, 0].min()-1, data[:, 0].max()+1
                                y_min, y_max = data[:, 1].min()-1, data[:, 1].max()+1
                                return np.meshgrid(np.arange(x_min, x_max, 0.01),
                                                   np.arange(y_min, y_max, 0.01))
                            print("Ok!")
                        </script>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <p style="text-align: left">Let's write a function to train on data,
                        predict the answer for each lattice point, and visualize the result.
                        Because the data is much better divided by petals;
                        we visualize only them (we take the average parameters of the sepals)</p>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            def plot_model(X, y, clf, prob=False, legend=True):
                                clf.fit(X, y)
                                xx2, xx3 = get_grid(X[:, [2,3]])
                                shape = xx2.ravel().shape
                                grid_xs = np.c_[np.full(shape, X[:,0].mean()), np.full(shape, X[:,1].mean()),
                                                xx2.ravel(), xx3.ravel()]
                                predicted = clf.predict(grid_xs).reshape(xx2.shape)
                                if prob:
                                    probs = clf.predict_proba(grid_xs)
                                    confidence = probs.max(axis=1).reshape(xx2.shape)
                                    mesh = plt.pcolormesh(xx2, xx3, confidence, cmap='autumn')
                                else:
                                    mesh = plt.pcolormesh(xx2, xx3, predicted, cmap='autumn')
                                plt.scatter(X[:, 2], X[:, 3], c=y, s=150, cmap='autumn', alpha=0.7, edgecolors="black")
                                plt.ylim([xx3.min(),xx3.max()]), plt.xlim([xx2.min(),xx2.max()])
                                plt.xlabel(features[2]), plt.ylabel(features[3])
                                if legend:
                                    if not prob:
                                        show_legend()
                                    else:
                                        cb = plt.colorbar(mesh)
                                return clf
                            print("Ok!")
                        </script>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            fig = plt.figure(figsize=(18.0, 10.0))
                            clf = plot_model(X, y, LogisticRegression())
                            fig
                        </script>
                    </section>
                    <section data-background-color="#ffffff">
                        <img src="../ml_with_python/images/LR_results.png" alt="LR_results" width="80%">
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            fig = plt.figure(figsize=(18.0, 10.0))
                            clf = plot_model(X, y, LogisticRegression(), prob=True)
                            fig
                        </script>
                    </section>
                    <section data-background-color="#ffffff">
                        <img src="../ml_with_python/images/LR_proba_results.png" alt="LR_proba_results" width="80%">
                    </section>
                </section>
                <section>
                    <section data-background-color="#fdf6e3">
                        <h3 style="text-align: left">Decision Tree Classifier</h3>
                        <p><a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">
                            Nice visual example of DecisionTreeClassifier</a></p>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            import matplotlib.pyplot as plt
                            from sklearn.tree import DecisionTreeClassifier

                            fig = plt.figure(figsize=(18.0, 10.0))
                            clf = plot_model(X, y, DecisionTreeClassifier())
                            fig
                        </script>
                    </section>
                    <section data-background-color="white">
                        <img src="images/DTC_default.png" alt="DTC_default">
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            fig = plt.figure(figsize=(18.0, 10.0))
                            clf = plot_model(X, y, DecisionTreeClassifier(), prob=True)
                            fig
                        </script>
                    </section>
                    <section data-background-color="white">
                        <img src="images/DTC_prob.png" alt="DTC_prob">
                    </section>
                    <section>
                        <div class="r-frame">
                          The model's confidence is close to 1.0 everywhere. What happened?
                        </div>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            fig = plt.figure(figsize=(18.0, 10.0))
                            clf = plot_model(X, y, DecisionTreeClassifier(max_depth=3))
                            fig
                        </script>
                    </section>
                    <section data-background-color="white">
                        <img src="images/DTC_max_depth3.png" alt="DTC_max_depth3">
                    </section>
                    <section data-background-color="#fdf6e3">
                        <script type="py-editor" env="shared">
                            from sklearn.tree import plot_tree

                            def draw_decision_tree(clf, column_names):
                                fig = plt.figure(figsize=(18,10))
                                plot_tree(clf, filled=True, feature_names=column_names)
                                return fig

                            # draw_decision_tree(clf, features)
                        </script>
                    </section>
                    <section data-background-color="white">
                        <img src="images/draw_DT.png" alt="draw_DT" width="900">
                    </section>
                    <section>
                      <h3>Training a Decision Tree: ID3 Algorithm</h3>

                      <p style="text-align: left">$v_0$ := TreeGrowing($X_\ell$)</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ol>
                                <li><b>FUNCTION</b> TreeGrowing($U \subseteq X_\ell$) $\mapsto$ root of a tree $v$</li>
                                <li><b>if</b> StopCriteria($U$) <b>then</b> <br/>
                                <b>return</b> a new leaf $v$, taking $y_v$ := Major($U$)
                                </li>
                                <li>find the most profitable feature for branching the tree:
                                    <p>$f_v = \arg\max\limits_{f \in F}$ Gain($f, U$)</p></li>
                                <li><b>if</b> Gain ($f_v$, $U$) < $G_0$ <b>then</b><br />
                                <b>return</b> a new leaf $v$, taking $y_v$ := Major($U$)</li>
                                <li>create a new internal node $v$ with the feature $f_v$</li>
                                <li><b>for all</b> $k \in D_v$
                                    $U_k = \{x \in U: f_v(x) = k\}, S_v(k)$ := TreeGrowing($U_k$)</li>
                                <li><b>return</b> $v$</li>
                            </ol>
                            <br />
                            <p style="text-align: left">Majority rule: Major($U$) := $\arg\max P(y|U)$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Measure of Uncertainty of Distribution</h3>

                        <p style="text-align: left">The frequency estimate of the class \(y\)
                            at the node \(v \in V_{\text{internal}}\):</p>

                        <p>\[ p_y \equiv P(y|U) = \frac{1}{|U|} \sum\limits_{x_i \in U} [y_i = y] \]</p>

                        <p style="text-align: left">\(\Phi(U)\) — measure of uncertainty (impurity) of the distribution \(p_y\):</p>

                        <div align="center">
                            <img src="../ml_with_python/images/F_impurity.jpg" alt="impurity">
                        </div>
                        <ol>
                            <li>is minimal, when \(p_y \in \{0,1\}\)</li>
                            <li>is maximal, when \(p_y = \frac{1}{|Y|}\) for all \(y \in Y\)</li>
                            <li>is symmetrical: does not depend on the renumbering of classes</li>
                        </ol>
                    </section>
                    <section>
                        \[
                        \Phi(U)  = \sum\limits_{y \in Y} p_y \mathcal{L}(p_y) =
                          \frac{1}{|U|} \sum\limits_{x_i \in U} \mathcal{L}(P(y_i| U)) \to \min ,
                        \]
                        <p style="text-align: left">where \(\mathcal{L}(p)\) decreases
                            and \(\mathcal{L}(1) = 0\), for example: \(-\log p, 1-p, 1-p^2\)</p>
                    </section>
                    <section>
                      <h3 style="text-align: left">Branching Criterion</h3>

                      <p style="text-align: left">
                        The uncertainty of the distributions \(P(y_i | U_k)\)
                          after branching the node \(v\) using the feature \(f\)
                          and partitioning \[U = \bigsqcup\limits_{k \in D_v} U_k\]
                      </p>

                      <p>
                        \[
                        \Phi(U_1, \dots , U_{|D_v|}) = \frac{1}{|U|} \sum\limits_{x_i \in U} \mathcal{L}(P(y_i| U_{f(x_i)})) = \\
                        = \frac{1}{|U|} \sum\limits_{k \in D_v} \sum\limits_{x_i \in U_k} \mathcal{L}(P(y_i| U_{k})) = \sum\limits_{k \in D_v} \frac{|U_k|}{|U|} \Phi(U_k)
                        \]
                      </p>
                    </section>
                    <section>
                      <p style="text-align: left">
                        The gain from branching the node \(v\):
                      </p>
                        \[
                        \text{Gain} (f, U) = \Phi(U) - \Phi(U_1, \dots , U_{|D_v|}) = \Phi(U) - \sum\limits_{k \in D_v} \frac{|U_k|}{|U|} \Phi(U_k) \to \max\limits_{f \in F}
                        \]
                    </section>
                    <section>
                      <h3 style="text-align: left">Gini and Entropy Criteria</h3>

                      <p>
                        Two classes, \(Y = \{0, 1\}\),
                        \(P(y|U) = \begin{cases}
                            q, &y = 1 \\
                            1-q, &y = 0
                          \end{cases}
                          \)
                      </p>
                      <br/>
                      <ul>
                        <li>
                          If \(\mathcal{L}(p) = -\log_2 p\), then
                          \(\Phi(U) = -q\log_2 q - (1-q) \log_2 (1-q)\) – entropy of the sample
                        </li>

                        <li>
                          If \(\mathcal{L}(p) = 2(1 - p)\), then
                          \(\Phi(U) = 4q(1 - q)\) – Gini impurity
                        </li>
                      </ul>
                    </section>
                    <section>
                      <h3>Greedy Descending Strategy: Advantages and Disadvantages</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                              <h4 style="text-align: left">Advantages:</h4>
                              <ul>
                                <li>Interpretability and simplicity of classification</li>
                                <li>Flexibility: the set \(F\) can be varied</li>
                                <li>Handles heterogeneous data and data with missing values</li>
                                <li>Linear complexity in relation to the length of the sample: \(O(|F|h\ell)\)</li>
                                <li>There are no classification refusals</li>
                              </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                              <h4 style="text-align: left">Disadvantages:</h4>
                              <ul>
                                <li>Greedy strategy overcomplicates the tree structure leading to severe overfitting</li>
                                <li>Sample fragmentation: the further \(v\) from the root, the less statistical reliability in the choice of \(f_v, y_v\)</li>
                                <li>High sensitivity to noise, composition of the sample, and the informativeness criterion</li>
                              </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>CART: Classification And Regression Trees</h3>

                        <p style="text-align: left">Generalization for regression: \( Y = \mathbb{R}, y_v \in \mathbb{R} \)</p>
                        \[ C(a) = \sum\limits_{i=1}^\ell (a(x_i) - y_i)^2 \to \min\limits_a \]

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Let \( U \) be the set of objects \( x_i \) that reach vertex \( v \)</p>

                                <p style="text-align: left">Uncertainty measure — mean squared error:</p>
                                \[ \Phi(U) = \min_{y \in Y} \frac{1}{|U|} \sum\limits_{x_i \in U} (y - y_i)^2 \]
                            </div>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">The value \( y_v \) in the terminal vertex \( v \) is the LSE solution:
                                $ y_v = \frac{1}{|U|} \sum\limits_{x_i \in U} y_i $</p>

                                <p style="text-align: left">Regression tree \( a(x) \) is a piecewise
                                    constant function.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Example. Regression Trees of Different Depths</h3>

                        <p style="text-align: left">The more complex the tree (the greater its depth),
                            the higher the influence of noise in the data and the higher
                            the risk of overfitting.</p>

                        <div align="center">
                          <img src="../ml_with_python/images/plot_tree_regression_001.png" alt="tree_regression" width="50%" />
                        </div>
                        <a href="https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html">https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html</a>
                    </section>
                    <section>
                        <h3 style="text-align: left">CART: Minimal Cost-Complexity Pruning Criterion</h3>

                        <p style="text-align: left">Mean squared error with a penalty for tree complexity:</p>
                        \[ C_\alpha(a) = \sum\limits_{i=1}^\ell (a(x_i) - y_i)^2 + \alpha |V_{\text{leaf}}| \to \min\limits_\alpha \]

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <p style="text-align: left">As \(\alpha\) increases, the tree is progressively simplified.
                            Moreover, the sequence of nested trees is unique.
                            From this sequence, the tree with the minimum error on the test sample (Hold-Out) is selected.</p>

                            <p style="text-align: left">For the case of classification,
                                a similar pruning strategy is used, but with the Gini criterion.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Auxiliary Task: Binarization of a Real Feature</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Purpose</strong>: to reduce the enumeration of predicates of the form
                                \([{\color{orange}\alpha} \leq f(x) \leq {\color{orange}\beta}]\)</p>
                                <p style="text-align: left"><strong>Given</strong>: a set of real-valued feature vectors
                                \(f(x_i), x_i \in X^l\)</p>
                                <p style="text-align: left"><strong>Find</strong>: the best (in any sense) division of the feature value range into
                                a relatively small number of zones:
                                <ul>
                                    <li>\(\zeta_0(x) = [f(x) < d_1]\)</li>
                                    <li>\(\zeta_s(x) = [d_s \leq f(x) < d_{s+1}], s = 1, \dots, r-1\)</li>
                                    <li>\(\zeta_r(x) = [d_r \leq f(x)]\)</li>
                                </ul>
                                <div align="center">
                                    <img src="../ml_with_python/images/partition.jpg" alt="partition"
                                         style="width: 800px;border-radius: 5%">
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">
                            Ways to Partition the Feature Value Area into Zones</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>Greedy Information Maximization by Merging</li>
                                    <li>Division into Equally Powered Subsamples</li>
                                    <li>Partitioning by a Uniform Grid of "Convenient" Values</li>
                                    <li>Combining Several Partitions — for Example, Uniform and Median</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ul>
                                <li>Basic requirements for logical regularities:<br />
                                    <ul>
                                        <li>interpretability</li>
                                        <li>informative</li>
                                        <li>diversity</li>
                                    </ul>
                                </li>
                                <li>Advantages of decision trees:<br/>
                                    <ul>
                                        <li>interpretability</li>
                                        <li>mixed types of data allowed</li>
                                        <li>capability to bypass missing data</li>
                                        <li>Drawbacks of decision trees:</li>
                                        <ul>
                                            <li>overfitting</li>
                                            <li>fragmentation</li>
                                            <li>instability to noise, composition of the sample, criteria</li>
                                        </ul>
                                    </ul>
                                </li>
                                <li>Ways to eliminate these drawbacks:<br/>
                                    <ul>
                                        <li>pruning</li>
                                        <li>regularization</li>
                                        <li>ensembles (forests) of trees — coming soon!</li>
                                    </ul>
                                </li>
                            </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Handling Missing Values</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                              <h4 style="text-align: left">During Training</h4>
                              <ul>
                                <li>If \(f_v(x_i)\) is undefined \(\Rightarrow\ x_i\) is excluded from \(U\) for \(\text{Gain}(f_v, U)\)</li>
                                <li>\(q_{vk} = \frac{|U_k|}{|U|}\) — probability estimate of the \(k\)-th branch, \(v \in V_{\text{internal}}\)</li>
                                <li>\(P(y|x, v) = \frac{1}{|U|} \sum\limits_{x_i \in U}[y_i = y]\) for all \(v \in V_{\text{leaf}}\)</li>
                              </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <h4 style="text-align: left">During Classification</h4>
                                <ul>
                                <li>If \(f_v(x)\) is undefined \(\Rightarrow\) take the distribution proportionally from the child</li>
                                    \[s = S_v(f_v(x)): P(y|x, v) = P(y|x, s)\]
                                <li>If \(f_v(x)\) is undefined \(\Rightarrow\) proportional distribution:
                                    \[P(y|x, v) = \sum\limits_{k \in D_v} q_{vk} P(y | x, S_v(k)))\]</li>
                                <li>Final decision — the most probable class: \(a(x) = \arg\max\limits_{y \in Y} P(y|x, v_0)\)</li>
                              </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p class="r-frame">Greedy strategy overcomplicates tree structure, so we need..</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                              <h3 style="text-align: left">Tree Pruning</h3>
                              <p style="text-align: left">\(X^q\) — Independent validation set, \(q \approx 0.5\ell\)</p>
                              <ol>
                                  <li><b>for all</b> \(v \in V_{\text{internal}}\)</li>
                                  <li>\(X_v^q :=\) subset of instances \(X^q\) that reach \(v\)</li>
                                  <li><b>if</b> \(X_v^q = \emptyset\) <b>then return</b> a new leaf \(v\), with \(y_v := \text{Major}(U)\)</li>
                                  <li>Count the number of misclassifications for \(X_v^q\) classified in different ways:</li>
                                  <ul>
                                      <li>\(\ \ \text{Err}(v)\) — by the subtree rooted at \(v\)</li>
                                      <li>\(\ \ \text{Err}_k(v)\) — by the subtree \(S_v(k)\), for \(k \in D_v\)</li>
                                      <li>\(\ \ \text{Err}_c(v)\) — by class \(c \in Y\)</li>
                                  </ul>
                                  <li>Depending on which is minimal:</li>
                                  <ul>
                                      <li>Keep the subtree \(v\)</li>
                                      <li>Replace the subtree \(v\) with the child \(S_v(k)\)</li>
                                      <li>Replace the subtree \(v\) with a leaf, with \(y_v := \arg\min\limits_{c \in Y} \text{Err}_c(v)\)</li>
                                  </ul>
                              </ol>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>