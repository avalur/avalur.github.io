<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Ensembles, gradient boosting and random forest</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

        <style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
        </style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div class="column">
                            <img src="images/jetbrains_logo_icon_145150.webp" alt="jetbrains_logo" />
                        </div>
                        <div class="column">
                            <h2>Youth AI club</h2>
                            <div class="fragment" style="margin-bottom:20px;">
                                    <div class="typesetting">
                                        <h3>Ensembles, gradient boosting and random forest</h3>
                                        <br />
                                        Alex Avdiushenko <br />
                                        November 6, 2024
                                    </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">In the last episode</h3>
                        <div class="fragment" style="text-align: left">
                            <div class="typesetting">
                                <ul>
                                    <li><span class="important">Logical rules</span>:
                                        family selection, local modification and informativeness criteria</li>
                                    <li><span class="important">Decision Tree Classifier</span> model</li>
                                    <li>Measure of uncertainty for probability distribution</li>
                                    <li>Handling missing values (optional)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Notations</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <p style="text-align: left">
                                    $X^\ell = (x_i, y_i)^\ell_{i=1} \subset X \times Y$ — training set, $y_i = y^*(x_i)$</p>

                            <p>$a(x) = C(b(x))$ — algorithm, where</p>
                            <p style="text-align: left">$b: X \to {R}$ — base algorithm (operator)</p>
                            <p style="text-align: left">$C: {R} \to Y$ — composition of raw estimates</p>
                            <p style="text-align: left">${R}$ — space of estimates</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Definition</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Ensemble of base algorithms \(b_1, \dots, b_T\)</p>
                                <p>\(a(x) = C(F(b_1(x), \dots, b_T(x)))\)</p>
                                <p style="text-align: left">where \(F: R^T \to R\) — correcting operation</p>
                                <hr/>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p class="r-frame important" style="text-align: left;padding-left: 20px">Why introduce correcting operation \(R\)?</p>
                                <p style="text-align: left">In classification tasks, the set of mappings \(\{F: R^T \to R\}\)
                                    is significantly wider than \(\{F: Y^T \to Y\}\)</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Examples of Estimation Spaces and Compositions</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>
                                        <strong>Example 1</strong>: binary classification, \(Y = \{-1, +1\}\):
                                        \[a(x) = \text{sign}(b(x))\]
                                        so \(R = \mathbb{R}, b: X \to \mathbb{R}, C(b) = \text{sign}(b)\)
                                    </li>
                                    <li>
                                        <strong>Example 2</strong>: multiclass classification with \(M\) classes \(Y = \{1,\dots, M\}\):
                                        \[a(x) = \arg\max_{y \in Y} b_y(x)\]
                                        so \(R = \mathbb{R}^M, b: X \to \mathbb{R}^M, C(b_1, \dots, b_M) \equiv \arg\max\limits_{y \in Y} b_y\)
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <ul>
                            <li>
                                <strong>Example 3</strong>: regression, \(Y = R = \mathbb{R}\)<br/>
                                \(C(b) \equiv b\) — there is no need for a composition rule
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h3>Examples of Ensembles (Correcting Operations)</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ul>
                                <li>
                                    <strong>Example 1</strong>: Simple Voting<br/>
                                    \(F(b_1(x),\dots, b_T(x)) = \frac{1}{T} \sum\limits_{t=1}^T b_t(x), x \in X\)
                                </li>
                                <li>
                                    <strong>Example 2</strong>: Weighted Voting<br/>
                                    \(F(b_1(x),\dots, b_T(x)) = \sum\limits_{t=1}^T \alpha_tb_t(x), x \in X, \alpha_t \in \mathbb{R}\)
                                </li>
                                <li>
                                    <strong>Example 3</strong>: Mixture of Experts<br/>
                                    \(F(b_1(x),\dots, b_T(x)) = \sum\limits_{t=1}^T g_t(x)b_t(x), x \in X, g_t: X \to \mathbb{R}\)
                                </li>
                            </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Stochastic Methods for Building Compositions</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">To make sure the algorithms in the composition are distinct:</p>
                                <ul>
                                    <li>They are trained on (random) sub-samples<br />
                                        — bagging = bootstrap aggregation,
                                        [Breiman, 1996]: sub-samples of length \(\ell\) with repetitions.
                                        Ratio of objects falling into the sample is  \(\left( 1 - \frac{1}{e}\right) \approx 0.632\)
                                    </li>
                                    <li>Or on (random) subsets of features<br />
                                        — RSM = random subspace method, [Ho, 1998]
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#ffffff">
                        <div>
                            <img src="../ml_with_python/images/bagging_visualization.jpg" alt="bagging_visualization" width=80% />
                        </div>
                    </section>
                </section>
                <section>
                    <section data-background-color="#ffffff">
                        <h3><a href="https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html">
                            Boosting explained</a></h3>
                        <img src="../ml_with_python/images/gbt_visualization.jpg" alt="gbt_visualization" width=70% />
                    </section>
                    <section>
                        <h3 style="text-align: left">Boosting for binary classification tasks</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <p>Let's take \(Y = \{\pm 1\}, b_t: X \to \{-1, {\color{orange}[0]}, +1\}, C(b) = \text{sign}(b)\)</p>

                            <h4 style="text-align: left">Weighted voting</h4>
                            <p>\(a(x) = \text{sign}\left( \sum\limits_{t=1}^T \alpha_t b_t (x)\right), x \in X\)</p>

                            <h4 style="text-align: left">Quality functional of composition</h4>
                            <p>Number of errors in \(X^\ell\):
                                \(Q_T = \sum\limits_{i=1}^\ell \left[y_i \sum\limits_{t=1}^T \alpha_t b_t(x_i) < 0 \right]\)</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h4 style="text-align: left">Two main boosting heuristics:</h4>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>Fix \(\alpha_1 b_1(x), \dots, \alpha_{t-1} b_{t-1}(x)\) when adding \(\alpha_{t}b_t(x)\)</li>
                                    <li>Smooth approximation of the threshold loss function \([M < 0]\)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Exponential Approximation of the Threshold Loss Function</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">The quality functional \(Q_T\) is estimated from above:</p>
                                \[Q_T \leq \tilde Q_T = \sum_{i=1}^\ell \exp(-M_i) = \\ = \sum_{i=1}^\ell \underbrace{\exp\left(-y_i \sum_{t=1}^{T-1} \alpha_t b_t (x_i)\right)}_{w_i} \exp(-y_i \alpha_T b_T (x_i))\]
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Normalized weights: \[\tilde W^\ell = (\tilde w_1, \dots, \tilde w_\ell), \tilde w_i = w_i / \sum\limits_{j=1}^\ell w_j\]</p>
                                <p>Weighted number of erroneous (negative) and correct (positive) classifications for weights vector \(U^\ell = (u_1, \dots, u_\ell)\):</p>
                                <p>\(N(b, U^\ell) = \sum\limits_{i=1}^\ell u_i [b(x_i) = -y_i]\)</p>
                                <p>\(P(b, U^\ell) = \sum\limits_{i=1}^\ell u_i [b(x_i) = y_i]\)</p>
                                <p style="text-align: left">\((1 - N - P)\) — weighted number of classification refusals</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Classical Variant of AdaBoost</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Assume there are no rejections, \(b_t: X \to \{\pm 1 \}\). Then \(P = 1 − N\)</p>

                                <h4 style="text-align: left">Theorem (Freund, Schapire, 1995)</h4>
                                <p style="text-align: left">Suppose that for any normalized weight vector \(U^\ell\) there exists an algorithm
                                    \(b \in B\), classifying the sample at least slightly better than at random:
                                    \(N(b, U^\ell) < \frac12\).
                                </p>
                                <p style="text-align: left">Then the minimum of the functional
                                    \(\tilde Q_T\) is reached when</p>
                                \[b_T = \arg\min_{b \in B} N (b, \tilde W^\ell), \quad
                                \alpha_T = \frac12 \ln\frac{1-N(b_T, \tilde W^\ell)}{N(b_T, \tilde W^\ell)}\]
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">AdaBoost Algorithm</h3>

                        <p style="text-align: left">
                            <strong>Input</strong>: Training sample \(X^\ell\), parameter <span style="color:orange">\(T\)</span></p>
                        <p style="text-align: left">
                            <strong>Output</strong>: Base algorithms and their weights \(\alpha_t b_t, t = 1, \dots, T\)</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ol>
                                    <li>Initialize object weights: \(w_i = \frac{1}{\ell}, i = 1, \dots, \ell\)</li>
                                    <li><strong>for all</strong> \(t = 1, \dots, T\)</li>
                                    <p>Train the basic algorithm: \(b_t = \arg \min\limits_b N(b, W^\ell)\)</p>
                                    <p>\(\alpha_t = \frac12 \ln \frac{1-N(b, W^\ell)}{N(b, W^\ell)}\)</p>
                                    <li>Update object weights: \(w_i = w_i \exp(-\alpha_t y_i b_t(x_i)), i = 1, \dots, \ell\)</li>
                                    <li>Normalize object weights:
                                        \[ w_0 = \sum_{j=1}^\ell w_j ,\ w_i = w_i / w_0, i = 1, \dots, \ell\]
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Let's have a break!</h3>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Heuristics and Recommendations</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>
                                        <strong>Base classifiers (weak classifiers)</strong>
                                        <ul>
                                            <li>Decision trees — most commonly used</li>
                                            <li>Threshold rules (data stumps)
                                                <br />
                                                \(B = \{b(x) = [f_j(x) \lessgtr\theta] | j = 1, \dots, n, \theta \in \mathbb{R}\}\)
                                            </li>
                                            <li>For strong classifiers (as SVM for instance), boosting is usually not effective</li>
                                        </ul>
                                    </li>
                                    <li><strong>Noise rejection</strong>: dispose of objects with the largest \(w_i\)</li>
                                    <li><strong>Additional stop criterion</strong>: increase in error rate on the validation set</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <div class="r-frame important" style="text-align: left;padding-left: 20px">
                                    <b>Question 2:</b> Why does Boosting work?
                                </div>
                            </div>
                        </div>
                        <br>
                        <div class="fragment">
                            <div class="typesetting">
                                <div class="r-frame important" style="text-align: left;padding-left: 20px">
                                    <b>Question 3:</b> What are the drawbacks of AdaBoost?
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Drawbacks of AdaBoost</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li>Over-sensitivity to outliers due to $e^{-M}$</li>
                                  <li>AdaBoost creates "black boxes" — cumbersome, not interpretable compositions of hundreds of algorithms </li>
                                  <li>It requires fairly large training sets (Bagging can manage with shorter ones) </li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Ways to eliminate:</p>
                                <ul>
                                  <li>Other approximations to the threshold loss function</li>
                                  <li>Continuous real base algorithms $b_t: X \to \mathbb{R}$</li>
                                  <li>Explicit optimization of margins, without approximation</li>
                                  <li>Less greedy strategies for growing the composition</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Gradient Boosting for Arbitrary Loss Function (<a href="https://jerryfriedman.su.domains/ftp/trebst.pdf">GB machine</a>)</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <p style="text-align: left"> Linear (convex) combination of base algorithms: </p>
                            $a(x) = \sum\limits_{t=1}^T \alpha_t b_t(x), \ x \in X, \ \alpha_t \in \mathbb{R}_+$

                            <p style="text-align: left">Quality functional with arbitrary loss function $\mathcal{L}(a, y)$:</p>
                            $Q(\alpha, b, X^\ell) = \sum\limits_{i=1}^\ell \mathcal{L}
                                \underbrace{\overbrace{(\sum\limits_{t=1}^{T-1} \alpha_t b_t(x_i)}^{f_{T-1,i}} +
                                \alpha b(x_i)}_{f_{T,i}} , y_i) \to \min\limits_{\alpha, b}$

                            <p>$f_{T-1, i}$ — current approximation, $f_{T, i}$ — next approximation</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">
                            Parametric approximation of the gradient step
                        </h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">
                                    Gradient method of minimization
                                    $Q(f) \to \min, f \in \mathbb{R}^\ell$: </p>

                                <p>$f_0 =$ initial approximation</p>
                                <p>$f_{T,i} = f_{T-1,i} - \alpha g_i, \ i = 1, \dots, \ell$</p>
                                <p>$g_i = \mathcal{L}^\prime (f_{T-1, i}, y_i)$ — components of the gradient vector,
                                    $\alpha$ — learning rate</p>

                                <p style="color:orange;text-align: left">Note: this is very similar to one boosting iteration!</p>

                                <p> $f_{T,i} = f_{T-1,i} + \alpha b(x_i), \ i = 1, \dots, \ell$ </p>

                                <p style="text-align: left"><b>Idea: </b> we will look for such a basic algorithm
                                $b_T$ that the vector $(b_T(x_i))_{i=1}^\ell$ will approximate
                                    the antigradient vector $(-g_i)_{i=1}^\ell$</p>
                                $b_T = \arg\min\limits_b \sum\limits_{i=1}^\ell (b(x_i) + g_i)^2$
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Gradient Boosting Algorithm</h3>

                        <p style="text-align: left"><b>Input:</b> training set $X^\ell$, parameter <span style="color:orange">$T$</span></p>
                        <p style="text-align: left"><b>Output:</b> base algorithms and their weights $\alpha_t b_t, t = 1, \dots, T$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ol>
                                    <li>Initialization: $f_i = 0, i = 1, \dots, \ell$ </li>
                                    <li><b>for all</b> $t = 1, \dots, \color{orange}{T}$ </li>
                                    <p>Basic algorithm approximating antigradient:</p>
                                    $b_t = \arg \min\limits_b \sum\limits_{i=1}^\ell (b(x_i) + \mathcal{L}^\prime(f_i, y_i))^2$
                                    <li>One-dimensional minimization problem:
                                    $\alpha_t = \arg \min\limits_{\alpha > 0} \sum\limits_{i=1}^\ell
                                    \mathcal{L}(f_i + \alpha b_t(x_i), y_i)$ </li>
                                    <li>Updating the vector of values on the sample objects: $f_i = f_i + \alpha_t b_t(x_i)), i = 1, \dots, \ell$</li>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Stochastic Gradient Boosting (SGB)</h3>
                        <p style="text-align: left"><strong>Idea:</strong>
                            In steps 2-4, use not the entire sample \(X^\ell\),
                            but a random subsample without replacement.</p>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Advantages:</strong></p>
                                <ul>
                                    <li>Improved quality</li>
                                    <li>Enhanced convergence</li>
                                    <li>Reduced training time</li>
                                </ul>
                            <hr>
                            <p style="text-align: left">
                                <em>Friedman G.</em> Stochastic Gradient Boosting, 1999</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <div class="r-frame important" style="text-align: left;padding-left: 20px">Why is Boosting so ideal?</div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Regression and AdaBoost</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Regression:</strong> \(\mathcal{L}(a,y) = (a-y)^2\)</p>
                                <ul>
                                    <li>\(b_T(x)\) is trained on differences \(y_i - \sum\limits_{t=1}^{T-1} \alpha_t b_t (x_i)\)</li>
                                    <li>If regression is linear, then \(\alpha_t\) doesn't need to be trained</li>
                                </ul>
                                <p style="text-align: left"><strong>Classification:</strong> \(\mathcal{L}(a,y) = e^{-ay}, b_t \in \{-1, 0 , +1\}\)</p>
                                <ul>
                                    <li>GB exactly matches AdaBoost</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>XGBoost — a popular and fast implementation of gradient boosting over trees</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Regression and classification trees (CART):</p>
                                $$b(x) = \sum\limits_{j=1}^J w_j [x \in R_j]$$

                                <p style="text-align: left">
                                    where $R_j$ is the area of space covered by leaf $j$,
                                    $w_j$ is the leaf weight, and $J$ is the number
                                    of leaves in the tree
                                </p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">Quality functional with a total of $L_0, L_1, L_2$ regularizers:</p>
                        $Q(b, \{w_j\}_{j=1}^J, X^\ell) = \sum\limits_{i=1}^\ell \mathcal{L} \left(\sum\limits_{t=1}^{T-1} \alpha_t b_t(x_i) +
                        \alpha b(x_i, y_i)\right) + \gamma \sum\limits_{j=1}^J [w_j \neq 0] + \mu \sum\limits_{j=1}^J |w_j| +
                        \frac{\lambda}{2} \sum\limits_{j=1}^J w_j^2 \to \min\limits_{b, w_j}$

                        <p style="text-align: left">The task has an analytic solution for $w_j$.</p>
                        <p style="text-align: left">
                            Other popular implementations of gradient boosting over random trees: <span style="color: orange">LightGBM, CatBoost</span>
                        </p>
                    </section>
                    <section>
                        <h3>Comparison: boosting — bagging — RSM</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                 <li>Boosting is better for large training datasets and
                                     classes with complex shape borders</li>
                                 <li>Bagging and RSM are better for small training datasets</li>
                                 <li>RSM is better in cases where there are more features than objects,
                                     or when there are many non-informative features</li>
                                 <li>Bagging and RSM can be effectively parallelized,
                                     boosting is performed strictly sequentially</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#ffffff">
                        <img src="../ml_with_python/images/rf_meme.jpeg" width="60%">
                    </section>
                    <section>
                        <h3>Random Forest</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">Training a random forest:</p>
                                <ul>
                                 <li>Bagging over decision trees, without pruning</li>
                                 <li>The feature in each tree node is chosen from a random subset of $k$ out of $n$ features</li>
                                </ul>
                                <hr>
                                <p style="text-align: left">The number of trees $T$ can be selected based on the out-of-bag criterion —
                                    the number of errors on objects $x_i$, if we don't consider the votes of trees
                                    for which $x_i$ was training:</p>
                                $$\text{out-of-bag}(a) = \sum\limits_{i=1}^\ell \left[\text{sign}(\sum\limits_{t=1}^T [x_i \notin U_t] b_t(x_i))
                                \neq y_i \right] \to \min$$

                                <p style="text-align: left">This is an unbiased estimate of generalization ability.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Summary of ensembles</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ul>
                             <li>Ensembles allow solving complex tasks that are poorly solved by individual algorithms</li>
                             <li>It's too challenging to train the ensemble as a whole.
                                 Therefore, we train basic algorithms one by one</li>
                             <li>An important discovery in the mid-90s: boosting's generalizing ability
                                 doesn't worsen with the increasing complexity $T$</li>
                             <li>Gradient boosting — the most general of all boostings:
                               <ul>
                                <li>an arbitrary loss function</li>
                                <li>an arbitrary space of estimates $R$</li>
                                <li>suitable for regression, classification, ranking</li>
                               </ul>
                             </li>
                             <li>Its stochastic version SGB — better and faster</li>
                             <li>Most often, GB is applied to decision trees</li>
                             <li>Gradient boosting over Oblivious Decision Trees with categorical features = Catboost</li>
                            </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Justification of Boosting</h3>

                        <p style="text-align: left">Strengthening the concept of the error rate
                            of the algorithm $a(x) = \text{sign}(b(x))$</p>
                        $$\nu_\theta (a, X^\ell) = \frac{1}{\ell} \sum\limits_{i=1}^\ell [b(x_i) y_i \leq \theta]$$

                        <p style="text-align: left">The usual error rate
                            $\nu_0 (a, X^\ell) \leq \nu_\theta (a, X^\ell)$ when $\theta > 0$</p>

                    </section>
                    <section>
                        <h3 style="text-align: left">Theorem (Freund, Schapire, Bartlett, 1998)</h3>

                        <p>If $|B| < \infty$, then $\forall \theta > 0, \forall \eta \in (0,1)$
                            with a probability $1-\eta$</p>
                        $$P[y a(x) < 0] \leq \nu_\theta (a, X^\ell) + C\sqrt{\frac{\ln |B| \ln \ell}{\ell\theta^2} + \frac{1}{\ell}\ln \frac{1}{\eta}}$$

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <h4 style="text-align: left">Main conclusion</h4>
                                <p style="text-align: left">The estimate depends on $|B|$, but not on $T$.
                                Voting does not increase the complexity of the effectively used set of algorithms.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Justification of Boosting: what is actually happening?</h3>

                        <div align="center">
                            <img src="../ml_with_python/images/boosting_exp.jpg" alt="boosting_exp" width=800 />
                        </div>
                        <p style="text-align: left">
                            <b>Distribution of margins</b>: the share of objects with a margin
                            smaller than a fixed $\theta$ after 5, 100, 1000 iterations (UCI classification task: vehicle)
                        </p>
                    </section>
                    <section>
                        <ul>
                            <li>With growing $T$, the margin distribution shifts to the right, that is, boosting "spreads" the classes in the space of growing-dimension vectors $(b_1(x), \dots, b_T(x))$</li>
                            <li>Therefore, the second term can be reduced in the estimate by increasing $\theta$ and not changing $\nu_\theta (a, X^l)$</li>
                            <li>The second term can be reduced if |B| is reduced, that is, if a simple family of basic algorithms is taken</li>
                        </ul>
                        <br><br>
                        <h4>Thank you for your attention!</h4>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>