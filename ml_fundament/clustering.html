<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Semi- and un-supervised learning, clustering</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./../advanced_ml/images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Fundamentals of Machine Learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>Semi- and unsupervised learning, clustering</h3>
								<br />
								Alex Avdiushenko <br />
								December 24, 2024
							</div>
					</div>
                </section>
                <section>
                    <section>
                        <h3>Clustering Task Formulation</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Given</strong></p>
                                <ul>
                                    <li>\(X\) — object's space</li>
                                    <li>\(X^\ell = \{x_1, \dots, x_\ell \}\) — training samples</li>
                                    <li>\(\rho: X \times X \to [0, \infty)\) — distance function between objects</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>The task is to find</strong></p>
                                <ul>
                                    <li>\(a: X \to Y\) — clustering algorithm</li>
                                    <li>where \(Y\) is a set of clusters</li>
                                </ul>

                                <p style="text-align: left">such that</p>
                                <ul>
                                    <li>each cluster consists of <span style="color: darkorange">close objects</span></li>
                                    <li>objects from different clusters are significantly different</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Incorrectness of the Clustering Task</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">The solution to the clustering task is fundamentally ambiguous:</p>

                                <ul>
                                    <li>there is no correct math formulation of the clustering task</li>
                                    <li>there are many criteria for the quality of clustering</li>
                                    <li>there are many heuristic methods of clustering</li>
                                    <li>the number of clusters \(|Y|\), as a rule, is not known in advance</li>
                                    <li>the result of clustering heavily depends on the metric \(\rho\),
                                        the choice of which is also heuristic</li>
                                </ul>
                            </div>
                        </div>
                        <br><br>
                        <div class="fragment">
                            <div class="typesetting">
                                <div class="r-frame">
                                    <b>Question 1:</b> How many clusters are here?
                                </div>

                                <div align="center">
                                    <img src="../advanced_ml/images/cluster_example.jpg" alt="cluster_example" width="1000" />
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Goals of Clustering</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                  <li><strong>Simplify further data processing</strong>
                                    <ul>
                                      <li>divide the set \(X^\ell\) into groups of similar objects to work with each group separately (tasks of classification or regression)</li>
                                    </ul>
                                  </li>
                                  <li><strong>Reduce the volume of stored data</strong>
                                    <ul>
                                      <li>keep one representative from each cluster (data compression tasks)</li>
                                    </ul>
                                  </li>
                                  <li><strong>Identify atypical objects (outliers)</strong>
                                    <ul>
                                      <li>those that do not fit into any of the clusters</li>
                                    </ul>
                                  </li>
                                  <li><strong>Construct a hierarchy of the set of objects</strong>
                                    <ul>
                                      <li>tasks of taxonomy</li>
                                    </ul>
                                  </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Types of Cluster Structures</h3>

                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type1.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Intra-cluster distances are generally smaller than inter-cluster distances
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type2.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br><br>
                                Ribbon clusters
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type3.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Center-based clusters
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type4.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Clusters may be connected by bridges
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type5.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Clusters may overlay a sparse background of infrequently placed objects
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type6.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Clusters may overlap
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type7.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Clusters can be formed not only by proximity but also by other types of regularities
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/type8.jpg" alt="type" />
                            </div>
                            <div class="column">
                                <br>
                                Clusters may not exist at all
                            </div>
                        </div>

                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                  <li>Each clustering method has its limitations and identifies only clusters of certain types</li>
                                  <li>The concept "type of cluster structure" depends on the method
                                      and also does not have a formal definition</li>
                                  <li>In the multidimensional case, all this is even more true</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Semi-supervised Learning Task Formulation</h3>

                        <p style="text-align: left"><strong>Given: </strong>set of objects $X$, set of classes $Y$</p>

                        $X^k =
                         \begin{array}{c}
                         \{x_1, \dots, x_k \} \\
                         \color{orange}{\{y_1, \dots, y_k \}} \\
                         \end{array}$ — labeled data
                        <br>
                        $U = \{x_{k+1}, \dots, x_\ell \}$ — unlabeled data
                        <br>
                        <p style="text-align: left">Two variants of the task formulation:</p>
                        <ul>
                            <li>Semi-supervised learning (SSL): construct a classification algorithm $a: X \to Y$</li>
                            <li>Transductive learning:
                            knowing ${\color{orange}\text{all}}\ \{x_{k+1}, \dots, x_\ell \}$, obtain labels $\{a_{k+1}, \dots, a_\ell\}$</li>
                        </ul>
                    </section>
                    <section data-background-color="antiquewhite">
                        <h3>SSL Does Not Reduce to Classification</h3>

                        <p style="text-align: left"><strong>Example 1: </strong>Class densities restored.</p>

                        <table style="width:80%">
                          <tr>
                            <td><img style="border-radius: 5%" src="../advanced_ml/images/not_only_classification_1.jpg" alt="not_only_classification" /></td>
                            <td><img style="border-radius: 5%" src="../advanced_ml/images/not_only_classification_2.jpg" alt="not_only_classification" /></td>
                          </tr>
                          <tr>
                            <td style="text-align:center">using labeled data $X^k$</td>
                            <td style="text-align:center">using full data $X^\ell$</td>
                          </tr>
                        </table>
                    </section>
                    <section data-background-color="antiquewhite">
                        <h3>SSL Does Not Reduce to Classification</h3>

                        <p style="text-align: left"><strong>Example 2: </strong>Classification methods
                            do not account for the cluster structure of unlabeled data</p>

                        <div align="center">
                          <img style="border-radius: 5%" src="../advanced_ml/images/two_moons.jpg" alt="two_moons" />
                        </div>
                    </section>
                    <section>
                        <div class="r-frame">
                            <b>Question 2:</b> Does SSL reduce to clustering?
                        </div>
                    </section>
                    <section data-background-color="antiquewhite">
                        <h3>However, SSL Also Does Not Reduce to Clustering</h3>

                        <p style="text-align: left"><strong>Example 3: </strong>Clustering methods do not consider the priority of labeling over cluster structure</p>

                        <div align="center">
                          <img style="border-radius: 5%" src="../advanced_ml/images/two_moons_2.jpg" alt="two_moons_2" width="700" />
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Clustering Quality Criteria</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p>Suppose <strong>only</strong> pairwise distances between objects are known.</p>

                                <p style="text-align: left">Average intra-cluster distance:</p>
                                $$F_0 = \frac{\sum\limits_{i < j}[a_i = a_j] \rho(x_i, x_j)}{\sum\limits_{i < j}[a_i = a_j]}
                                \to \min$$

                                <p style="text-align: left">Average inter-cluster distance:</p>
                                $$F_1 = \frac{\sum\limits_{i < j}[a_i \neq a_j] \rho(x_i, x_j)}{\sum\limits_{i < j}[a_i \neq a_j]}
                                \to \max$$

                                <p style="text-align: left">Ratio of functionals: $F_0 / F_1 \to \min$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Clustering Quality Criteria in Vector Space</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">
                                    Let the objects \(x_i\) be represented by vectors of features \( (f_1(x_i),\dots,f_n(x_i)) \)</p>

                                <ul>
                                    <li>
                                        <p><b>Sum of average intra-cluster distances:</b></p>
                                        <p>\[ \Phi_0 = \sum\limits_{a \in Y} \frac{1}{|X_a|} \sum\limits_{i: a_i = a} \rho(x_i, \mu_a) \to \min \]</p>
                                        <p>\( X_a = \{x_i \in X^\ell | a_i = a\} \) — cluster \( a \), \( \mu_a \) — centroid of cluster \( a \)</p>
                                    </li>
                                    <li>
                                        <p><b>Sum of inter-cluster distances:</b></p>
                                        <p>\[ \Phi_1 = \sum\limits_{a, b \in Y} \rho(\mu_a, \mu_b) \to \max \]</p>
                                    </li>
                                    <li>
                                        <p><b>Ratio of functionals:</b> $ \Phi_0 / \Phi_1 \to \min $</p>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Clustering as a Discrete Optimization Problem</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Weights on pairs of objects (proximity): \( w_{ij} = \exp(-\beta \rho(x_i, x_j)) \),
                                where \( \rho(x, x^\prime) \) is the distance between objects, \( \beta \) is a parameter</p>

                                <h4  style="text-align: left">Clustering Task:</h4>
                                <p>Find cluster labels \(a_i:\
                                \sum\limits_{i=1}^\ell \sum\limits_{j=i+1}^\ell w_{ij} [a_i \neq a_j] \to \min\limits_{\{a_i \in Y\}}\)</p>

                                <h4 style="text-align: left">Partial Learning Task:</h4>

                                $$ \sum\limits_{i=1}^\ell \sum\limits_{j=i+1}^\ell w_{ij} [a_i \neq a_j] +
                                    {\color{orange}\lambda \sum\limits_{i=1}^k [a_i \neq y_i]}  \to \min\limits_{\{a_i \in Y\}}$$
                                <p style="text-align: left">where \(\lambda\) is another parameter</p>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <img src="../advanced_ml/images/brace-yourself-kmeans.jpg" width="60%">
                    </section>
                    <section>
                        <h3 style="text-align: left">k-means Method for Clustering</h3>
                        <p style="text-align: left">Minimization of the sum of squared intra-class distances:</p>
                        <p>\[ \sum\limits_{i=1}^\ell \|x_i - \mu_{a_i} \|^2 \to \min\limits_{\{a_i\}, \{\mu_a\}},\\
                        \|x_i - \mu_{a} \|^2 = \sum\limits_{j=1}^n (f_j(x_i) - \mu_{a_j})^2 \]</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Lloyd's Algorithm for k-means</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">
                                    <b>Input</b>: \(X^\ell, K = |Y|\)
                                    <br>
                                    <b>Output</b>: cluster centers \( \mu_a \) and labels \( a \in Y \)
                                </p>

                                <ul>
                                    <li>\( \mu_a = \) initial approximation of centers, for all \( a \in Y \)</li>
                                    <li><b>Repeat</b></li>
                                    <ol>
                                        <li><b>First step</b>: assign each \( x_i \) to the nearest center:<br>
                                         \( a_i = \arg\min\limits_{a \in Y} \|x_i - \mu_a \|, i = 1, \dots, \ell \)</li>
                                        <li><b>Second step</b>: calculate new positions of centers:<br>
                                         \( \mu_{a_j} = \frac{\sum\limits_{i = 1}^\ell [a_i = a] x_i}{\sum\limits_{i = 1}^\ell [a_i = a]}, a \in Y \)</li>
                                    </ol>
                                    <li><b>Until</b> \( a_i \) stop changing</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">k-means++</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ol>
                                    <li>Randomly and uniformly select a center \( c_1 \) from \( X \)</li>
                                    <li><b>Repeat \( k - 1 \) times</b><br>
                                        $\ \ \ $ Choose \( x \in X \) as the center of a new cluster with probability \( \frac{D^2(x)}{\sum\limits_{x \in X} D^2(x)} \)</li>
                                    <li>Use the standard k-means algorithm</li>
                                </ol>
                                <p style="text-align: left">* Here \( D(x) \) is the distance from \( x \) to the nearest cluster</p>
                                <hr>
                                <p><em>David Arthur and Sergei Vassilvitskii. k-means++: The Advantages of Careful Seeding, 2007</em></p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Examples of Unsuccessful k-means Clustering</h3>

                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/k-means-1.jpg" alt="k-means-1" height="350" />
                            </div>
                            <div class="column">
                                <img style="border-radius: 5%" src="../advanced_ml/images/k-means-3.jpg" alt="k-means-3" height="350" />
                            </div>
                        </div>

                        <p>Reason: unfortunate initial approximation and "ribbon-like" clusters</p>
                    </section>
                    <section>
                        <h3>k-means Modification for Semi-Supervised Learning</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p><span style="color:orange">Given labeled objects \( \{x_1, \dots, x_k\} \)</span>,
                                    with \( \color{orange}{U} \) being the set of unlabeled objects</p>
                                <p style="text-align: left"><b>Input</b>: \( X^\ell, K = |Y| \). <b>Output</b>: cluster centers \( \mu_a, a \in Y \)</p>
                                <ol>
                                    <li>\( \mu_a = \) initial approximation of centers, for all \( a \in Y \)</li>
                                    <li><b>Repeat</b></li>
                                    <ul>
                                        <li><b>First step</b>: assign each \( x_i \in {\color{orange}U} \) to the nearest center:<br>
                                         \( a_i = \arg\min\limits_{a \in Y} \|x_i - \mu_a \|, i = {\color{orange}k+1, \dots, \ell}\)</li>
                                        <li><b>Second step</b>: calculate new positions of centers:<br>
                                         \( \mu_{a_j} = \frac{\sum\limits_{i = 1}^\ell [a_i = a] x_i}{\sum\limits_{i = 1}^\ell [a_i = a]}, a \in Y \)</li>
                                    </ul>
                                    <li><b>Until</b> \( a_i \) stop changing</li>
                                </ol>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">DBSCAN Clustering Algorithm</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">DBSCAN — Density-Based Spatial Clustering of Applications with Noise</p>
                                <p style="text-align: left">Consider an object \( x \in U \), and its \( \varepsilon \)-neighborhood \( U_\varepsilon(x) = \{ u \in U: \rho(x,u) \leq \varepsilon\} \)</p>
                                <p style="text-align: left">Each object can be one of three types:</p>
                                <ul>
                                    <li>core: having a dense neighborhood, \( |U_\varepsilon (x)| \geq m \)</li>
                                    <li>border: not core, but in the neighborhood of a core</li>
                                    <li>noise (outlier): neither core nor border</li>
                                </ul>
                                <hr>
                                <p>Ester, Kriegel, Sander, Xu. <em>A density-based algorithm for discovering clusters in large spatial databases with noise.</em> KDD-1996</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <img src="../advanced_ml/images/DBSCAN-density-data.png" alt="DBSCAN" width=800 />
                    </section>
                    <section>
                        <h3 style="text-align: left">DBSCAN Clustering Algorithm</h3>
                        <p style="text-align: left"><b>Input</b>: dataset \( X^\ell = \{x_1, \dots, x_\ell\} \), parameters \( \varepsilon \) and \( m \)</p>
                        <p style="text-align: left"><b>Output</b>: partition of the dataset into clusters and noise points</p>
                    </section>
                    <section>
                        <p>\( U = X^\ell \) — unlabeled, \( a = 0 \)</p>
                        <div class="fragment">
                            <div class="typesetting">
                            <p style="text-align: left"><b>While</b> there are unlabeled points in the dataset, i.e. \( U \neq \emptyset \):</p>
                                <ol>
                                    <li>Take a random point \( x \in U \)</li>
                                    <li><b>If</b> \( |U_\varepsilon (x)| < m \) <b>then</b> mark \( x \) as potentially noisy</li>
                                    <li><b>Else</b></li>
                                    <ul>
                                        <li>create a new cluster: \( K = U_\varepsilon(x), a = a + 1 \)</li>
                                        <li><b>for all</b> \( x^\prime \in K \), not marked or noisy</li>
                                        <ul>
                                            <li><b>if</b> \( |U_\varepsilon(x^\prime)| \geq m \) <b>then</b> \( K = K \cup U_\varepsilon(x^\prime) \)</li>
                                            <li><b>else</b> mark \( x^\prime \) as a border of cluster \( K \)</li>
                                        </ul>
                                        <li>\( a_i = a \) for all \( x_i \in K \)</li>
                                        <li>\( U = U/K \)</li>
                                    </ul>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="r-frame">
                            <b>Question 3:</b> What advantages does DBSCAN have?
                        </div>
                        <br><br>
                        <div class="fragment">
                            <div class="typesetting">
                                <h3>Advantages of the DBSCAN Algorithm</h3>
                                <div class="row">
                                    <div class="column">
                                        <ul>
                                            <li>Fast clustering of large data:
                                                <ul>
                                                    <li>\( O(\ell^2) \) in the worst case</li>
                                                    <li>\( O(\ell\ln \ell) \) with efficient implementation of \( U_\varepsilon(x) \)</li>
                                                </ul>
                                            </li>
                                            <li>Clusters of arbitrary shapes (no centers)</li>
                                            <li>Classification of objects into core, border, and noise</li>
                                        </ul>
                                    </div>
                                    <div class="column">
                                        <img style="border-radius: 5%" src="../advanced_ml/images/DBSCAN.jpg" alt="DBSCAN" />
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Agglomerative Hierarchical Clustering</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Algorithm of hierarchical clustering (Lance, Williams, 1967):</p>
                                <p style="text-align: left">
                                    Iterative recalculation of distances \( R_{UV} \) between clusters \( U, V \)</p>
                                <ol>
                                    <li>\( С_1 = \{\{x_1\},\dots, \{x_\ell\}\} \) — all clusters are 1-element<br>
                                    \( R_{\{x_i\}\{x_j\}} = \rho(x_i, x_j) \) — distances between them</li>
                                    <li><b>for all</b> \( t = 2, \dots, \ell \) (\( t \) — iteration number):</li>
                                    <ul>
                                        <li>Find in \( C_{t-1} \) a pair of clusters \( (U, V) \) with minimal \( R_{UV} \)</li>
                                        <li>Merge them into one cluster:<br>
                                        \( W = U \cup V \)<br>
                                        \( С_t = С_{t-1} \cup \{W\}/\{U,V\} \)</li>
                                        <li><b>for all</b> \( S \in C_t \) calculate \(R_{WS}\) using Lance-Williams formula:<br>
                                        \( R_{WS} = \alpha_U R_{US} + \alpha_V R_{VS} + \beta R_{UV} + \gamma |R_{US} - R_{VS}| \)</li>
                                    </ul>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h4>Specific Cases of the Lance-Williams Formula</h4>
                        <div class="row">
                            <div class="column">
                                <p>1. Nearest neighbor distance:</p>
                                <p>\( R^{\text{nn}}_{WS} = \min_{w \in W, \in S} \rho (w,s) \)</p>
                                <p>\( \alpha_{U} = \alpha_{V} = \frac12, \beta = 0, \gamma = -\frac12 \)</p>
                            </div>
                            <div class="column">
                                <img src="../advanced_ml/images/R1.jpg" alt="R1" width="500" />
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <p>2. Farthest neighbor distance:</p>
                                <p>\( R^{\text{fn}}_{WS} = \max_{w \in W, \in S} \rho (w,s) \)</p>
                                <p>\( \alpha_{U} = \alpha_{V} = \frac12, \beta = 0, \gamma = \frac12 \)</p>
                            </div>
                            <div class="column">
                                <img src="../advanced_ml/images/R2.jpg" alt="R2" width="500" />
                            </div>
                        </div>
                        <div class="row">
                            <div class="column">
                                <p>3. Group average distance:</p>
                                <p>\( R^{\text{g}}_{WS} = \frac{1}{|W| |S|} \sum\limits_{w \in W} \sum\limits_{s \in S} \rho(w, s) \)</p>
                                <p>\( \alpha_{U} = \frac{|U|}{|W|}, \alpha_{V} = \frac{|V|}{|W|}, \beta = \gamma = 0 \)</p>
                            </div>
                            <div class="column">
                                <img src="../advanced_ml/images/R3.jpg" alt="R3" width="500" />
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Specific Cases of the Lance-Williams Formula</h3>
                        <ol start="4">
                            <li>Distance between centroids:
                                <p>\( R^{\text{c}}_{WS} = \rho^2\left(\sum\limits_{w \in W} \frac{w}{|W|}, \sum\limits_{s \in S} \frac{s}{|S|} \right) \)</p>
                                <p>\( \alpha_{U} = \frac{|U|}{|W|}, \alpha_{V} = \frac{|V|}{|W|}, \beta = -\alpha_{U}\alpha_{V}, \gamma = 0 \)</p>
                            </li>
                            <li>Ward's distance:
                                <p>\( R^{\text{w}}_{WS} = {\color{orange}\frac{|S||W|}{|S|+|W|}} \rho^2\left(\sum\limits_{w \in W} \frac{w}{|W|}, \sum\limits_{s \in S} \frac{s}{|S|} \right) \)</p>
                                <p>\( \alpha_{U} = \frac{|S|+|U|}{|S|+|W|}, \alpha_{V} = \frac{|S|+|V|}{|S|+|W|}, \beta = \frac{-|S|}{|S|+|W|}, \gamma = 0 \)</p>
                            </li>
                        </ol>
                    </section>
                    <section>
                        <h3 style="text-align: left">Visualization of Cluster Structure</h3>
                        <h4 style="text-align: left">1. Nearest Neighbor Distance</h4>
                        <table style="width:80%">
                          <tr>
                            <th style="text-align:center">Embedding Diagram</th>
                            <th style="text-align:center">Dendrogram</th>
                          </tr>
                          <tr>
                            <td style="text-align:center"><img style="border-radius: 5%" src="../advanced_ml/images/diag1.jpg" alt="D1" width="800" /></td>
                            <td style="text-align:center"><img style="border-radius: 5%" src="../advanced_ml/images/dendr1.jpg" alt="D1" width="800" /></td>
                          </tr>
                        </table>
                    </section>
                    <section>
                        <h4 style="text-align: left">2. Farthest Neighbor Distance</h4>
                        <table style="width:80%">
                          <tr>
                            <th style="text-align:center">Embedding Diagram</th>
                            <th style="text-align:center">Dendrogram</th>
                          </tr>
                          <tr>
                            <td><img style="border-radius: 5%" src="../advanced_ml/images/diag2.jpg" alt="D2" width="800" /></td>
                            <td><img style="border-radius: 5%" src="../advanced_ml/images/dendr2.jpg" alt="D2" width="800" /></td>
                          </tr>
                        </table>
                    </section>
                    <section>
                        <h3>Main Properties of Hierarchical Clustering</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li><b>Monotonicity</b>: the dendrogram has no self-intersections, and with each merge, the distance between the clusters being combined only increases:<br>
                                    \( R_2 \leq R_3 \leq \dots \leq R_\ell \)</li>
                                    <li><b>Contraction Distance</b>: \( R_t \leq \rho(\mu_U, \mu_V), \forall t \)</li>
                                    <li><b>Expansion Distance</b>: \( R_t \geq \rho(\mu_U, \mu_V), \forall t \)</li>
                                </ul>
                                <p style="text-align: left"><b>Theorem</b> (Milligan, 1979): Clustering is monotonic if the conditions
                                \( \alpha_U \geq 0, \alpha_V \geq 0, \alpha_U + \alpha_V + \beta \geq 1, \min\{\alpha_U, \alpha_V\} + \gamma \geq 0 \) are met<br>
                                    <br><br>
                                \( R^\text{c} \) is not monotonic; \( R^\text{nn} \), \( R^\text{fn} \), \( R^\text{g} \), \( \color{orange}{R^\text{w}} \) are monotonic<br>
                                \( R^\text{nn} \) is contracting; \( R^\text{fn}, \color{orange}{R^\text{w}} \) are expanding</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Recommendations about Hierarchical Clustering</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>It is recommended to use Ward's distance \( R^\text{w} \)</li>
                                    <li>Usually, several variants are constructed and the best one is chosen visually based on the dendrogram</li>
                                    <li>The number of clusters is determined by the maximum of \( |R_{t+1} - R_t| \),<br>
                                    then the resulting set of clusters \( = C_t \)</li>
                                </ul>
                                <table style="width:80%">
                                  <tr>
                                    <td><img style="border-radius: 5%" src="../advanced_ml/images/conc1.jpg" alt="conc" height="500" /></td>
                                    <td><img style="border-radius: 5%" src="../advanced_ml/images/conc2.jpg" alt="conc" height="500" /></td>
                                  </tr>
                                </table>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Self-Training Method in Semi-Supervised Learning</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Let \( \mu: X^k \to a \) be a method for learning classification</p>
                                <p style="text-align: left">Classifiers are of the form \( a(x) = \arg\max\limits_{y \in Y} \Gamma_y(x) \)
                                , where \(\Gamma_y(x)\) is raw score</p>
                                <p style="text-align: left"><b>Pseudo-margin</b> — the degree of confidence in the classification \( a_i = a(x_i) \):</p>
                                \[ \boxed{ M_i(a) = \Gamma_{a_i}(x_i) - \max\limits_{y \in Y/a_i} \Gamma_y(x_i)} \]
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Self-Training Algorithm</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p><b>Self-Training Algorithm</b> — a wrapper over the method \( \mu \):</p>
                                <ol>
                                    <li>\( Z = X^k \)</li>
                                    <li><b>While</b> \( |Z| < \ell \)</li>
                                    <ul>
                                        <li>\( a = \mu(Z) \)</li>
                                        <li>\( \Delta = \{x_i \in U/Z | M_i(a) \geq \color{orange}{M_0}\} \)</li>
                                        <li>\( a_i = a(x_i) \) for all \( x_i \in \Delta \)</li>
                                    </ul>
                                    <li>\( Z = Z \cup \Delta \)</li>
                                </ol>
                                <p>\( \color{orange}{M_0} \) can be determined, for example, from the condition \( |\Delta| = 0.05 |U| \)</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Semi-Supervised Learning Method: Co-Learning (deSa, 1993)</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Let \( \mu_t: X^k \to a_t \) be different learning methods, \( t = 1, \dots, T \)</p>
                                <p style="text-align: left"><b>Co-learning Algorithm</b> is self-training for a composition — simple voting of base algorithms \( a_1, \dots, a_T \):</p>
                                \[ \boxed{a(x) = \arg\max\limits_{y in Y} \Gamma_y(x),\ \Gamma_y(x_i) = \sum\limits_{t=1}^T[a_t(x_i) = y]} \]
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Semi-Supervised Learning Method: Co-Training (Blum, Mitchell, 1998)</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Let \( \mu_1: X^k \to a_1,\ \mu_2: X^k \to a_2 \) be two substantially different learning methods, using either:</p>
                                <ul>
                                    <li>different sets of features</li>
                                    <li>different learning paradigms (inductive bias)</li>
                                    <li>different data sources \( X_1^{k_1}, X_2^{k_2} \)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <ol>
                            <li>\( Z_1 = X_1^{k_1} \), \( Z_2 = X_2^{k_2} \)</li>
                            <li><b>While</b> \( |Z_1 \cup Z_2| < \ell \)</li>
                            <ul>
                                <li>\( a_1 = \mu_1(Z_1), \Delta_1 = \{x_i \in U/Z_1/Z_2 | M_i(a_1) \geq {\color{orange}M_{01}}\} \)</li>
                                <li>\( a_i = a_1(x_i) \) for all \( x_i \in \Delta_1 \)</li>
                            </ul>
                            <li>\( Z_2 = Z_2 \cup \Delta_1 \)</li>
                            <ul>
                                <li>\( a_2 = \mu_2(Z_2), \Delta_2 = \{x_i \in U/Z_1/Z_2 | M_i(a_2) \geq {\color{orange}M_{02}}\} \)</li>
                                <li>\( a_i = a_2(x_i) \) for all \( x_i \in \Delta_2 \)</li>
                            </ul>
                            <li>\( Z_1 = Z_1 \cup \Delta_2 \)</li>
                        </ol>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Summary</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>Clustering is unsupervised learning, an ill-posed problem; there exist many optimization and heuristic algorithms for clustering</li>
                                    <li>DBSCAN is a classic and fast clustering algorithm</li>
                                    <li>The task of semi-supervised learning (SSL) occupies an intermediate position
                                        between classification and clustering but does not reduce to one of them</li>
                                    <li>Clustering methods are easily adapted to SSL
                                        by introducing constraints (constrained clustering)</li>
                                    <li>Adapting classification methods is more complex,
                                        involving additional regularization, and typically leads to more effective methods</li>
                                    <li>Regularization combines criteria on labeled and unlabeled data into a single optimization task</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">What else to look at?</p>
                                <ul>
                                    <li><a href="https://www.nature.com/articles/s41467-018-04964-5">SotA-2024 in bioinformatics</a></li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Lance-Williams Algorithm for Semi-Supervised Learning (1967)</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ol>
                                    <li>\( С_1 = \{\{x_1\},\dots, \{x_\ell\}\} \) — all clusters are 1-element<br>
                                    \( R_{\{x_i\}\{x_j\}} = \rho(x_i, x_j) \) — distances between them</li>
                                    <li><b>for all</b> \( t = 2, \dots, \ell \) (\( t \) — iteration number):</li>
                                    <li>Find in \( C_{t-1} \) a pair of clusters \( (U, V) \) with minimal \( R_{UV} \)<br>
                                    <span style="color:orange">under the condition that \( U \cup V \) does not contain objects with different labels</span></li>
                                    <li>Merge them into one cluster:<br>
                                    \( W = U \cup V \), \( С_t = С_{t-1} \cup \{W\}/\{U,V\} \)</li>
                                    <li><b>for all</b> \( S \in C_t \) calculate \( R_{WS} \) using the Lance-Williams formula:<br>
                                    \( R_{WS} = \alpha_U R_{US} + \alpha_V R_{VS} + \beta R_{UV} + \gamma |R_{US} - R_{VS}| \)</li>
                                </ol>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>