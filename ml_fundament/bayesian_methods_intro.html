<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Bayesian methods intro</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
		<script defer src="https://pyscript.net/latest/pyscript.js"></script>
		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="../ml_with_python/images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
						<h2>Fundamentals of Machine Learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>Bayesian methods intro</h3>
								<br />
								Alex Avdiushenko <br />
								December 17, 2024
							</div>
					</div>
                </section>
                <section>
                    <section>
                        <h3>Conditional Probability</h3>

                        <p>$P(y|x) =\frac{P(x, y)}{P(x)}$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <h4 style="text-align: left">Example</h4>
                                <p>The probability of rolling a 6 on a die given that an even number has rolled.</p>
                                <p>$P(6|\mathrm{even}) = \frac{P(\mathrm{even},\ 6)}{P(\mathrm{even})} = \frac13$</p>
                                <p style="text-align: left">Symmetry: $$P(y|x)P(x) = P(x, y) = P(x|y)P(y)$$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Marginalization</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p>\[ p(y|x)p(x) = p(x, y) = p(x|y)p(y) \Rightarrow \]</p>
                                <p>\[ \Rightarrow p(y|x) = \frac{p(x|y)p(y)}{p(x)} \]</p>
                                <p>\[ p(x) = \int p(x, y)dy = \int p(x|y)p(y)dy = \mathbb{E}_y p(x|y)\]</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Rules of Sum and Product</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Sum Rule:</strong></p>
                                \[p(x_1, \dots , x_k) = \int p(x_1, \dots , x_k, \dots , x_n)d{x_{k+1}} \dots d{x_n}\]

                                <p style="text-align: left"><strong>Product Rule:</strong></p>
                                \[ p(x_1, x_2, x_3, \dots , x_{n−1}, x_n) = p(x_1|x_2, x_3, \dots, x_{n−1}, x_n) \cdot \\
                                    \cdot p(x_2|x_3, \dots, x_{n−1}, x_n) \cdot \ldots \cdot p(x_{n}) \]
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Bayes' Theorem</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                \[
                                \begin{cases}
                                p(y|x) = \frac{p(x|y)p(y)}{p(x)} \\
                                p(x) = \int p(x|y)p(y)dy
                                \end{cases}
                                \Rightarrow
                                \]

                                \[
                                p(y|x) = \frac{p(x|y)p(y)}{\int p(x|y)p(y)dy}
                                \]

                                \[
                                \mathrm{Posterior} = \frac{\mathrm{Likelihood}\cdot \mathrm{Prior}}{\mathrm{Evidence}}
                                \]
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Example with a Friend and an Exam</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left"><strong>Given:</strong></p>

                                <ul>
                                <li>$x$ — student is (not) happy</li>
                                <li>$y$ — exam is (not) passed</li>
                                <li>$P(\text{exam is passed}) = 1/3$</li>
                                <li>$P(\text{student is happy}|\text{exam is passed}) = 1$</li>
                                <li>$P(\text{student is happy}|\text{exam is NOT passed}) = 1/6$</li>
                                </ul>

                                <p style="text-align: left"><strong>Find:</strong></p>
                                <p>$P(y=1|x=1) = P(\mathrm{exam\ is\ passed}|\mathrm{student\ is\ happy})\ —\ ?$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Solution:</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                \[
                                P(\text{exam is passed}|\text{student is happy}) =
                                \\[0.1cm]
                                = \frac{P(\text{student is happy}|\text{exam is passed})P(\text{exam is passed})}{P(\text{student is happy})} =
                                \\[0.2cm]
                                = \frac{P(\text{student is happy}|\text{exam is passed})P(\text{exam is passed})}{P(\cdot,\ \text{exam is passed})
                                + P(\cdot,\ \text{exam is NOT passed})} =
                                \\[0.2cm]
                                =
                                \frac{P(\text{student is happy}|\text{exam is passed})P(\text{exam is passed})}{
                                P(\cdot|\cdot)P(\text{exam is passed}) + P(\cdot|\cdot)P(\text{exam is NOT passed})
                                } =
                                \\[0.1cm]
                                =\frac{\frac{1}{3}}{\frac{1}{3} + \frac{1}{6}\cdot\frac{2}{3}} = \frac{3}{4}
                                \]
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Graphical Models</h3>
                            $$p(x_1, x_2, x_3, x_4, x_5) = \\ p(x_1 | x_3) p(x_2 | x_4)p(x_3 | x_4, x_5)p(x_4)p(x_5)$$
                            <img style="border-radius: 5%" src="../ml_with_python/images/graph.jpg" alt="graph" width="300" />
                    </section>
                    <section>
                        <div class="row">
                            <div class="column">
                                <br><br>
                                For example, what influences a friend's mood?
                            </div>
                            <div class="column">
                                <img style="border-radius: 5%" src="../ml_with_python/images/glad_student.jpg" alt="glad_student" width="500" />
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Maximum Likelihood Method (frequentist approach)</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                $$X = \{x_1, x_2, \dots, x_n\}, x_i \sim p(x|\theta)$$

                                $$\theta_{\text{MaxL}} = \arg\max\limits_\theta p(X|θ) {\color{orange}=} \arg\max\limits_\theta \prod\limits_{i=1}^n p(x_i|\theta)
                                \\
                                = \arg\max\limits_\theta \sum\limits_{i=1}^n \log p(x_i|\theta)$$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h4 style="text-align: left">Properties of MaxL:</h4>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>MaxL estimate is consistent: $$\theta_{\text{MaxL}} \overset{p}{\to} \theta_{\text{ground truth}}$$</li>
                                    <li>Asymptotically efficient: there is no consistent estimate that improves mean-square error as $n \to \infty$</li>
                                    <li>Asymptotically normal: $$\sqrt{n} \left(\theta_{\text{MaxL}} - \theta_{\text{ground truth}}\right) \overset{d}{\to}
                                        N(0, \text{FI}^{-1})$$</li>
                                </ul>
                                $\text{FI} = -\lim\limits_{n \to \infty} \frac1n \mathbb{E}(\text{Hessian})$ — asymptotic Fisher information matrix
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Bayesian approach to the problem of machine learning</h3>
                        $$p(\theta|X) = \frac{p(X|\theta)p(\theta)}{\int p(X|\theta)p(\theta)d\theta}$$
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <div class="r-frame important">
                                    <b>Question 1: </b>What are the pros and cons of the frequentist and Bayesian approaches?
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Comparison of Approaches</h3>
                        <table style="width:95%">
                          <tr>
                            <th>$\quad\quad\quad\quad$</th>
                            <th>Frequentist Approach (Fischer)</th>
                            <th>Bayesian Approach</th>
                          </tr>
                        </table>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <table style="width:95%">
                            <tr>
                                <th>Relationship to Randomness</th>
                                <td>Cannot predict</td>
                                <td>Can predict given enough information</td>
                            </tr>
                            </table>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <table style="width:95%">
                                  <tr>
                                    <th>Values            $\quad\quad\quad$</th>
                                    <td>Random and deterministic</td>
                                    <td>All random</td>
                                  </tr>
                             </table>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <table style="width:95%">
                                  <tr>
                                    <th>Inference Method</th>
                                    <td>Maximum likelihood</td>
                                    <td>Bayes' Theorem</td>
                                  </tr>
                            </table>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <table style="width:95%">
                                  <tr>
                                    <th>Parameter Estimates</th>
                                    <td>Point estimates</td>
                                    <td>Posterior distribution</td>
                                  </tr>
                            </table>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <table style="width:95%">
                              <tr>
                                <th>Use Cases</th>
                                <td>$n \gg d$</td>
                                <td>Always</td>
                              </tr>
                            </table>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Relationship Between Frequentist and Bayesian Approaches</h3>
                        $$\lim\limits_{n \to \infty} p(\theta|x_1, \dots, x_n) = \delta(\theta - \theta_{\text{MaxL}})$$
                        $\delta$ — Dirac delta function
                    </section>
                    <section>
                        <h3>Advantages of the Bayesian Approach</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li>Regularization</li>
                                  <li>Compositeness. For example, the popularity of topics on social networks —
                                      first, we calculated based on the news feed, then refined using video data</li>
                                  <li>On-the-fly data processing iteratively updates the posterior distribution</li>
                                  <li>We can calculate various statistics</li>
                                  <li>We can estimate the confidence in the values of model parameters</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#ffffff">
                        <h3>Confidence Estimation</h3>
                        <img src="../ml_with_python/images/robustness.png" alt="robustness">
                    </section>
                    <section>
                        <h3>Computational Difficulties</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                $$p(\theta|X) = \frac{p(X|\theta)p(\theta)}{\underbrace{\int p(X|\theta)p(\theta)d\theta}_{\text{multidimensional integral}}}$$
                                <h4 style="text-align: left">Calculation Methods</h4>
                                <ul>
                                  <li>Analytical calculation</li>
                                  <li>Markov chain Monte Carlo (MCMC)</li>
                                  <li><span style="color: gray">Variational inference (not covered in the first part of the course)</span></li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Analytical Approach (conjugate distributions)</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p>Let $p(x|\theta) = A(\theta)$ and $p(\theta) = B(\alpha)$</p>

                                <p class="r-frame" style="text-align: left;padding-left: 20px;">
                                    The prior distribution of parameters $p(\theta)$ is called
                                    conjugate to $p(x|\theta)$ (conjugate priors),
                                    if the posterior distribution belongs to the same family as the prior
                                    $p(\theta|x) = B(\alpha^\prime)$</p>
                                <br>
                                <p style="text-align: left">Knowing the formula for the family to which our posterior distribution should belong,
                                    it is easy to calculate its normalizing factor (the integral in the denominator)</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Example 1</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p>Posterior distribution of the expected value of a normal distribution</p>
                                <p style="text-align: left">Normal distribution:</p>
                                $N(x|\mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2} \right)$

                                <div class="row">
                                    <div class="column">
                                        <p style="text-align: left">Likelihood:</p>
                                        $p(x|\mu) = N(x|\mu, 1) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{(x-\mu)^2}{2} \right)$

                                        <p style="text-align: left">Conjugate prior:</p>
                                        $p(\mu) = N(\mu|m,s^2)$
                                    </div>
                                    <div class="column">
                                        <img style="border-radius: 5%" src="../ml_with_python/images/gauss_graph.jpg" alt="gauss_graph" width=400 />
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Example 2</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p>Posterior probability of getting heads when flipping a biased coin</p>
                                <h4 style="text-align: left">Frequentist approach</h4>
                                <p>$q$ — the probability of heads equals the fraction of heads in $n$ flips</p>

                                <h4 style="text-align: left">Likelihood</h4>
                                <p style="text-align: left">Binomial distribution: $p(x|q) = C_n^x q^x (1-q)^{n-x}$</p>

                                <h4 style="text-align: left">Conjugate prior</h4>
                                <p style="text-align: left">Beta distribution:
                                    $p(q| \alpha, \beta) = \frac{q^{\alpha-1} (1-q)^{\beta-1}}{B(\alpha, \beta)}$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">The numerator of the posterior distribution:</p>
                        $$q^{\alpha-1+x} (1-q)^{\beta-1+(n-x)} = q^{\alpha^\prime-1} (1-q)^{\beta^\prime-1},$$
                        <p style="text-align: left">where $\alpha^\prime = \alpha + x, \beta^\prime = \beta + n - x$, so essentially,
                            we also know the denominator, it equals $B(\alpha^\prime, \beta^\prime)$. </p>

                        <h4 style="text-align: left">Posterior distribution</h4>
                        $$p(q|x) = \frac{q^{\alpha-1+x} (1-q)^{\beta-1+(n-x)}}{B(\alpha + x, \beta + n - x)}$$
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Markov chain Monte Carlo</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <h4 style="text-align: left">Motivation</h4>
                                <ul>
                                    <li>We'd like to be able to calculate mathematical expectations and
                                        other quantities according to the posterior distribution, known up to a constant</li>
                                    <li>For example, we'd like to know the response of an ensemble consisting of an infinite number of algorithms :)</li>
                                    <li>We can sum up (integrate) the expected responses of the classifier,
                                        according to the probability of the model parameters</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Monte Carlo Method</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">A broad class of computational algorithms based on the repeated execution
                                    of random experiments and the subsequent calculation of quantities of interest.</p>
                                $$\int f(x)dx \approx \frac{1}{n} \sum\limits_{i=1}^n f(x_i)$$
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>Invented by Stanislaw Ulam in 1949 when he was playing Solitaire while ill and
                                        wondered what is the probability of being able to play out all the cards?
                                        He didn't solve it combinatorically, but simply by the share of successful games</li>
                                    <li>The method was named after the Monte Carlo casino in Monaco for the sake of secrecy
                                        since it was used in the Manhattan Project for the development of nuclear weapons in the USA</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Example of Using the Monte Carlo Method</h3>
                        <div class="row">
                            <div class="column">
                                $$\pi = 4 E[x^2 + y^2 < 1] \approx
                                \\
                                \frac{1}{n} \sum\limits_{i=1}^n [x_i^2 + y_i^2 < 1],
                                \\
                                x_i, y_i \sim U(0, 1)$$
                            </div>
                            <div class="column">
                                <img style="border-radius: 5%" src="../ml_with_python/images/monte_carlo.jpg" alt="monte_carlo" width=400 />
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Markov Chains</h3>
                        <div class="row">
                            <div class="column">
                                <br>
                                <ul>
                                    <li>Markov chains are used for modeling many probability distributions</li>
                                    <li>That is, it's possible to construct a process where the sequence $x$
                                        will appear to have been generated from a probability distribution</li>
                                </ul>
                            </div>
                            <div class="column">
                                <img style="border-radius: 5%" src="../ml_with_python/images/MarkovAA.jpg" alt="MarkovAA" width=600 />
                                <p>Andrey Andreyevich Markov (senior), 1906</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p>The sequence $x_1, x_2, \dots, x_{n-1}, x_n$ is
                            a <i>Markov chain</i>, if</p>

                        <p>$$p(x_1, x_2, \dots, x_{n-1}, x_n) = p(x_n|x_{n-1}) \dots p(x_2|x_1)p(x_1)$$</p>
                    </section>
                    <section>
                        <h3>MCMC in Calculating Mathematical Expectations</h3>

                        <p style="text-align: left">In this case, we can use the Monte Carlo method to calculate mathematical expectations for
                            this probability distribution</p>
                        $$E_{p(x)} f(x) \approx \frac{1}{n} \sum\limits_{i=1}^n f(x_i),$$
                        <p style="text-align: left">where $x_i \sim p(x)$</p>

                        <div class="fragment" style="margin-top:20px;">
                            <div class="typesetting">
                                <div class="r-frame important">
                                    <b>Question 2:</b> How to model distributions using Markov chains?
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Gibbs Sampling (1984)</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <div class="row">
                                    <div class="column">
                                        <br>
                                        <p style="text-align: left">An instance of the Metropolis-Hastings algorithm,
                                            <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">named after</a>
                                            the distinguished scientist Josiah Gibbs.</p>
                                        <p style="text-align: left">Suppose we derived the posterior distribution only up to a normalization constant:</p>
                                        $$p(x_1, x_2, x_3) = \frac{\hat p(x_1, x_2, x_3)}{C}$$
                                    </div>
                                    <div class="column">
                                        <img style="border-radius: 5%" src="../ml_with_python/images/Gibbs.jpeg" alt="Gibbs" width=400 />
                                        <p>Josiah Willard Gibbs</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p>We start with $(x_1^0, x_2^0, x_3^0)$, and within a loop, we perform the following procedure:</p>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ul>
                              <li>$x_1^1 \sim \hat p(x_1 | x_2 = x_2^0, x_3 = x_3^0)$</li>
                              <li>$x_2^1 \sim \hat p(x_2 | x_1 = x_1^1, x_3 = x_3^0)$</li>
                              <li>$x_3^1 \sim \hat p(x_3 | x_1 = x_1^1, x_2 = x_2^1)$</li>
                              <li>$x_1^2 \sim \hat p(x_1 | x_2 = x_2^1, x_3 = x_3^1)$ etc.</li>
                            </ul>
                            <p>After a few iterations "the chain has warmed up",
                                we can estimate expectations based on these samples.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Sampling from a One-Dimensional Distribution</h3>

                        <p style="text-align: left">Rejection Sampling</p>

                        <div class="row">
                            <div class="column">
                                <p style="text-align: left">\(i = 0\), repeat many times:</p>
                                <ul>
                                    <li>sample \(y \sim q(y)\)</li>
                                    <li>sample \(\xi \sim U[0;Cq(y)]\)</li>
                                    <li><b>if</b> \(\xi < \hat p(y)\):
                                        <ul>
                                            <li>\(x_i = y\)</li>
                                            <li>\(i = i+1\)</li>
                                        </ul>
                                    </li>
                                </ul>
                            </div>
                            <div class="column">
                                <img style="border-radius: 5%" src="../ml_with_python/images/sampling_1D.jpg" alt="sampling_1D" width="400" />
                            </div>
                        </div>

                        <p style="text-align: left">The difficulty lies in choosing the parameter \(C\), it's often too large.</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Summary</h3>

                        <ul>
                            <li>The frequentist approach is a limiting case of the Bayesian approach.</li>
                            <li>The Bayesian approach has many nice properties:
                                <ul>
                                    <li>Regularization</li>
                                    <li>Composability</li>
                                    <li>Processing data "on the fly"</li>
                                    <li>Ability to calculate various statistics</li>
                                    <li>Ability to estimate confidence</li>
                                </ul>
                            </li>
                            <li>However, it is often computationally expensive.</li>
                            <li>It is possible to use some advantages of the Bayesian approach without calculating the normalizing constant.</li>
                        </ul>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>