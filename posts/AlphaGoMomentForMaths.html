<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AlphaGo Moment for Olympiad Maths</title>

    <link rel="stylesheet" href="./css/style.css">

    <!-- MathJax for mathematical formulas -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            },
            startup: {
                ready: function () {
                    MathJax.startup.defaultReady();
                    // Add copy buttons after MathJax is ready
                    setTimeout(addMathCopyButtons, 100);
                }
            }
        };
    </script>

    <!-- Prism.js for syntax highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />

    <!-- Open Graph meta-tags for better social sharing -->
    <meta property="og:title" content="LLM Math Reasoning Evaluation">
    <meta property="og:description" content="This experiment benchmarks state-of-the-art large language models on university-level math contest problems from the International Mathematics Competition (IMC).">
    <meta property="og:url" content="">
    <meta property="og:type" content="article">
    <meta property="og:image" content="https://your-domain.com/path-to-preview-image.jpg">

    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="LLM Math Reasoning Evaluation">
    <meta name="twitter:description" content="Benchmarking AI models on IMC 2025 problems">
    <meta name="twitter:image" content="https://your-domain.com/path-to-preview-image.jpg">

</head>
<body>
    <script src="./js/utils.js"></script>
    <!-- Prism.js Scripts -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <div class="container">
        <header class="header">
            <h1 class="title">AlphaGo Moment for Olympiad Maths</h1>
            <p class="subtitle">LLM Math Reasoning Evaluation Based on <a href="https://www.imc-math.org.uk/?year=2025&item=problems">IMC 2025 problems</a></p>
            <div class="meta">
                <span class="author">Alex Avdiushenko</span>
                <span>•</span>
                <span class="date">August 11, 2025</span>
                <span>•</span>
                <span>10 min read</span>
            </div>
        </header>

        <article class="content">
            <h2>Introduction</h2>
            <p>
                For several years now, I’ve been preparing a team from Neapolis University Pafos (Cyprus)
                and serving on the Jury of the International Mathematics Competition (IMC) for university students.
                <br>
                Naturally in parallel, it’s also fascinating to follow, the progress of LLMs (Large Language Models) in solving extremely
                difficult Olympiad-level math problems. Beyond its scientific significance,
                mathematics also serve as a nearly universal system for testing intellectual ability in children:
                in almost every country, a mathematics exam is a mandatory requirement for university admission.
                The advantages are obvious: problems of varying difficulty can be solved quickly using pen and paper,
                and the grading criteria are relatively objective.
            </p>
            <h3>A bit of History</h3>
            <p>
                Three years ago, even the most advanced models couldn’t solve anything from the IMC,
                although the first public version of ChatGPT (based on GPT‑3.5, released by OpenAI on November 30, 2022)
                could already handle simpler math problems.
                <br>
                In January 2024, <a href="https://www.nature.com/articles/s41586-023-06747-5">Google DeepMind
                published AlphaGeometry</a>
                a specialized system with Reinforcement Learning and Symbolic Deduction focused on solving
                high school-level geometry problems from the IMO (International Mathematical Olympiad).
                <br>
                A year later, in January 2025, another specialized system <a href="https://openreview.net/forum?id=FiyS0ecSm0">LIPS</a>
                appeared, designed to tackle challenging Olympiad-style inequalities —
                and also general-purpose LLMs kept getting smarter too.
                <br>
                Then recently came two major announcements in July 2025, just two days apart:
                both <a href="https://x.com/OpenAI/status/1946594928945148246">OpenAI</a>
                and <a href="https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/">Google DeepMind</a>
                models had achieved <strong>gold medal performance</strong> at the IMO level!
                <br>
                Reading the news about all this is one thing —
                but testing the current capabilities of the strongest publicly
                available models firsthand is something else entirely.
            </p>
            <h2>Fair Experimental Setup</h2>
            <p>
                This experiment will benchmark state-of-the-art LLMs
                on university-level math Olympiad problems
                from the IMC-2025.
                The goal is to assess each model's reasoning capabilities and problem-solving performance
                under conditions similar to the human competition.
                We presented the same 10 problems (5 per round, 2 rounds) to each model
                and evaluated their solutions rigorously. As at real IMC,
                together with <a href="https://www.linkedin.com/in/andrei-smolensky/">Andrei Smolensky</a>,
                who has also been participating in the work of the IMC Jury for a few years,
                we perform blind double-checking and adjusting the final mark for each problem
                after discussion between the judges.
                <br>
                By mimicking contest conditions (no prior exposure to the problems,
                independent work), we aim to see how close these models
                come to human-level performance on challenging math problems.
                <br>
                To ensure the results are comparable and unbiased,
                we carefully controlled the experimental setup and created a Python script for solution generation.
                The script is available <a href="https://github.com/avalur/avalur.github.io/blob/master/posts/LLM-math-reasoning-eval/imc_math_llm_eval.py">on GitHub</a>.
                <br>
                <ul>
                    <li>We provided each problem in a clear LaTeX format. This guarantees all models can read the problem easily.
                Each model receives the problem via an identical prompt structure.</li>
                    <li>We used a system message to prime the model with general problem-solving instructions
                and a user message containing the problem statement.
                This two-part prompt (detailed in the script above) is the same
                for GPT, DeepSeek, Gemini, Claude, and Grok tailored only as needed to fit each platform's format.</li>
                    <li>We run each reasoning model 3 times, and then ask the same model
                    to read through all 3 solution variants and come up with the final version.
                    Sometimes we got only 2 variants, or even 1 due to the token limits for thinking,
                    but to be honest, even at real IMC, some solutions occasionally get lost at the beginning
                    and are only checked later during the appeals process.
                    We marked all the solution variants and set the maximum score
                    among all solutions.
                    At a real IMC, usually only one solution in the final version is checked at first,
                    but often team leaders come to the appeal with drafts and explanations,
                    from which a more complete solution to the problem can be assembled =)</li>
                    <li>We allow the models to perform multi-step reasoning or use tools
                    (like a scratchpad, calculator, or code execution) to solve problems,
                    but we ask them not to use internet search.
                    All models run in a single-turn (single prompt) setting –
                    meaning we present the problem and the model must produce a solution in one go,
                    possibly with internal chain-of-thought, but without any further user feedback.</li>
                    <li>Each problem is given in a fresh session
                    and context for the model, so it has no memory of other problems or its own previous answers.
                    This prevents any carry-over of information.</li>
                </ul>
            </p>

            <h2>Problem Example</h2>
            <p><strong>Problem 2, First Day.</strong>
            Let $f:\mathbb{R}\to\mathbb{R}$ be a twice continuously differentiable
                function, and suppose that</p>
            <div class="math-block">
                <button class="math-copy-btn" onclick="copyMathToClipboard(this)">Copy LaTeX</button>
                <div class="math-content">
                    $$\int_{-1}^{1} f(x)\,\mathrm{d}x = 0 \quad\text{and}\quad f(1) = f(-1) = 1$$
                </div>
                <div class="math-latex-source" style="display: none;" data-latex="\int_{-1}^{1} f(x)\,\mathrm{d}x = 0 \quad\text{and}\quad f(1) = f(-1) = 1"></div>
            </div>
            <p>Prove that
            <div class="math-block">
                <button class="math-copy-btn" onclick="copyMathToClipboard(this)">Copy LaTeX</button>
                <div class="math-content">
                    $$\int_{-1}^{1} \bigl(f''(x)\bigr)^2 \,\mathrm{d}x \;\ge\; 15,$$
                </div>
                <div class="math-latex-source" style="display: none;" data-latex="\int_{-1}^{1} \bigl(f''(x)\bigr)^2 \,\mathrm{d}x \;\ge\; 15,"></div>
            </div>
            and find all such functions for which equality holds.
            You can look at all ten problems <a href="https://github.com/avalur/avalur.github.io/blob/master/posts/LLM-math-reasoning-eval/IMC2025problems.pdf">here</a>.</p>

            <h2>Results</h2>
            <div class="table-container">
                <table class="results-table">
                    <thead>
                    <tr>
                        <th>Model</th>
                        <th> P1</th>
                        <th> P2</th>
                        <th> P3</th>
                        <th> P4</th>
                        <th> P5</th>
                        <th> P6</th>
                        <th> P7</th>
                        <th> P8</th>
                        <th> P9</th>
                        <th>P10</th>
                        <th>Sum</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>gemini-2.5-pro</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>100</td>
                    </tr>
                    <tr>
                        <td>o3-2025-04-16</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>3</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>93</td>
                    </tr>
                    <tr>
                        <td>claude-opus-4-20250514</td>
                        <td>10</td>
                        <td>2</td>
                        <td>10</td>
                        <td>1</td>
                        <td>10</td>
                        <td>10</td>
                        <td>0</td>
                        <td>10</td>
                        <td>10</td>
                        <td>2</td>
                        <td>65</td>
                    </tr>
                    <tr>
                        <td>grok-4</td>
                        <td>10</td>
                        <td>10</td>
                        <td>10</td>
                        <td>1</td>
                        <td>8</td>
                        <td>10</td>
                        <td>2</td>
                        <td>10</td>
                        <td>1</td>
                        <td>2</td>
                        <td>64</td>
                    </tr>
                    <tr>
                        <td>deepseek-R1-0528</td>
                        <td>10</td>
                        <td>2</td>
                        <td>10</td>
                        <td>1</td>
                        <td>10</td>
                        <td>10</td>
                        <td>0</td>
                        <td>8</td>
                        <td>1</td>
                        <td>-</td>
                        <td>52</td>
                    </tr>
                    </tbody>
                </table>
            </div>
            AI team score (best 3 + average) is <strong>332.80</strong>.
            If such an AI team took part in the unofficial team competition of IMC-2025,
            it would take a confident second place, losing only to one human team
            from St. Petersburg State University =)

            <div class="image-container">
                <img src="./LLM-math-reasoning-eval/Top2human_teams.png" alt="Top two teams from the IMC-2025">
                <div class="image-caption">TOP two results of human teams, IMC-2025</div>
            </div>

            <h2>Conclusion</h2>
            <p>Summer 2025 for Olympiad Mathematics is obviously analogous
                to Spring 1997 for Chess and Spring 2016 for Go:
                from now on, even the best humans cannot outperform AI
                in the highly specialized solution of Olympiad Mathematics problems.
                In recent years, it has been clear that this moment would inevitably come,
                but for some reason I still personally feel a little sad that this milestone has fallen.
                <br>
                We can rejoice in the new possibilities that such tools open up for us,
                exploring new depths and discovering new facets of mathematics with their help.
            </p>

            <h2>Some takeaways</h2>
            <ol>
                <li>It was fascinating to read <a href="https://github.com/avalur/avalur.github.io/blob/master/posts/LLM-math-reasoning-eval/ai_solutions/deepseek_thinking_pr2.pdf">DeepSeek’s thought process</a> while solving the problems.</li>
                <li>DeepSeek thinks a lot and for a long time, which actually hinders it on olympiad-style problems.</li>
                <li>Gemini writes very clearly and reasonably, but despite this for problem 7, for example,
                    only its second attempt was correct.</li>
                <li>The hardest problems for LLMs were #4, #7, and #10 —
                    they don’t rely on a single clever idea, but rather require multiple steps
                    and deeper reasoning.</li>
                <li>Overall, checking <a href="https://github.com/avalur/avalur.github.io/blob/master/posts/LLM-math-reasoning-eval/ai_solutions/">AI solutions</a>
                    feels eerily similar to grading students' work..</li>
                <li>Even though I asked not to use the internet search via the system prompt,
                    I'm not sure that all LLMs listened =)
                    What's more, on the contrary, I'm sure that some of them used the search,
                    but at the same time I'm also sure that we owe the main advances and final scores
                    to the increasingly "smarter" LLMs, and not to simple cheating.</li>
                <li>Such a simple Python script and a few API keys find solutions to even very complex problems.</li>
            </ol>
            <h2>A bit of self-promotion</h2>
            <p>From time to time I plan to write similar thoughts
                in the Telegram channel <a href="https://t.me/TechneNotes">@TechneNotes</a>,
                so join me if you liked it.
            </p>
        </article>

        <!-- Social Sharing Section -->
        <div class="social-sharing">
            <div class="share-buttons">
                <a href="https://www.facebook.com/sharer/sharer.php?u=" class="share-btn facebook" onclick="shareOnFacebook(); return false;" title="Share on Facebook">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"/>
                    </svg>
                    Facebook
                </a>
                
                <a href="https://www.linkedin.com/sharing/share-offsite/?url=" class="share-btn linkedin" onclick="shareOnLinkedIn(); return false;" title="Share on LinkedIn">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                    LinkedIn
                </a>
                
                <a href="https://twitter.com/intent/tweet?url=&text=LLM%20Math%20Reasoning%20Evaluation" class="share-btn twitter" onclick="shareOnTwitter(); return false;" title="Share on X (Twitter)">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/>
                    </svg>
                    X (Twitter)
                </a>
            </div>
        </div>
    </div>

    <!-- Toast notification element -->
    <div id="copyToast" class="copy-toast"></div>

    <script>
        // Social sharing functions
        function shareOnFacebook() {
            const url = encodeURIComponent(window.location.href);
            window.open(`https://www.facebook.com/sharer/sharer.php?u=${url}`, '_blank', 'width=600,height=400');
        }

        function shareOnLinkedIn() {
            const url = encodeURIComponent(window.location.href);
            window.open(`https://www.linkedin.com/sharing/share-offsite/?url=${url}`, '_blank', 'width=600,height=400');
        }
        
        function shareOnTwitter() {
            const url = encodeURIComponent(window.location.href);
            const text = encodeURIComponent(document.title);
            window.open(`https://twitter.com/intent/tweet?url=${url}&text=${text}`, '_blank', 'width=600,height=400');
        }
    </script>

</body>
</html>