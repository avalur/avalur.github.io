{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G40l9lO2ObLr"
   },
   "source": "## Homework: EM and neural networks theory (100 points max)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve the proposed problems. Each task must be thoroughly substantiated, tasks without\n",
    "justification are not counted. Solutions are written in a free form, however, so that the reviewer can understand. If the reviewer couldn't understand the solution to some problem,\n",
    "it is automatically not considered. If any external sources are used, they must be indicated.\n",
    "\n",
    "Please, don't use ChatGPT and other AI assistants for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem 1 (20 points). EM algorithm and mixture distributions\n",
    "\n",
    "Consider a model of a mixture of two one-dimensional Gaussian distributions:\n",
    "    $$p(x) = w_1N(x|\\mu_1, \\sigma_1^2) + w_2N(x| \\mu_2, \\sigma_2^2).$$\n",
    "\n",
    "Given a sample $X^\\ell = \\{x_1, \\dots, x_\\ell\\}$.\n",
    "\n",
    " * (10 points) Prove that the likelihood of this sample in this model is unbounded\n",
    " * (10 points) Assuming that the variances $\\sigma_1^2$ and $\\sigma_2^2$ are known, introduce latent variables into the model and derive the formulas for the EM algorithm steps to adjust the parameters $w_1, w_2, \\mu_1, \\mu_2$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem 2 (10 points). Neural Networks\n",
    "\n",
    "Implement the following Boolean function of three variables using a neural network:\n",
    "$$\n",
    "        f(x) =\n",
    "        (\\bar x_1 \\vee x_3) \\& (x_1 \\vee x_2 \\vee x_3).\n",
    "$$\n",
    "\n",
    "All neurons should use the activation function $\\sigma(x) = [x > 0]$."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem 3 (20 points). word2vec, backpropagation\n",
    "\n",
    "Consider a neural network for the `word2vec` semantic similarity model. For simplicity, we'll assume that for a word $x$, one word $y$ from its context is predicted. This task can be interpreted as a multiclass classification problem, where the classes are words from the vocabulary.\n",
    "\n",
    "The neural network receives as input a word $x$ from vocabulary $V$, encoded using a vector of dimension $|V|$ consisting of zeros and one 1 (one-hot encoding):\n",
    "$$\n",
    "    x = (0, 0, \\dots 0, 1, 0, \\dots 0)_{|V|}\n",
    "$$\n",
    "\n",
    "A fully connected layer with a weight matrix $W$ of dimension ${|V|\\times d}$ transforms the vector $x$ into a hidden representation $h$ of some dimension $d \\approx 300$:\n",
    "$$\n",
    "    h = x W\n",
    "$$\n",
    "\n",
    "There is no activation function, and another fully connected layer with a weight matrix $W'$ of dimension ${d \\times |V|}$ transforms the hidden representation into a vector of scores $a$ for each class, in this case - for each word:\n",
    "$$\n",
    "    a = h W'\n",
    "$$\n",
    "\n",
    "To obtain probabilities from these scores, SoftMax is used. For example, the probability of encountering the $j$-th word of the vocabulary in the context of word $x$ according to the neural network looks like this:\n",
    "$$\n",
    "    p_j = \\frac{\\exp(a_j)}{\\sum_{k=1}^{|V|} \\exp(a_k)}\n",
    "$$\n",
    "\n",
    "Cross-entropy loss is used as the loss function:\n",
    "$$\n",
    "    \\mathcal{L} = - \\sum_{j=1}^{|V|} y_j \\log p_j,\n",
    "$$\n",
    "where $y$ is the one-hot encoding for the true word from the context.\n",
    "\n",
    "So, we have fully described the forward pass through the neural network: how to find class probabilities $p_j$ from the input vector $x$ and calculate the value of the loss function, knowing the answer $y$ for the example under consideration.\n",
    "\n",
    "Describe the backward pass through the neural network: write out the formulas for updating the weight matrices $W$ and $W'$ in stochastic gradient descent for the backpropagation method."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem 4 (30 points). Optimal rewards\n",
    "\n",
    "Suppose we are solving the problem of displaying advertisements on a website.\n",
    "There are a total of $n$ advertisements with certain rewards.\n",
    "The rewards are selected from a probability distribution with density $p(x)$, $x > 0$, known to us, and are fixed before the experiment begins; however, the actual rewards are not known to us in advance.\n",
    "A total of $n$ ad shows need to be made.\n",
    "\n",
    "We will follow a simple strategy: first, we will display several different ads and learn their rewards, and then we will use the most expensive one.\n",
    "Suppose we have tried $t$ ads and found that their rewards are $r_1, \\dots, r_t$ â€” these are known values of random variable realizations.\n",
    "\n",
    "Therefore, we can now treat them as fixed numbers and estimate what'll await us in the future. Let's fix some $\\alpha > 1$.\n",
    "\n",
    "1. Let's show one more new advertisement. What is the probability that its reward $r_{t + 1}$ will be at least $\\alpha$ times greater than the best of $r_1, \\dots, r_t$?\n",
    "2. Let's show $k$ new advertisements. What is the probability that at least one of the rewards $r_{t + 1}, \\dots, r_{t + k}$ will be at least $\\alpha$ times greater than the best of $r_1, \\dots, r_t$?\n",
    "3. Let's continue showing new advertisements until we find an advertisement with a reward not less than $\\left[ \\alpha \\max(r_1, \\dots, r_t) \\right]$, or until we reach the shows limit, and then use this advertisement in all remaining shows. Estimate the expected profit with this strategy. The answer can be left as a sum."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem 5 (20 points). Handling Missing Data in Neural Network Training\n",
    "\n",
    "In real-world applications, datasets often contain missing feature values. One common strategy is to treat these missing entries as latent variables and account for them during training.\n",
    "\n",
    "* (10 points) **Incorporating Missing Data:**\n",
    "Propose a method to integrate missing data directly into the neural network training process. Describe how you would adjust the loss function and modify the backpropagation algorithm to handle missing values.\n",
    "\n",
    "* (10 points) **EM Approach vs. Imputation:**\n",
    "Discuss the potential advantages and limitations of using an expectation-maximization (EM) strategy for handling missing data compared to traditional imputation techniques. Provide clear reasoning and examples where applicable."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw_lin_models_theory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
