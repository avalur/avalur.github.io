{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/nup_logo_dark.jpeg\" width=300 style=\"display: inline-block;\"></center>\n",
    "\n",
    "## Advanced ML\n",
    "### Topic modeling and word2vec\n",
    "\n",
    "<br />\n",
    "March 18, 2025\n",
    "\n",
    "\n",
    "This notebook examines two topic modeling models from the `gensim` library:\n",
    "  - LDA (Latent Dirichlet Allocation)\n",
    "  - word2vec\n",
    "\n",
    "Sources of inspiration:\n",
    "  - https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n",
    "  - https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0fw1oPDs6wY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA (Latent Dirichlet Allocation)\n",
    "We install the topic modeling library gensim (http://radimrehurek.com/gensim/) and load the NLTK library (http://nltk.org/), which will be needed for lemmatization.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2025-01-23T08:57:56.184159Z",
     "start_time": "2025-01-23T08:57:46.138169Z"
    }
   },
   "source": [
    "!pip install --upgrade gensim\n",
    "!pip install --upgrade nltk"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\r\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/1f/76/616bc781bc19ee76b387a101211f73e00cf59368fcc221e77f88ea907d04/gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\r\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\r\n",
      "  Obtaining dependency information for numpy<2.0,>=1.18.5 from https://files.pythonhosted.org/packages/75/5b/ca6c8bd14007e5ca171c7c03102d17b4f4e0ceb53957e8c44343a9546dcc/numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.1/61.1 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\r\n",
      "  Obtaining dependency information for scipy<1.14.0,>=1.7.0 from https://files.pythonhosted.org/packages/dc/5a/2043a3bde1443d94014aaa41e0b50c39d046dda8360abd3b2a1d3f79907d/scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata\r\n",
      "  Downloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.6/60.6 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting smart-open>=1.8.1 (from gensim)\r\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/7a/18/9a8d9f01957aa1f8bbc5676d54c2e33102d247e146c1a3679d3bd5cc2e3a/smart_open-7.1.0-py3-none-any.whl.metadata\r\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\r\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\r\n",
      "  Obtaining dependency information for wrapt from https://files.pythonhosted.org/packages/48/2a/97928387d6ed1c1ebbfd4efc4133a0633546bec8481a2dd5ec961313a1c7/wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\r\n",
      "Downloading gensim-4.3.3-cp312-cp312-macosx_11_0_arm64.whl (24.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.0/24.0 MB\u001B[0m \u001B[31m50.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.7/13.7 MB\u001B[0m \u001B[31m71.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.4/30.4 MB\u001B[0m \u001B[31m29.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.7/61.7 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\r\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 2.2.2\r\n",
      "    Uninstalling numpy-2.2.2:\r\n",
      "      Successfully uninstalled numpy-2.2.2\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.15.1\r\n",
      "    Uninstalling scipy-1.15.1:\r\n",
      "      Successfully uninstalled scipy-1.15.1\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "manim 0.19.0 requires numpy>=2.1; python_version >= \"3.10\", but you have numpy 1.26.4 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting nltk\r\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/4d/66/7d9e26593edda06e8cb531874633f7c2372279c3b0f46235539fe546df8b/nltk-3.9.1-py3-none-any.whl.metadata\r\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: click in /Users/Aleksandr.Avdiushenko/IdeaProjects/avalur.github.io/.venv/lib/python3.12/site-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /Users/Aleksandr.Avdiushenko/IdeaProjects/avalur.github.io/.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\r\n",
      "Collecting regex>=2021.8.3 (from nltk)\r\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/60/85/cebcc0aff603ea0a201667b203f13ba75d9fc8668fab917ac5b2de3967bc/regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.5/40.5 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /Users/Aleksandr.Avdiushenko/IdeaProjects/avalur.github.io/.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\r\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "Downloading regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m284.8/284.8 kB\u001B[0m \u001B[31m3.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: regex, nltk\r\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gdyoxngvyleT",
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2025-01-23T08:58:25.749889Z",
     "start_time": "2025-01-23T08:58:10.029731Z"
    }
   },
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from math import log\n",
    "from time import time\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MBeC_QT3wz3",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We read the collection of source texts into a list of documents. Each document is a list of lemmas (tokens). In this example, we load the entire collection into memory. In fact, `gensim` allows you to avoid this at all stages of model building.\n",
    "\n",
    "The collection used is articles from the NeurIPS conference, one of the standard collections for topic modeling. The number of documents is about 1700, with each document having a length of 1000-2000 words. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNi0iXA-1WS1",
    "outputId": "cf89826a-16a6-4100-fd10-4c10151eb683",
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:17.622991Z",
     "start_time": "2025-01-23T08:59:17.620133Z"
    }
   },
   "source": [
    "import tarfile\n",
    "import re\n",
    "import urllib.request, zipfile\n",
    "\n",
    "\n",
    "tarfile_url = 'https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'\n",
    "filename = 'nips12raw_str602.tgz'\n",
    "# urllib.request.urlretrieve(tarfile_url, filename)\n",
    "\n",
    "def extract_documents(fname=filename):\n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:21.878814Z",
     "start_time": "2025-01-23T08:59:21.159311Z"
    }
   },
   "source": [
    "docs = list(extract_documents())\n",
    "print(len(docs))\n",
    "print(print(docs[0][:500]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a pr\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1jWGQ6143kc"
   },
   "source": [
    "Data preparation:\n",
    "- Create a dictionary\n",
    "- Perform lemmatization\n",
    "- Build n-grams\n",
    "- Filter out tokens that are too frequent or too rare"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BPPS5jSN4Fy9",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:27.037214Z",
     "start_time": "2025-01-23T08:59:26.424686Z"
    }
   },
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words."
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-kzrHz65gQh",
    "outputId": "fbb1e8cf-3b42-4c95-f75c-deef3efad1d0",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:29.735008Z",
     "start_time": "2025-01-23T08:59:29.585508Z"
    }
   },
   "source": [
    "print(np.sum([len(doc) for doc in docs]))\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "print(np.sum([len(doc) for doc in docs]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5461201\n",
      "5115888\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:32.577993Z",
     "start_time": "2025-01-23T08:59:32.575708Z"
    }
   },
   "source": [
    "print(docs[1][:50])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stochastic', 'learning', 'networks', 'and', 'their', 'electronic', 'implementation', 'joshua', 'alspector', 'robert', 'b', 'allen', 'victor', 'hut', 'and', 'srinagesh', 'satyanarayana', 'bell', 'communications', 'research', 'morristown', 'nj', 'abstract', 'we', 'describe', 'a', 'family', 'of', 'learning', 'algorithms', 'that', 'operate', 'on', 'a', 'recurrent', 'symmetrically', 'connected', 'neuromorphic', 'network', 'that', 'like', 'the', 'boltzmann', 'machine', 'settles', 'in', 'the', 'presence', 'of', 'noise']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQkmqQQ97bIj",
    "outputId": "65967b8f-6af2-462f-b550-525e07f82452",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:35.963309Z",
     "start_time": "2025-01-23T08:59:35.787552Z"
    }
   },
   "source": [
    "# Remove words that are only one character\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "print(np.sum([len(doc) for doc in docs]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4629808\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSjycOUfGhCw",
    "outputId": "7f6d019c-6f18-4cf3-c6ae-d91cbf923185",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:39.412624Z",
     "start_time": "2025-01-23T08:59:39.285618Z"
    }
   },
   "source": [
    "# Remove words with underscores, since we are going to use them as delimiters in bigrams\n",
    "docs = [[token for token in doc if '_' not in token] for doc in docs]\n",
    "print(np.sum([len(doc) for doc in docs]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4626035\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HUBuhfJNERyR",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:44.244923Z",
     "start_time": "2025-01-23T08:59:44.042510Z"
    }
   },
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Aleksandr.Avdiushenko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtGLu8yaEkvU",
    "outputId": "77dca455-dd2a-4cbe-f17b-4a5759feaa57",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:46.389483Z",
     "start_time": "2025-01-23T08:59:45.358089Z"
    }
   },
   "source": [
    "print(lemmatizer.lemmatize('abstracts'),\n",
    "      lemmatizer.lemmatize('fishes'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract fish\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fCkF1Hqm50BK",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:55.932511Z",
     "start_time": "2025-01-23T08:59:49.705173Z"
    }
   },
   "source": [
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdyb94Br6OZh",
    "outputId": "792d541f-af70-45a3-bcc5-4e37b99add50",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:58.085318Z",
     "start_time": "2025-01-23T08:59:55.936582Z"
    }
   },
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 10:59:55,937 : INFO : collecting all words and their counts\n",
      "2025-01-23 10:59:55,937 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-01-23 10:59:58,083 : INFO : collected 1114271 token types (unigram + bigrams) from a corpus of 4626035 words and 1740 sentences\n",
      "2025-01-23 10:59:58,084 : INFO : merged Phrases<1114271 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "2025-01-23 10:59:58,084 : INFO : Phrases lifecycle event {'msg': 'built Phrases<1114271 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000> in 2.15s', 'datetime': '2025-01-23T10:59:58.084307', 'gensim': '4.3.3', 'python': '3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iT1DIBdvG3jq",
    "outputId": "ce101124-b0c0-49b7-e295-ae4ba21c8ac7",
    "ExecuteTime": {
     "end_time": "2025-01-23T08:59:58.144938Z",
     "start_time": "2025-01-23T08:59:58.142789Z"
    }
   },
   "source": [
    "for token in bigram[docs[0][:100]]:\n",
    "    if '_' in token:\n",
    "        print(token)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abu_mostafa\n",
      "california_institute\n",
      "technology_pasadena\n",
      "ca_abstract\n",
      "neural_network\n",
      "boolean_function\n",
      "can_be\n",
      "very_low\n",
      "learning_rule\n",
      "lower_bound\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBnDw_SwHJvv",
    "outputId": "668b3088-fd02-470c-b612-20b7f277664c",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:00.915496Z",
     "start_time": "2025-01-23T08:59:58.209996Z"
    }
   },
   "source": [
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document\n",
    "            docs[idx].append(token)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:04.899869Z",
     "start_time": "2025-01-23T09:00:03.989735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = Dictionary(docs)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 11:00:03,990 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-01-23 11:00:04,898 : INFO : built Dictionary<77939 unique tokens: ['0a', '2h', '2h2', '2he', '2n']...> from 1740 documents (total 4944995 corpus positions)\n",
      "2025-01-23 11:00:04,898 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<77939 unique tokens: ['0a', '2h', '2h2', '2he', '2n']...> from 1740 documents (total 4944995 corpus positions)\", 'datetime': '2025-01-23T11:00:04.898816', 'gensim': '4.3.3', 'python': '3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove words that are too rare (e.g., typos) and words that are too frequent (e.g., stop words or just common non-topic terms). The `filter_extremes` function removes tokens from the dictionary that appear in less than `no_below` documents or in more than `no_above` fraction of the total number of documents."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:06.508158Z",
     "start_time": "2025-01-23T09:00:06.439828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove rare and common tokens\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 11:00:06,494 : INFO : discarding 69316 tokens: [('0a', 19), ('2h', 16), ('2h2', 1), ('2he', 3), ('a', 1740), ('about', 1058), ('abstract', 1740), ('after', 1087), ('alently', 2), ('all', 1658)]...\n",
      "2025-01-23 11:00:06,495 : INFO : keeping 8623 tokens which were in no less than 20 and no more than 870 (=50.0%) documents\n",
      "2025-01-23 11:00:06,505 : INFO : resulting dictionary: Dictionary<8623 unique tokens: ['2n', 'a2', 'a_follows', 'ability', 'abu']...>\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Represent all documents in vector form (Bag-of-Words)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GH6NNGAKH5OD",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:14.321670Z",
     "start_time": "2025-01-23T09:00:13.749212Z"
    }
   },
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yKG6JJ9Ili4",
    "outputId": "fc7dab88-81ef-4cc9-e96e-921fa7499252",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:15.435132Z",
     "start_time": "2025-01-23T09:00:15.432725Z"
    }
   },
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8623\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:21.253074Z",
     "start_time": "2025-01-23T09:00:21.250704Z"
    }
   },
   "cell_type": "code",
   "source": "print(corpus[0][:10])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4), (1, 1), (2, 1), (3, 2), (4, 4), (5, 4), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGMtwCDqxg8N"
   },
   "source": [
    "### Training\n",
    "Now we are ready to build a topic model for our collection. We will build an online LDA model, implemented in the `gensim` library. We specify the vectorized corpus of texts, the dictionary, and the number of topics (10). We will discuss the remaining parameters later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGrywSb9MvMK",
    "outputId": "cc0be619-05ae-46ca-8546-77cee2305076",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:40.056425Z",
     "start_time": "2025-01-23T09:00:24.757972Z"
    }
   },
   "source": [
    "start = time()\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000  # batch-size\n",
    "epochs = 5   \n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=epochs,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "print('Evaluation time: {}'.format((time()-start) / 60))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 11:00:24,760 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "2025-01-23 11:00:24,761 : INFO : using serial LDA version on this node\n",
      "2025-01-23 11:00:24,766 : INFO : running online (multi-pass) LDA training, 10 topics, 5 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2025-01-23 11:00:24,766 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2025-01-23 11:00:24,767 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2025-01-23 11:00:29,848 : INFO : optimized alpha [0.059415117, 0.08211947, 0.050295256, 0.11127745, 0.0716258, 0.0535777, 0.0707049, 0.07356323, 0.09876121, 0.08361459]\n",
      "2025-01-23 11:00:29,851 : INFO : topic #2 (0.050): 0.006*\"cell\" + 0.004*\"image\" + 0.003*\"component\" + 0.003*\"architecture\" + 0.003*\"net\" + 0.003*\"approximation\" + 0.003*\"rule\" + 0.003*\"distance\" + 0.003*\"direction\" + 0.002*\"neuron\"\n",
      "2025-01-23 11:00:29,851 : INFO : topic #5 (0.054): 0.005*\"hidden\" + 0.004*\"object\" + 0.003*\"hidden_unit\" + 0.003*\"net\" + 0.003*\"rule\" + 0.003*\"action\" + 0.003*\"machine\" + 0.003*\"matrix\" + 0.003*\"chip\" + 0.003*\"class\"\n",
      "2025-01-23 11:00:29,851 : INFO : topic #9 (0.084): 0.006*\"neuron\" + 0.004*\"layer\" + 0.003*\"hidden\" + 0.003*\"dynamic\" + 0.003*\"cell\" + 0.003*\"net\" + 0.003*\"rule\" + 0.003*\"matrix\" + 0.003*\"optimal\" + 0.003*\"sequence\"\n",
      "2025-01-23 11:00:29,852 : INFO : topic #8 (0.099): 0.006*\"image\" + 0.004*\"signal\" + 0.004*\"cell\" + 0.004*\"class\" + 0.003*\"node\" + 0.003*\"noise\" + 0.003*\"tree\" + 0.003*\"recognition\" + 0.003*\"object\" + 0.003*\"field\"\n",
      "2025-01-23 11:00:29,852 : INFO : topic #3 (0.111): 0.004*\"neuron\" + 0.004*\"noise\" + 0.003*\"cell\" + 0.003*\"hidden\" + 0.003*\"layer\" + 0.003*\"class\" + 0.003*\"response\" + 0.002*\"component\" + 0.002*\"sequence\" + 0.002*\"word\"\n",
      "2025-01-23 11:00:29,852 : INFO : topic diff=1.182486, rho=1.000000\n",
      "2025-01-23 11:00:29,855 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2025-01-23 11:00:33,209 : INFO : optimized alpha [0.048229348, 0.06747717, 0.042165864, 0.07820867, 0.061116114, 0.044037238, 0.061386544, 0.06416694, 0.08282439, 0.06840214]\n",
      "2025-01-23 11:00:33,212 : INFO : topic #2 (0.042): 0.006*\"cell\" + 0.004*\"direction\" + 0.004*\"component\" + 0.004*\"approximation\" + 0.003*\"image\" + 0.003*\"net\" + 0.003*\"distance\" + 0.003*\"tangent\" + 0.003*\"architecture\" + 0.003*\"polynomial\"\n",
      "2025-01-23 11:00:33,212 : INFO : topic #5 (0.044): 0.006*\"hidden\" + 0.005*\"object\" + 0.005*\"hidden_unit\" + 0.004*\"net\" + 0.003*\"action\" + 0.003*\"circuit\" + 0.003*\"rule\" + 0.003*\"chip\" + 0.003*\"machine\" + 0.003*\"code\"\n",
      "2025-01-23 11:00:33,213 : INFO : topic #9 (0.068): 0.005*\"neuron\" + 0.004*\"layer\" + 0.004*\"hidden\" + 0.003*\"dynamic\" + 0.003*\"sequence\" + 0.003*\"matrix\" + 0.003*\"net\" + 0.003*\"rule\" + 0.003*\"optimal\" + 0.003*\"recurrent\"\n",
      "2025-01-23 11:00:33,213 : INFO : topic #3 (0.078): 0.004*\"noise\" + 0.004*\"hidden\" + 0.003*\"cell\" + 0.003*\"sequence\" + 0.003*\"layer\" + 0.003*\"component\" + 0.003*\"neuron\" + 0.003*\"word\" + 0.003*\"class\" + 0.003*\"source\"\n",
      "2025-01-23 11:00:33,214 : INFO : topic #8 (0.083): 0.008*\"image\" + 0.004*\"class\" + 0.004*\"signal\" + 0.004*\"cell\" + 0.004*\"tree\" + 0.004*\"object\" + 0.003*\"recognition\" + 0.003*\"noise\" + 0.003*\"node\" + 0.003*\"field\"\n",
      "2025-01-23 11:00:33,214 : INFO : topic diff=0.280817, rho=0.577350\n",
      "2025-01-23 11:00:33,217 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2025-01-23 11:00:35,750 : INFO : optimized alpha [0.042028423, 0.06103854, 0.03745379, 0.061977584, 0.055842847, 0.03868414, 0.05652231, 0.05979326, 0.07294145, 0.062161736]\n",
      "2025-01-23 11:00:35,753 : INFO : topic #2 (0.037): 0.006*\"cell\" + 0.004*\"approximation\" + 0.004*\"direction\" + 0.004*\"component\" + 0.004*\"polynomial\" + 0.004*\"net\" + 0.004*\"distance\" + 0.004*\"tangent\" + 0.003*\"image\" + 0.003*\"bound\"\n",
      "2025-01-23 11:00:35,753 : INFO : topic #5 (0.039): 0.007*\"hidden\" + 0.006*\"object\" + 0.005*\"hidden_unit\" + 0.004*\"net\" + 0.004*\"code\" + 0.004*\"circuit\" + 0.004*\"rule\" + 0.003*\"machine\" + 0.003*\"chip\" + 0.003*\"action\"\n",
      "2025-01-23 11:00:35,753 : INFO : topic #3 (0.062): 0.005*\"noise\" + 0.003*\"hidden\" + 0.003*\"source\" + 0.003*\"component\" + 0.003*\"sequence\" + 0.003*\"word\" + 0.003*\"matrix\" + 0.003*\"gradient\" + 0.003*\"action\" + 0.003*\"signal\"\n",
      "2025-01-23 11:00:35,754 : INFO : topic #9 (0.062): 0.004*\"neuron\" + 0.004*\"layer\" + 0.004*\"sequence\" + 0.004*\"hidden\" + 0.004*\"dynamic\" + 0.004*\"matrix\" + 0.003*\"net\" + 0.003*\"recurrent\" + 0.003*\"convergence\" + 0.003*\"rule\"\n",
      "2025-01-23 11:00:35,754 : INFO : topic #8 (0.073): 0.010*\"image\" + 0.005*\"class\" + 0.004*\"object\" + 0.004*\"tree\" + 0.004*\"signal\" + 0.004*\"cell\" + 0.004*\"visual\" + 0.004*\"classifier\" + 0.004*\"recognition\" + 0.003*\"field\"\n",
      "2025-01-23 11:00:35,754 : INFO : topic diff=0.255488, rho=0.500000\n",
      "2025-01-23 11:00:35,757 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2025-01-23 11:00:37,995 : INFO : optimized alpha [0.03839956, 0.057847276, 0.03425132, 0.053778086, 0.052369006, 0.03538787, 0.053025104, 0.057110522, 0.06697845, 0.059225284]\n",
      "2025-01-23 11:00:37,998 : INFO : topic #2 (0.034): 0.005*\"cell\" + 0.005*\"approximation\" + 0.005*\"polynomial\" + 0.004*\"direction\" + 0.004*\"tangent\" + 0.004*\"distance\" + 0.004*\"net\" + 0.004*\"component\" + 0.004*\"bound\" + 0.004*\"dimension\"\n",
      "2025-01-23 11:00:37,998 : INFO : topic #5 (0.035): 0.008*\"hidden\" + 0.006*\"hidden_unit\" + 0.006*\"object\" + 0.005*\"code\" + 0.005*\"net\" + 0.004*\"circuit\" + 0.004*\"rule\" + 0.003*\"activation\" + 0.003*\"binary\" + 0.003*\"layer\"\n",
      "2025-01-23 11:00:37,999 : INFO : topic #1 (0.058): 0.005*\"generalization\" + 0.005*\"hidden\" + 0.004*\"gaussian\" + 0.004*\"sample\" + 0.004*\"prediction\" + 0.004*\"regression\" + 0.004*\"density\" + 0.003*\"class\" + 0.003*\"component\" + 0.003*\"layer\"\n",
      "2025-01-23 11:00:37,999 : INFO : topic #9 (0.059): 0.004*\"layer\" + 0.004*\"sequence\" + 0.004*\"hidden\" + 0.004*\"neuron\" + 0.004*\"dynamic\" + 0.004*\"matrix\" + 0.004*\"net\" + 0.004*\"recurrent\" + 0.003*\"node\" + 0.003*\"convergence\"\n",
      "2025-01-23 11:00:37,999 : INFO : topic #8 (0.067): 0.012*\"image\" + 0.005*\"object\" + 0.005*\"class\" + 0.004*\"visual\" + 0.004*\"tree\" + 0.004*\"classifier\" + 0.004*\"recognition\" + 0.004*\"signal\" + 0.004*\"cell\" + 0.004*\"field\"\n",
      "2025-01-23 11:00:38,000 : INFO : topic diff=0.237938, rho=0.447214\n",
      "2025-01-23 11:00:38,003 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2025-01-23 11:00:40,047 : INFO : optimized alpha [0.03610617, 0.055974625, 0.032003902, 0.04900631, 0.050236102, 0.033242922, 0.05048406, 0.055391137, 0.0629612, 0.057374757]\n",
      "2025-01-23 11:00:40,051 : INFO : topic #2 (0.032): 0.005*\"polynomial\" + 0.005*\"approximation\" + 0.005*\"tangent\" + 0.005*\"bound\" + 0.005*\"cell\" + 0.005*\"distance\" + 0.005*\"net\" + 0.005*\"dimension\" + 0.004*\"direction\" + 0.004*\"component\"\n",
      "2025-01-23 11:00:40,051 : INFO : topic #5 (0.033): 0.009*\"hidden\" + 0.007*\"hidden_unit\" + 0.006*\"object\" + 0.006*\"code\" + 0.005*\"net\" + 0.005*\"circuit\" + 0.005*\"rule\" + 0.004*\"activation\" + 0.003*\"layer\" + 0.003*\"binary\"\n",
      "2025-01-23 11:00:40,051 : INFO : topic #1 (0.056): 0.005*\"generalization\" + 0.005*\"gaussian\" + 0.005*\"hidden\" + 0.004*\"sample\" + 0.004*\"prediction\" + 0.004*\"regression\" + 0.004*\"density\" + 0.004*\"class\" + 0.003*\"estimate\" + 0.003*\"noise\"\n",
      "2025-01-23 11:00:40,051 : INFO : topic #9 (0.057): 0.005*\"sequence\" + 0.005*\"layer\" + 0.004*\"hidden\" + 0.004*\"dynamic\" + 0.004*\"net\" + 0.004*\"matrix\" + 0.004*\"neuron\" + 0.004*\"recurrent\" + 0.004*\"node\" + 0.003*\"gradient\"\n",
      "2025-01-23 11:00:40,052 : INFO : topic #8 (0.063): 0.013*\"image\" + 0.006*\"object\" + 0.005*\"visual\" + 0.005*\"class\" + 0.005*\"classifier\" + 0.004*\"tree\" + 0.004*\"recognition\" + 0.004*\"filter\" + 0.004*\"signal\" + 0.004*\"field\"\n",
      "2025-01-23 11:00:40,052 : INFO : topic diff=0.226248, rho=0.408248\n",
      "2025-01-23 11:00:40,055 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=8623, num_topics=10, decay=0.5, chunksize=2000> in 15.29s', 'datetime': '2025-01-23T11:00:40.055323', 'gensim': '4.3.3', 'python': '3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.25493288040161133\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5knApWshObLO"
   },
   "source": "Let's see what we got. We are interested in part of the Phi matrix – the probabilities of words in topics. The NeurIPS collection is entirely dedicated to machine learning. It's difficult to evaluate the topics, though some interpretability can be traced."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJVQU_1rP2f2",
    "outputId": "a2e11a73-e3ff-4313-b926-7de2990f70fb",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:45.583046Z",
     "start_time": "2025-01-23T09:00:45.565850Z"
    }
   },
   "source": [
    "for position in range(10):\n",
    "    row = []\n",
    "    for topic in range(10):\n",
    "        row.append(model.show_topic(topic)[position][0].center(11, ' '))\n",
    "    print(''.join(row))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  control  generalization polynomial   noise       cell      hidden     neuron     image      image     sequence \n",
      "   policy    gaussian approximation component    neuron  hidden_unit   signal  recognition   object     layer   \n",
      "   action     hidden    tangent     source    response    object     memory      net       visual     hidden  \n",
      "  dynamic     sample     bound      matrix    activity     code       chip      layer      class     dynamic  \n",
      "   motor    prediction    cell     sequence   control      net       spike    character  classifier    net    \n",
      "  optimal   regression  distance   mixture    stimulus   circuit     analog      node       tree      matrix  \n",
      "reinforcement  density      net       hidden    synaptic     rule     circuit     hidden  recognition   neuron  \n",
      " trajectory   class    dimension      em       firing   activation    word     trained     filter   recurrent \n",
      "  movement   estimate  direction    signal     layer      layer    connection   class      signal      node   \n",
      "reinforcement_learning   noise    component  likelihood connection   binary     delay      search     field     gradient \n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLIuGnNpQpyP",
    "outputId": "8f33e7e4-9250-4898-cebe-db4e1a60c4ec",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:48.907507Z",
     "start_time": "2025-01-23T09:00:48.812341Z"
    }
   },
   "source": [
    "top_topics = model.top_topics(corpus)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 11:00:48,857 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ-5eVhBQrsQ",
    "outputId": "8d47a367-ccf1-4569-be41-f7ec6c372d4a",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:50.712809Z",
     "start_time": "2025-01-23T09:00:50.708663Z"
    }
   },
   "source": [
    "top_topics[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.0054766214, 'generalization'),\n",
       "  (0.0047958074, 'gaussian'),\n",
       "  (0.004777913, 'hidden'),\n",
       "  (0.0044106385, 'sample'),\n",
       "  (0.0043535293, 'prediction'),\n",
       "  (0.00389188, 'regression'),\n",
       "  (0.0036687148, 'density'),\n",
       "  (0.0035643762, 'class'),\n",
       "  (0.0032825277, 'estimate'),\n",
       "  (0.0031644437, 'noise'),\n",
       "  (0.0030338306, 'training_set'),\n",
       "  (0.003029318, 'approximation'),\n",
       "  (0.00301275, 'component'),\n",
       "  (0.0029484706, 'variance'),\n",
       "  (0.0029387628, 'bound'),\n",
       "  (0.0029217678, 'hidden_unit'),\n",
       "  (0.00291219, 'kernel'),\n",
       "  (0.0027721105, 'optimal'),\n",
       "  (0.0027302164, 'layer'),\n",
       "  (0.0026089945, 'matrix')],\n",
       " -0.9447828080187861)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-2vzw2RSGFA",
    "outputId": "b4f7c699-b384-430f-83c5-953fe591bf0f",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:51.528222Z",
     "start_time": "2025-01-23T09:00:51.523980Z"
    }
   },
   "source": [
    "model.inference([corpus[0]])[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.6106169e-02, 5.5974636e-02, 1.0229638e+02, 7.4336639e+01,\n",
       "        5.0236102e-02, 1.6639297e+02, 5.4127239e+01, 5.5391144e-02,\n",
       "        6.2961258e-02, 4.0246011e+02]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X-HTzPGxs3F"
   },
   "source": [
    "### Perplexity evaluation\n",
    "We want to assess the model with something more convincing than just looking at topic profiles and document profiles. This is necessary for the possibility of comparing different models, for example, those obtained with different run parameters. Let's learn to measure **perplexity**. The function `model.state.get_lambda` returns the unnormalized $\\Phi$ matrix, and `model.inference` estimates the unnormalized $\\Theta$ matrix for a list of documents.\n",
    "\n",
    "We iterate through the collection and calculate perplexity using the formula. The lower the perplexity, the better."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8gVV_PFRxmIP",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:54.105081Z",
     "start_time": "2025-01-23T09:00:54.102271Z"
    }
   },
   "source": [
    "def perplexity(model, corpus):\n",
    "    corpus_length = 0\n",
    "    log_likelihood = 0\n",
    "    topic_profiles = model.state.get_lambda() / np.sum(model.state.get_lambda(), axis=1)[:, np.newaxis]\n",
    "    for document in corpus:\n",
    "        gamma, _ = model.inference([document])\n",
    "        document_profile = gamma / np.sum(gamma)\n",
    "        for term_id, term_count in document:\n",
    "            corpus_length += term_count\n",
    "            term_probability = np.dot(document_profile, topic_profiles[:, term_id])\n",
    "            log_likelihood += term_count * log(term_probability.item())\n",
    "    perplexity = np.exp(-log_likelihood / corpus_length)\n",
    "    return perplexity"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iN6amrvHTDKt",
    "outputId": "a268a642-2efd-4217-b899-cb3abeac1f31",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:00:57.713874Z",
     "start_time": "2025-01-23T09:00:55.280105Z"
    }
   },
   "source": [
    "print('Perplexity: {}'.format(perplexity(model, corpus)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2868.6156534545394\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5_Sqil0TPYD",
    "outputId": "df2374d5-d768-4e5e-f741-15eb8ed0f0ea",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:12.482045Z",
     "start_time": "2025-01-23T09:00:57.716861Z"
    }
   },
   "source": [
    "model_5 = models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=5,\n",
    "    passes=epochs,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "model_20 = models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=20,\n",
    "    passes=epochs,\n",
    "    eval_every=eval_every\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 11:00:57,717 : INFO : using autotuned alpha, starting with [0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "2025-01-23 11:00:57,718 : INFO : using serial LDA version on this node\n",
      "2025-01-23 11:00:57,719 : INFO : running online (multi-pass) LDA training, 5 topics, 5 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2025-01-23 11:00:57,720 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2025-01-23 11:00:57,720 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2025-01-23 11:01:02,245 : INFO : optimized alpha [0.122527294, 0.17584074, 0.09676569, 0.1317677, 0.20862874]\n",
      "2025-01-23 11:01:02,247 : INFO : topic #0 (0.123): 0.007*\"image\" + 0.003*\"response\" + 0.003*\"stimulus\" + 0.003*\"field\" + 0.003*\"neuron\" + 0.003*\"layer\" + 0.003*\"cell\" + 0.002*\"recognition\" + 0.002*\"dynamic\" + 0.002*\"rule\"\n",
      "2025-01-23 11:01:02,247 : INFO : topic #1 (0.176): 0.007*\"neuron\" + 0.004*\"signal\" + 0.004*\"cell\" + 0.003*\"hidden\" + 0.003*\"layer\" + 0.003*\"image\" + 0.003*\"noise\" + 0.003*\"response\" + 0.002*\"visual\" + 0.002*\"spike\"\n",
      "2025-01-23 11:01:02,247 : INFO : topic #2 (0.097): 0.006*\"neuron\" + 0.005*\"layer\" + 0.004*\"cell\" + 0.004*\"circuit\" + 0.003*\"signal\" + 0.003*\"chip\" + 0.003*\"net\" + 0.002*\"hidden\" + 0.002*\"simulation\" + 0.002*\"noise\"\n",
      "2025-01-23 11:01:02,248 : INFO : topic #3 (0.132): 0.004*\"cell\" + 0.004*\"class\" + 0.003*\"node\" + 0.003*\"neuron\" + 0.003*\"action\" + 0.002*\"architecture\" + 0.002*\"estimate\" + 0.002*\"control\" + 0.002*\"memory\" + 0.002*\"hidden\"\n",
      "2025-01-23 11:01:02,248 : INFO : topic #4 (0.209): 0.004*\"hidden\" + 0.003*\"layer\" + 0.003*\"class\" + 0.003*\"image\" + 0.003*\"field\" + 0.003*\"neuron\" + 0.003*\"net\" + 0.003*\"noise\" + 0.003*\"matrix\" + 0.002*\"recognition\"\n",
      "2025-01-23 11:01:02,248 : INFO : topic diff=1.049038, rho=1.000000\n",
      "2025-01-23 11:01:02,250 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2025-01-23 11:01:05,272 : INFO : optimized alpha [0.08581312, 0.0906369, 0.074020624, 0.09381458, 0.12156813]\n",
      "2025-01-23 11:01:05,274 : INFO : topic #0 (0.086): 0.010*\"image\" + 0.004*\"field\" + 0.003*\"response\" + 0.003*\"stimulus\" + 0.003*\"object\" + 0.003*\"cell\" + 0.003*\"visual\" + 0.003*\"neuron\" + 0.003*\"layer\" + 0.003*\"face\"\n",
      "2025-01-23 11:01:05,274 : INFO : topic #1 (0.091): 0.009*\"neuron\" + 0.005*\"cell\" + 0.004*\"signal\" + 0.004*\"spike\" + 0.003*\"response\" + 0.003*\"visual\" + 0.003*\"layer\" + 0.003*\"noise\" + 0.003*\"hidden\" + 0.003*\"activity\"\n",
      "2025-01-23 11:01:05,274 : INFO : topic #2 (0.074): 0.007*\"neuron\" + 0.005*\"circuit\" + 0.005*\"layer\" + 0.004*\"chip\" + 0.004*\"cell\" + 0.004*\"signal\" + 0.004*\"analog\" + 0.003*\"net\" + 0.003*\"connection\" + 0.002*\"control\"\n",
      "2025-01-23 11:01:05,275 : INFO : topic #3 (0.094): 0.004*\"class\" + 0.004*\"action\" + 0.003*\"node\" + 0.003*\"policy\" + 0.003*\"control\" + 0.003*\"cell\" + 0.003*\"bound\" + 0.003*\"estimate\" + 0.002*\"let\" + 0.002*\"optimal\"\n",
      "2025-01-23 11:01:05,275 : INFO : topic #4 (0.122): 0.005*\"hidden\" + 0.004*\"layer\" + 0.004*\"class\" + 0.003*\"net\" + 0.003*\"matrix\" + 0.003*\"rule\" + 0.003*\"noise\" + 0.003*\"recognition\" + 0.003*\"image\" + 0.003*\"component\"\n",
      "2025-01-23 11:01:05,275 : INFO : topic diff=0.203642, rho=0.577350\n",
      "2025-01-23 11:01:05,277 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2025-01-23 11:01:07,418 : INFO : optimized alpha [0.07119089, 0.07208312, 0.062457144, 0.07920289, 0.10192028]\n",
      "2025-01-23 11:01:07,420 : INFO : topic #0 (0.071): 0.012*\"image\" + 0.005*\"object\" + 0.004*\"field\" + 0.003*\"visual\" + 0.003*\"stimulus\" + 0.003*\"response\" + 0.003*\"cell\" + 0.003*\"face\" + 0.003*\"recognition\" + 0.003*\"layer\"\n",
      "2025-01-23 11:01:07,420 : INFO : topic #1 (0.072): 0.010*\"neuron\" + 0.007*\"cell\" + 0.005*\"signal\" + 0.005*\"spike\" + 0.004*\"response\" + 0.003*\"visual\" + 0.003*\"activity\" + 0.003*\"layer\" + 0.003*\"noise\" + 0.003*\"stimulus\"\n",
      "2025-01-23 11:01:07,420 : INFO : topic #2 (0.062): 0.008*\"neuron\" + 0.006*\"circuit\" + 0.005*\"chip\" + 0.005*\"layer\" + 0.005*\"analog\" + 0.004*\"signal\" + 0.004*\"net\" + 0.003*\"cell\" + 0.003*\"connection\" + 0.003*\"voltage\"\n",
      "2025-01-23 11:01:07,421 : INFO : topic #3 (0.079): 0.004*\"action\" + 0.004*\"class\" + 0.003*\"node\" + 0.003*\"control\" + 0.003*\"bound\" + 0.003*\"policy\" + 0.003*\"optimal\" + 0.003*\"let\" + 0.003*\"estimate\" + 0.003*\"dynamic\"\n",
      "2025-01-23 11:01:07,421 : INFO : topic #4 (0.102): 0.006*\"hidden\" + 0.004*\"class\" + 0.004*\"layer\" + 0.003*\"net\" + 0.003*\"rule\" + 0.003*\"matrix\" + 0.003*\"recognition\" + 0.003*\"noise\" + 0.003*\"hidden_unit\" + 0.003*\"component\"\n",
      "2025-01-23 11:01:07,421 : INFO : topic diff=0.167635, rho=0.500000\n",
      "2025-01-23 11:01:07,424 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2025-01-23 11:01:09,165 : INFO : optimized alpha [0.06278481, 0.06324577, 0.05643526, 0.07139262, 0.09298895]\n",
      "2025-01-23 11:01:09,166 : INFO : topic #0 (0.063): 0.014*\"image\" + 0.006*\"object\" + 0.005*\"field\" + 0.004*\"visual\" + 0.003*\"stimulus\" + 0.003*\"response\" + 0.003*\"cell\" + 0.003*\"face\" + 0.003*\"recognition\" + 0.003*\"distance\"\n",
      "2025-01-23 11:01:09,167 : INFO : topic #1 (0.063): 0.011*\"neuron\" + 0.009*\"cell\" + 0.005*\"spike\" + 0.005*\"signal\" + 0.005*\"response\" + 0.004*\"activity\" + 0.004*\"visual\" + 0.004*\"stimulus\" + 0.003*\"firing\" + 0.003*\"synaptic\"\n",
      "2025-01-23 11:01:09,167 : INFO : topic #2 (0.056): 0.008*\"neuron\" + 0.007*\"circuit\" + 0.006*\"chip\" + 0.005*\"analog\" + 0.005*\"layer\" + 0.004*\"signal\" + 0.004*\"net\" + 0.004*\"connection\" + 0.003*\"memory\" + 0.003*\"voltage\"\n",
      "2025-01-23 11:01:09,167 : INFO : topic #3 (0.071): 0.004*\"action\" + 0.004*\"class\" + 0.004*\"bound\" + 0.004*\"control\" + 0.003*\"policy\" + 0.003*\"optimal\" + 0.003*\"node\" + 0.003*\"let\" + 0.003*\"theorem\" + 0.003*\"estimate\"\n",
      "2025-01-23 11:01:09,168 : INFO : topic #4 (0.093): 0.006*\"hidden\" + 0.004*\"class\" + 0.004*\"layer\" + 0.003*\"rule\" + 0.003*\"net\" + 0.003*\"matrix\" + 0.003*\"recognition\" + 0.003*\"hidden_unit\" + 0.003*\"noise\" + 0.003*\"gaussian\"\n",
      "2025-01-23 11:01:09,168 : INFO : topic diff=0.136891, rho=0.447214\n",
      "2025-01-23 11:01:09,170 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2025-01-23 11:01:10,726 : INFO : optimized alpha [0.05772038, 0.057874486, 0.05308681, 0.06711834, 0.08897346]\n",
      "2025-01-23 11:01:10,728 : INFO : topic #0 (0.058): 0.015*\"image\" + 0.006*\"object\" + 0.005*\"field\" + 0.004*\"visual\" + 0.003*\"recognition\" + 0.003*\"orientation\" + 0.003*\"distance\" + 0.003*\"face\" + 0.003*\"response\" + 0.003*\"cell\"\n",
      "2025-01-23 11:01:10,728 : INFO : topic #1 (0.058): 0.012*\"neuron\" + 0.010*\"cell\" + 0.005*\"spike\" + 0.005*\"response\" + 0.005*\"signal\" + 0.004*\"activity\" + 0.004*\"stimulus\" + 0.004*\"visual\" + 0.004*\"synaptic\" + 0.004*\"firing\"\n",
      "2025-01-23 11:01:10,728 : INFO : topic #2 (0.053): 0.008*\"neuron\" + 0.007*\"circuit\" + 0.006*\"chip\" + 0.006*\"analog\" + 0.005*\"signal\" + 0.005*\"layer\" + 0.004*\"net\" + 0.004*\"memory\" + 0.004*\"connection\" + 0.003*\"voltage\"\n",
      "2025-01-23 11:01:10,729 : INFO : topic #3 (0.067): 0.004*\"action\" + 0.004*\"bound\" + 0.004*\"class\" + 0.004*\"control\" + 0.004*\"optimal\" + 0.004*\"policy\" + 0.003*\"let\" + 0.003*\"node\" + 0.003*\"theorem\" + 0.003*\"sample\"\n",
      "2025-01-23 11:01:10,729 : INFO : topic #4 (0.089): 0.006*\"hidden\" + 0.004*\"class\" + 0.004*\"layer\" + 0.004*\"rule\" + 0.003*\"net\" + 0.003*\"matrix\" + 0.003*\"recognition\" + 0.003*\"hidden_unit\" + 0.003*\"gaussian\" + 0.003*\"classifier\"\n",
      "2025-01-23 11:01:10,729 : INFO : topic diff=0.116415, rho=0.408248\n",
      "2025-01-23 11:01:10,732 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=8623, num_topics=5, decay=0.5, chunksize=2000> in 13.01s', 'datetime': '2025-01-23T11:01:10.732079', 'gensim': '4.3.3', 'python': '3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'created'}\n",
      "2025-01-23 11:01:10,732 : INFO : using autotuned alpha, starting with [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "2025-01-23 11:01:10,732 : INFO : using serial LDA version on this node\n",
      "2025-01-23 11:01:10,738 : INFO : running online (multi-pass) LDA training, 20 topics, 5 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2025-01-23 11:01:10,738 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2025-01-23 11:01:10,738 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2025-01-23 11:01:33,215 : INFO : optimized alpha [0.040413823, 0.045460667, 0.0410384, 0.040413514, 0.036669545, 0.0423263, 0.053231977, 0.03782567, 0.043962523, 0.051532093, 0.0375901, 0.040741656, 0.049330376, 0.042438917, 0.039175503, 0.03940653, 0.042287953, 0.043770358, 0.045009516, 0.042189036]\n",
      "2025-01-23 11:01:33,226 : INFO : topic #4 (0.037): 0.005*\"word\" + 0.005*\"control\" + 0.004*\"target\" + 0.003*\"policy\" + 0.003*\"hidden\" + 0.003*\"recognition\" + 0.003*\"reinforcement\" + 0.003*\"path\" + 0.003*\"action\" + 0.003*\"dynamic\"\n",
      "2025-01-23 11:01:33,228 : INFO : topic #10 (0.038): 0.006*\"image\" + 0.005*\"neuron\" + 0.004*\"layer\" + 0.004*\"cell\" + 0.003*\"control\" + 0.003*\"stimulus\" + 0.003*\"signal\" + 0.002*\"convergence\" + 0.002*\"fig\" + 0.002*\"gradient\"\n",
      "2025-01-23 11:01:33,229 : INFO : topic #12 (0.049): 0.004*\"speech\" + 0.004*\"class\" + 0.003*\"gaussian\" + 0.003*\"mixture\" + 0.003*\"layer\" + 0.003*\"signal\" + 0.003*\"hidden\" + 0.003*\"sequence\" + 0.003*\"recognition\" + 0.003*\"noise\"\n",
      "2025-01-23 11:01:33,230 : INFO : topic #9 (0.052): 0.006*\"neuron\" + 0.004*\"layer\" + 0.004*\"hidden\" + 0.004*\"generalization\" + 0.003*\"signal\" + 0.003*\"noise\" + 0.003*\"dynamic\" + 0.002*\"net\" + 0.002*\"matrix\" + 0.002*\"class\"\n",
      "2025-01-23 11:01:33,231 : INFO : topic #6 (0.053): 0.008*\"image\" + 0.006*\"neuron\" + 0.004*\"hidden\" + 0.004*\"net\" + 0.003*\"solution\" + 0.003*\"noise\" + 0.003*\"spike\" + 0.002*\"filter\" + 0.002*\"connection\" + 0.002*\"architecture\"\n",
      "2025-01-23 11:01:33,231 : INFO : topic diff=1.661153, rho=1.000000\n",
      "2025-01-23 11:01:33,238 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2025-01-23 11:01:44,796 : INFO : optimized alpha [0.03657953, 0.039859135, 0.03830967, 0.03708379, 0.033499114, 0.03701948, 0.048099056, 0.03355838, 0.03875175, 0.048509736, 0.032807894, 0.035917852, 0.045524657, 0.03860081, 0.034551598, 0.03604814, 0.03726438, 0.03910524, 0.0390727, 0.036903687]\n",
      "2025-01-23 11:01:44,807 : INFO : topic #10 (0.033): 0.006*\"image\" + 0.005*\"neuron\" + 0.004*\"layer\" + 0.003*\"cell\" + 0.003*\"stimulus\" + 0.003*\"entropy\" + 0.003*\"signal\" + 0.003*\"control\" + 0.003*\"convergence\" + 0.003*\"cue\"\n",
      "2025-01-23 11:01:44,809 : INFO : topic #4 (0.033): 0.007*\"policy\" + 0.007*\"word\" + 0.006*\"action\" + 0.006*\"control\" + 0.005*\"reinforcement\" + 0.004*\"target\" + 0.004*\"recognition\" + 0.003*\"speech\" + 0.003*\"reinforcement_learning\" + 0.003*\"hidden\"\n",
      "2025-01-23 11:01:44,818 : INFO : topic #12 (0.046): 0.006*\"speech\" + 0.005*\"mixture\" + 0.004*\"gaussian\" + 0.004*\"class\" + 0.004*\"expert\" + 0.003*\"recognition\" + 0.003*\"density\" + 0.003*\"sequence\" + 0.003*\"hidden\" + 0.003*\"prediction\"\n",
      "2025-01-23 11:01:44,824 : INFO : topic #6 (0.048): 0.009*\"image\" + 0.006*\"neuron\" + 0.005*\"net\" + 0.004*\"hidden\" + 0.003*\"analog\" + 0.003*\"code\" + 0.003*\"noise\" + 0.003*\"spike\" + 0.003*\"solution\" + 0.003*\"bit\"\n",
      "2025-01-23 11:01:44,831 : INFO : topic #9 (0.049): 0.005*\"layer\" + 0.005*\"hidden\" + 0.005*\"neuron\" + 0.005*\"generalization\" + 0.003*\"noise\" + 0.003*\"net\" + 0.003*\"signal\" + 0.003*\"dynamic\" + 0.003*\"hidden_unit\" + 0.003*\"trained\"\n",
      "2025-01-23 11:01:44,835 : INFO : topic diff=0.499072, rho=0.577350\n",
      "2025-01-23 11:01:44,850 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2025-01-23 11:01:55,742 : INFO : optimized alpha [0.034381706, 0.036039896, 0.037602298, 0.035925206, 0.03198715, 0.033774473, 0.044551406, 0.03106907, 0.035376243, 0.04717869, 0.029647425, 0.03294115, 0.043068536, 0.036336504, 0.031577386, 0.0346205, 0.03396384, 0.03593878, 0.034765802, 0.033440344]\n",
      "2025-01-23 11:01:55,755 : INFO : topic #10 (0.030): 0.005*\"image\" + 0.005*\"neuron\" + 0.004*\"entropy\" + 0.004*\"layer\" + 0.003*\"signal\" + 0.003*\"stimulus\" + 0.003*\"cue\" + 0.003*\"dendritic\" + 0.003*\"control\" + 0.003*\"fig\"\n",
      "2025-01-23 11:01:55,756 : INFO : topic #7 (0.031): 0.016*\"memory\" + 0.011*\"word\" + 0.006*\"recognition\" + 0.004*\"associative\" + 0.004*\"matrix\" + 0.003*\"capacity\" + 0.003*\"activation\" + 0.003*\"object\" + 0.003*\"action\" + 0.003*\"sequence\"\n",
      "2025-01-23 11:01:55,757 : INFO : topic #12 (0.043): 0.007*\"speech\" + 0.006*\"mixture\" + 0.004*\"expert\" + 0.004*\"gaussian\" + 0.004*\"density\" + 0.004*\"recognition\" + 0.004*\"class\" + 0.004*\"sequence\" + 0.003*\"prediction\" + 0.003*\"hidden\"\n",
      "2025-01-23 11:01:55,757 : INFO : topic #6 (0.045): 0.008*\"image\" + 0.007*\"neuron\" + 0.005*\"net\" + 0.004*\"code\" + 0.004*\"analog\" + 0.004*\"bit\" + 0.004*\"hidden\" + 0.004*\"noise\" + 0.003*\"solution\" + 0.003*\"connection\"\n",
      "2025-01-23 11:01:55,758 : INFO : topic #9 (0.047): 0.006*\"hidden\" + 0.006*\"generalization\" + 0.006*\"layer\" + 0.004*\"neuron\" + 0.004*\"noise\" + 0.004*\"net\" + 0.003*\"hidden_unit\" + 0.003*\"gradient\" + 0.003*\"optimal\" + 0.003*\"training_set\"\n",
      "2025-01-23 11:01:55,759 : INFO : topic diff=0.493875, rho=0.500000\n",
      "2025-01-23 11:01:55,769 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2025-01-23 11:02:03,362 : INFO : optimized alpha [0.0330128, 0.033393294, 0.037367962, 0.035519186, 0.030966463, 0.031638134, 0.042192526, 0.029601056, 0.033124305, 0.04641355, 0.027440064, 0.031032702, 0.041290604, 0.034929644, 0.029489595, 0.034188118, 0.03179565, 0.033889823, 0.03167709, 0.031112151]\n",
      "2025-01-23 11:02:03,371 : INFO : topic #10 (0.027): 0.005*\"neuron\" + 0.005*\"entropy\" + 0.005*\"image\" + 0.004*\"cue\" + 0.004*\"dendritic\" + 0.004*\"signal\" + 0.003*\"layer\" + 0.003*\"stimulus\" + 0.003*\"student\" + 0.003*\"gaussian\"\n",
      "2025-01-23 11:02:03,373 : INFO : topic #14 (0.029): 0.008*\"visual\" + 0.006*\"cell\" + 0.006*\"ii\" + 0.006*\"neuron\" + 0.005*\"activity\" + 0.005*\"eye\" + 0.004*\"layer\" + 0.004*\"response\" + 0.003*\"class\" + 0.003*\"correlation\"\n",
      "2025-01-23 11:02:03,377 : INFO : topic #12 (0.041): 0.008*\"speech\" + 0.007*\"mixture\" + 0.005*\"expert\" + 0.005*\"gaussian\" + 0.005*\"density\" + 0.004*\"recognition\" + 0.004*\"likelihood\" + 0.004*\"sequence\" + 0.004*\"class\" + 0.004*\"prediction\"\n",
      "2025-01-23 11:02:03,383 : INFO : topic #6 (0.042): 0.008*\"image\" + 0.008*\"neuron\" + 0.006*\"net\" + 0.005*\"analog\" + 0.005*\"code\" + 0.005*\"bit\" + 0.004*\"noise\" + 0.003*\"connection\" + 0.003*\"solution\" + 0.003*\"chip\"\n",
      "2025-01-23 11:02:03,390 : INFO : topic #9 (0.046): 0.006*\"generalization\" + 0.006*\"hidden\" + 0.006*\"layer\" + 0.004*\"net\" + 0.004*\"noise\" + 0.003*\"neuron\" + 0.003*\"gradient\" + 0.003*\"hidden_unit\" + 0.003*\"training_set\" + 0.003*\"optimal\"\n",
      "2025-01-23 11:02:03,397 : INFO : topic diff=0.501261, rho=0.447214\n",
      "2025-01-23 11:02:03,419 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2025-01-23 11:02:12,453 : INFO : optimized alpha [0.032176964, 0.031549346, 0.037263386, 0.035263386, 0.03020934, 0.030168764, 0.040701404, 0.028718205, 0.031551495, 0.046081584, 0.025848713, 0.029682864, 0.040121753, 0.033879016, 0.028143503, 0.034247342, 0.030319786, 0.032495327, 0.029389825, 0.029400928]\n",
      "2025-01-23 11:02:12,464 : INFO : topic #10 (0.026): 0.006*\"entropy\" + 0.005*\"neuron\" + 0.004*\"image\" + 0.004*\"student\" + 0.004*\"dendritic\" + 0.004*\"gaussian\" + 0.004*\"cue\" + 0.004*\"signal\" + 0.004*\"density\" + 0.004*\"field\"\n",
      "2025-01-23 11:02:12,465 : INFO : topic #14 (0.028): 0.009*\"visual\" + 0.006*\"cell\" + 0.006*\"activity\" + 0.006*\"ii\" + 0.005*\"neuron\" + 0.005*\"eye\" + 0.004*\"response\" + 0.004*\"layer\" + 0.003*\"location\" + 0.003*\"field\"\n",
      "2025-01-23 11:02:12,466 : INFO : topic #12 (0.040): 0.008*\"speech\" + 0.008*\"mixture\" + 0.005*\"expert\" + 0.005*\"gaussian\" + 0.005*\"density\" + 0.004*\"recognition\" + 0.004*\"likelihood\" + 0.004*\"sequence\" + 0.004*\"class\" + 0.004*\"prediction\"\n",
      "2025-01-23 11:02:12,467 : INFO : topic #6 (0.041): 0.008*\"neuron\" + 0.007*\"image\" + 0.006*\"net\" + 0.006*\"analog\" + 0.006*\"code\" + 0.005*\"bit\" + 0.004*\"noise\" + 0.004*\"chip\" + 0.004*\"connection\" + 0.003*\"memory\"\n",
      "2025-01-23 11:02:12,468 : INFO : topic #9 (0.046): 0.007*\"hidden\" + 0.007*\"generalization\" + 0.006*\"layer\" + 0.004*\"net\" + 0.004*\"noise\" + 0.004*\"gradient\" + 0.003*\"training_set\" + 0.003*\"hidden_unit\" + 0.003*\"optimal\" + 0.003*\"prediction\"\n",
      "2025-01-23 11:02:12,469 : INFO : topic diff=0.506869, rho=0.408248\n",
      "2025-01-23 11:02:12,479 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=8623, num_topics=20, decay=0.5, chunksize=2000> in 61.74s', 'datetime': '2025-01-23T11:02:12.479771', 'gensim': '4.3.3', 'python': '3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYR1kXEFTWMg",
    "outputId": "2463aa3f-b0d7-4980-dcdf-43e7f62e41c7",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:22.985515Z",
     "start_time": "2025-01-23T09:02:12.499213Z"
    }
   },
   "source": [
    "print('Perplexity 5: {}'.format(perplexity(model_5, corpus)))\n",
    "print('Perplexity 20: {}'.format(perplexity(model_20, corpus)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 5: 3066.2736778426897\n",
      "Perplexity 20: 2594.579183798892\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5yh-RhCxwcT"
   },
   "source": [
    "### Word2Vec model\n",
    "Word2Vec is one of the fundamental neural network models of the \"pre-transformer\" era (2013-2018). The essence of the model is to build a mapping of words into an $N$-dimensional space (embeddings) with certain characteristics. Two words have more similar embeddings the more similar the contexts in which they are used.\n",
    "\n",
    "In the `gensim` library, two methods for building word2vec are implemented:\n",
    "  - Skip-grams (SG)\n",
    "  - Continuous-bag-of-words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYtz7rDrVyKS"
   },
   "source": [
    "## Demo\n",
    "For the demonstration, let's take a pre-trained model trained on the Google News dataset, containing approximately 3 million English words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTcMvyG1V1wH",
    "outputId": "e1aac14d-a61b-4876-84e4-7a751e10ddaf",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:44.007078Z",
     "start_time": "2025-01-23T09:02:31.115730Z"
    }
   },
   "source": [
    "# download the model ~1.6GB \n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 11:02:31,398 : INFO : loading projection weights from /Users/Aleksandr.Avdiushenko/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2025-01-23 11:02:44,005 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /Users/Aleksandr.Avdiushenko/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2025-01-23T11:02:44.005317', 'gensim': '4.3.3', 'python': '3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]', 'platform': 'macOS-15.2-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-gUr1QvQWCg_",
    "outputId": "5fdde298-1dfe-4067-b132-b95e91e7c6da",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:53.266709Z",
     "start_time": "2025-01-23T09:02:53.264298Z"
    }
   },
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key )} is {word}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word #0/3000000 is </s>\n",
      "word #1/3000000 is in\n",
      "word #2/3000000 is for\n",
      "word #3/3000000 is that\n",
      "word #4/3000000 is is\n",
      "word #5/3000000 is on\n",
      "word #6/3000000 is ##\n",
      "word #7/3000000 is The\n",
      "word #8/3000000 is with\n",
      "word #9/3000000 is said\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewckl1qAX7Cp",
    "outputId": "f7c5715c-3273-4933-f98b-ba68ca0616fd",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:02:55.369963Z",
     "start_time": "2025-01-23T09:02:55.367565Z"
    }
   },
   "source": [
    "vec_king = wv['king']\n",
    "print(vec_king[:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12597656  0.02978516  0.00860596  0.13964844 -0.02563477 -0.03613281\n",
      "  0.11181641 -0.19824219  0.05126953  0.36328125]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the model, you can compute the distances between words."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "359E_KR7YcQG",
    "outputId": "1cfcd100-166f-4efe-94aa-f7a0c0c21817",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:03:03.791138Z",
     "start_time": "2025-01-23T09:03:03.787915Z"
    }
   },
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'car'\t'minivan'\t0.69\n",
      "'car'\t'bicycle'\t0.54\n",
      "'car'\t'airplane'\t0.42\n",
      "'car'\t'cereal'\t0.14\n",
      "'car'\t'communism'\t0.06\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofdJmw4mYqKz"
   },
   "source": "You can also find the most similar words to a given one."
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "OJ1Gj7JMYtvp",
    "outputId": "c4ebce20-8cb3-41cb-bcc4-0cc2617b037d",
    "ExecuteTime": {
     "end_time": "2025-01-23T09:03:10.767848Z",
     "start_time": "2025-01-23T09:03:09.666127Z"
    }
   },
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SUV', 0.8532192707061768), ('vehicle', 0.8175783753395081), ('pickup_truck', 0.7763689756393433), ('Jeep', 0.7567334175109863), ('Ford_Explorer', 0.7565719485282898)]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:03:13.209081Z",
     "start_time": "2025-01-23T09:03:13.138304Z"
    }
   },
   "source": [
    "vec_example = wv['king'] - wv['man'] + wv['woman']\n",
    "\n",
    "similars = wv.most_similar(positive=[vec_example])\n",
    "print(similars)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.8449392318725586), ('queen', 0.7300516366958618), ('monarch', 0.6454660296440125), ('princess', 0.6156251430511475), ('crown_prince', 0.5818676948547363), ('prince', 0.5777117609977722), ('kings', 0.5613663792610168), ('sultan', 0.5376776456832886), ('Queen_Consort', 0.5344247817993164), ('queens', 0.5289887189865112)]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T09:03:14.511239Z",
     "start_time": "2025-01-23T09:03:14.455011Z"
    }
   },
   "source": [
    "vec_example = wv['programmer'] - wv['man'] + wv['woman'] \n",
    "\n",
    "similars = wv.most_similar(positive=[vec_example])\n",
    "print(similars)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('programmer', 0.885962188243866), ('programmers', 0.6040860414505005), ('computer_programmer', 0.5623369216918945), ('coder', 0.5616979598999023), ('Programmer', 0.5576066374778748), ('programer', 0.5161396265029907), ('graphic_designer', 0.5139066576957703), ('coders', 0.48765403032302856), ('designer', 0.4822673797607422), ('librarian', 0.4649229943752289)]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
