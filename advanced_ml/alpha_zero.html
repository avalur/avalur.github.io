<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>From AlphaGo to AlphaZero</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Advanced machine learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>From AlphaGo to AlphaZero</h3>
								<br />
								Alex Avdiushenko<br />
								February 25, 2025
							</div>
					</div>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3>RL recap</h3>
                        <img src="images/rl_recap.png" width="80%" style="border-radius: 5%">
                    </section>
                    <section data-background-color="white">
                        <img src="images/mcmc_barto.png" width="50%" style="border-radius: 5%"><br>
                        <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Source: BartoSutton.pdf</a>
                    </section>
                    <section>
                        <h3>Policy gradient</h3>
                        <p style="text-align: left">We are trying to train just policy</p>
                            $$\pi(a|s, \theta) = Pr[a_t = a| s_t = s]$$

                        <div class="fragment">
                            <div class="typesetting">
                                <ol>
                                    <li>We can train stochastic policies, where $\pi(a|s) \neq 0,1$</li>
                                    <li>We even can train raw scores only,
                                        applying <span style="color: orange">Softmax</span>
                                        for moves probabilities</li>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Policy gradient theorem</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                $$
                                \theta_{t+1} = \theta_{t} + \alpha \nabla_\theta J(\theta_t) \\
                                J(\theta) = V_{\pi_\theta}(s_0) \underset{\theta}{\to} \max \\[0.25cm]
                                \boxed{\nabla_\theta J(\theta) \propto \sum\limits_s Pr_\pi[s]
                                \sum\limits_a Q_\pi(s,a) \nabla_\theta \pi(a|s)}
                                $$
                                <p style="text-align: left">
                                    Probabilities $Pr_\pi[s]$ are very hard to calculate,
                                    but we can just play using policy $\pi,$ and this gives us
                                    correct samples from this distribution</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="typesetting">
                            <p style="text-align: left">The REINFORCE algorithm is a policy-based method used in reinforcement learning.
                                Unlike value-based methods, which aim to learn a value function by interaction with
                                the environment, REINFORCE learns a stochastic policy directly.</p>
                            $${\nabla_\theta J(\theta) \propto \sum\limits_s Pr_\pi[s]
                                \sum\limits_a Q_\pi(s,a) \nabla_\theta \pi(a|s)} = \\
                                \quad = \mathbb{E}_{\pi} \left[ \sum\limits_a Q_\pi(S_t,a) \nabla_\theta \pi(a|S_t, \theta)\right]$$
                            <p style="text-align: left">So the SGD updates are</p>
                            $$\theta_{t+1} = \theta_{t} + \alpha \sum\limits_a \hat Q_\pi(S_t,a,\theta) \nabla_\theta
                            \pi(a|S_t, \theta),$$
                            <p style="text-align: left">
                                where $\hat Q_\pi(S_t,a,\theta)$ is some learned approximation of the action-value function $Q$.</p>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>TD-Gammon (1992)</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">TD-Gammon is a computer gammon program that uses artificial intelligence
                                    and machine learning techniques to play the game at a highly competitive level.

                                    Developed by Gerald Tesauro from IBM.</p>

                                <img src="images/TDgammon.png" width="50%" style="border-radius: 5%">
                            </div>
                        </div>
                        <aside class="notes">
                            TD-Gammon uses an algorithm called Temporal Difference (TD) learning.
                            The core idea behind TD learning is that it learns directly
                            from raw experience without any explicit supervisory signal.
                            And firstly, people think that it works because of randomness internally embedded to gammon.

                            TD-Gammon uses a multi-layer feed-forward neural network as its function approximator
                            to generalize from its previous gameplay experience.
                            The input to the network is the raw board position,
                            and the output is a single number representing the program's prediction of the expected outcome.

                            Despite being a program developed for a specific game,
                            TD-Gammon's approach is widely applicable and has greatly influenced many other machine learning projects.
                            It was one of the earliest successful demonstrations of a system that learns to improve
                            its performance from raw experience using neural networks.
                        </aside>
                    </section>
                    <section data-background-color="antiquewhite">
                        <img src="images/rl_algorithms_9_15.svg" width="1500" style="border-radius: 5%">
                    </section>
                    <section>
                        <h3>Game Go: <a href="https://www.youtube.com/watch?v=5PTXdR8hLlQ">Basic Rules</a></h3>
                            <img src="images/FloorGoban.jpeg" width="800" style="border-radius: 5%">

                        <aside class="notes">
                            For a better understanding of the next part,
                            let's talk a bit about the game of Go.

                            Its structure and rules are not very hard: a 19x19 grid, on which players alternately place
                            black and white chips in the nodes.
                            If a player's chip closes off an area, they "capture" all of its interior.

                            At the end of the game, the player with the most control (area) wins.
                        </aside>
                    </section>
                    <section>
                        <h3>History of Programs Playing Go</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <h3 style="text-align: left">1960-s: Beginnings</h3>
                                <ul style="text-align: left">
                                    <li>The first attempts to program a computer to play Go began in the 1960s</li>
                                    <li>The earliest Go programs could only play on smaller 9x9 or 13x13 boards and
                                    demonstrated limited capabilities due to the complexity of the game and technological constraints.
                                    1968: Albert Zobrist</li>
                                </ul>
                            </div>
                        </div>

                        <div class="fragment">
                            <div class="typesetting">
                                <h3 style="text-align: left">1980-1990-s: Incremental Improvements</h3>
                                <ul style="text-align: left">
                                    <li>Throughout the 80s and 90s, Go programs gradually improved with advancements
                                    in computing power and AI algorithms</li>
                                    <li>However, they were still far from matching the skills of human players.
                                    Goliath, Go Intellect, Handtalk, Goemate</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>The power of computer Go programs</h3>
                        <ul>
                            <li class="fragment">1997: Janice Kim (1p) won HandTalk program with a 25-stone handicap</li>
                            <li class="fragment">In the same year HandTalk program won 11-13 year old amateur 2-6 dans with 11-stone handicap</li>
                            <li class="fragment">2001: Many Faces program won Insei (1p) on a 25-stone handicap</li>
                            <li class="fragment">At the 2010 European Go Congress MogoTW played 19x19 Go against Catalin Taranu (5p).
                                MogoTW received a 7-stone handicap and won</li>
                            <li class="fragment">We know that DeepBlue defeated Garry Kasparov in 1997</li>
                        </ul>
                            <p class="fragment"><span style="color: orange">What is the problem? Why is Go harder than Chess for computer?</span></p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Schematic tree of possible moves in Go</h3>
                            <img src="images/moves_tree_in_go.png" width="1400" style="border-radius: 5%">

                        <aside class="notes">
                            Unlike Chess, for example, the tree of possible moves in Go expands greatly —
                            with over 200 variants for each next move.

                            Due to this, up until 2015, the best Go-playing programs could only reach an amateur level.
                            Some developers claimed that computers would never be able to beat the best human players.
                            However, most AI specialists believed that such a program would be developed,
                            but not before 2020–2025.
                        </aside>
                    </section>
                    <section>
                        <h3>Why is Go really hard for computer?</h3>

                        <ul>
                            <li>Huge branching factor</li>
                            <ul class="fragment">
                                <li>Chess: there are 30–40 possibilities, and lots of them are obviously weak</li>
                                <li>Go: there are about 250 possibilities, and about 100 from them are "reasonable"</li>
                            </ul>
                            <li>Hard to evaluate current position</li>
                            <ul class="fragment">
                                <li>Chess: one can count pieces material and there are simple heuristics</li>
                                <li>Go: it is really challenging even to understand who wins in the final position</li>
                            </ul>
                        </ul>

                        <aside class="notes">
                            There are at least two reasons why Go is hard, one is quantative, the other is qualitative.

                            Go has a much larger branching factor than Chess.

                            The qualitative reason is that it is way more challenging to evaluate the current position in Go.
                            Computers could not understand who is winning in Go. There was even a whole benchmark of
                            different final positions in Go, according to human evaluation. And the task was simply
                            to determine, who won.
                        </aside>
                    </section>
                    <section data-background-color="antiquewhite">
                        <h3 style="text-align: left">2000-s: Monte-Carlo Tree Search</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>In the 2000s, Monte Carlo Tree Search (MCTS) algorithms were introduced,
                                    resulting in significant improvements in the performance of Go programs</li>
                                    <li>We launch <em>random simulations</em> from the current position,
                                        observe which branches yield more wins, and then repeat the process</li>
                                </ul>

                                <img src="images/MCTS_svg.png" style="border-radius: 5%">
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Monte Carlo Tree Search — UCB1</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">The main part of MCTS is the formula for selection of the next node.
                                The UCB1 formula looks like this (the same as in multi-armed bandits):</p>

                                $$ UCB1 = \frac{w_i}{n_i} + c \sqrt{\frac{\ln t}{n_i}} $$
                                <p style="text-align: left">where:</p>
                                <ul>
                                    <li>$w_i$ is the number of wins</li>
                                    <li>$n_i$ is the count of simulations for the node associated with action $i$</li>
                                    <li>$c$ is the parameter, by default $c=\sqrt{2}$</li>
                                    <li>$t = \sum\limits_i n_i$ is the total count of simulations for the parent node</li>
                                </ul>

                                <p style="text-align: left">
                                    The algorithm prefers actions with higher estimated rewards and higher uncertainties.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section data-background-image="images/lee_sedol.png">
                        <span style="color: white">
                            <div class="fragment">
                                <div class="typesetting">
                                    <h3 style="text-align: left;color: white"><p>AlphaGo vs. Lee Sedol</p>
                                    9-15 March 2016</h3>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="typesetting">
                                    <p style="text-align: left;font-size: 52px"> Forecast was 1 : 4 </p>
                                </div>
                            </div>
                            <div class="fragment">
                                <div class="typesetting">
                                    <p style="text-align: left;font-size: 52px"> Result was 4 : 1 </p>
                                </div>
                            </div>
                        </span>
                        <br><br><br><br><br><br><br><br><br><br><br><br>
                        <aside class="notes">
                            And then, March 2016 became.
                            This is a photo from a Go match in which the legendary Korean player Lee Sedol,
                            one of the best in the world, is playing against an AI-based program, AlphaGo.

                            Before the match, Lee Sedol, in fact being a very modest person,
                            said that he would either win cleanly 5:0,
                            or he might underestimate the strength of the program and somewhere due to a mistake,
                            he would win 4:1.

                            What do you think the result was? The computer won with a score of 4:1.
                            It was so-called "sputnik"-moment for all Asia, when they truly understood that
                            deep learning is huge thing.
                        </aside>
                    </section>
                    <section>
                        <h3>History of AlphaGo</h3>
                        <div class="fragment">
                            <div class="typesetting">
                            <ul>
                                <li>AlphaGo is a computer program developed by Google DeepMind in 2014-2015:
                                David Silver, Aja Huang etc.</li>
                                <li><strong>October 2015:</strong> AlphaGo plays its first match against
                                    the reigning three-time European Go champion, Fan Hui (2p), and wins 5:0.
                                    This is the first time a computer Go program has beaten
                                    a human professional player without handicaps on a full-sized 19x19 board.</li>
                                <li>Paper in Nature
                                    <a href="https://www.nature.com/articles/nature16961">
                                    "Mastering the game of Go with deep neural networks and tree search"</a></li>
                                <li><strong>March 2016:</strong> AlphaGo plays a five-game match against Lee Sedol,
                                    one of the world's top Go players, and wins 4:1</li>
                                <li><strong>May 2017:</strong> AlphaGo plays a three-game match against Ke Jie,
                                    the world No.1 ranked player at the time, and wins 3:0</li>
                            </ul>
                            <p style="text-align: left">The development of AlphaGo was a significant achievement
                            in the field of Artificial Intelligence.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>What is inside AlphaGo?</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <ol>
                                    <li>
                                        Supervised learning (SL) policy network for next move prediction:
                                        <ul>
                                            <li>13-layers CNN for features extraction from the board</li>
                                            <li>30 billion positions from human expert games</li>
                                            <li>Moves probability distribution as output: $p_\sigma(a \mid s)$</li>
                                            <li>Accuracy reaches 57%, which is very high</li>
                                            <li>Also train fast and worse strategy $p_\pi$ (accuracy is 24%,
                                                but inference time is much less: 3 millisecond $\to$ 2 nanosecond)</li>
                                        </ul>
                                    </li>
                                </ol>
                            </div>
                        </div>

                        <aside class="notes">
                        </aside>
                    </section>
                    <section>
                        <ol>
                            <li>SL policy network predicts the next move</li>
                        <div class="fragment">
                            <div class="typesetting">
                            <li>RL policy network improves SL using reinforcement learning:</li>
                            <ul>
                                <li>the same structure of 13-layers CNN</li>
                                <li>initialize with SL policy network</li>
                                <li>self plays with one of the previous iterations of RL policy</li>
                                <li>new weights maximize the probability of winning</li>
                                <li>RL policy wins 80% games versus SL and 85% versus Pachi
                                    (one of the best MCTS programs)</li>
                                <li>and this is without any MCTS-counting, due to only the strong moves' prediction!</li>
                            </ul>
                            </div>
                        </div>
                        </ol>
                    </section>
                    <section>
                        <ol>
                            <li>SL policy network predicts the next move</li>
                            <li>RL policy network improves SL using reinforcement learning</li>
                            <div class="fragment">
                                <div class="typesetting">
                                    <li>Also, they train function $V_\theta(s)$ for state evaluation:</li>
                                    <ul>
                                        <li>the same structure of 13-layers CNN,
                                            but now output is the probability of winning</li>
                                        <li>cannot train on the human expert's games only because of overfitting</li>
                                        <li>so they used self-play games from the RL policy network</li>
                                        <li>as a result, they got a great tool for position evaluation, which
                                            is 15K times faster than MCTS + RL</li>
                                    </ul>
                                </div>
                            </div>
                        </ol>
                    </section>
                    <section>
                        <ol>
                            <li>SL policy network predicts the next move</li>
                            <li>RL policy network improves SL using reinforcement learning</li>
                            <li>Function $V_\theta(s)$ for state evaluation</li>
                            <div class="fragment">
                                <div class="typesetting">
                                    <li>Eventually we can use MCTS counting:</li>
                                    <ul>
                                        <li>build an MCTS-like tree</li>
                                        <li>apriori probabilities initialize as $p_\sigma(a \mid s)$ from SL policy</li>
                                        <li>in each node of a tree for state evaluation
                                            we use $V_\theta(s)$, combined with the random rollout of self-played
                                            fast strategy $p_\pi$</li>
                                        <li>it is interesting that SL policy works better for tree building,
                                            but for training function $V_\theta(s)$ is better use stronger RL policy</li>
                                    </ul>
                                </div>
                            </div>
                        </ol>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>AlphaZero</h3>

                        <p style="text-align: left">AlphaZero is a more generalized and simple variant
                            of the AlphaGo algorithm, and is capable of playing Chess, Shogi and Go.</p>

                            <div class="fragment">
                                <div class="typesetting">
                                <ul>
                                    <li>AlphaZero is a reinforcement learning system,
                                        it learns to play by playing games against itself
                                        and improving from its mistakes only</li>
                                    <li>Silver et al., 2017:
                                        <a href="https://arxiv.org/abs/1712.01815">"Mastering Chess and Shogi
                                            by Self-Play with a General Reinforcement Learning Algorithm"</a></li>
                                    <li>AlphaZero also outperformed Stockfish,
                                        one of the top-ranked chess engines,
                                        winning 28 games and drawing the rest in a 100-game match</li>
                                </ul>
                                <img src="images/AlphaZeroPlots.png" style="border-radius: 5%">
                                </div>
                            </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Inside AlphaZero</h3>
                            <div class="fragment">
                                <div class="typesetting">
                                    <ul>
                                        <li>we have only one deep neural network for next move prediction and position evaluation</li>
                                        <li>it self-plays and simultaneously builds an MCTS tree</li>
                                        <li>it improves both the NN weights (via RL) and MCTS tree</li>
                                    </ul>

                                    <img src="images/AlphaGoZero.png" style="border-radius: 3%;width: 40%">
                                </div>
                            </div>
                    </section>
                    <section>
                        <p style="text-align: left">We minimize</p>
                        $$ Loss = (z-v)^2 - \pi^T \cdot \log p + c \|\theta\|^2 $$
                        <p style="text-align: left">
                            where $p$ is a vector with moves probabilities from NN,
                            $\pi$ — improved by MCTS vector of probabilities,
                            $z \in \mathbb{R}$ and $v \in \mathbb{R}$ are value function estimations from MCTS and NN,
                            $\theta$ are weights of NN, and $c$ is regularization coefficient
                        </p>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>We started with policy gradient descent (the central method in robotics)</li>
                                    <li>We got acquainted with the AlphaGo and AlphaZero models</li>
                                    <li>Reinforcement learning allows you to pass a gradient through a sequence
                                        of discrete steps with a reward at the end</li>
                                    <li>Best used in tasks where there is a discrete sequence of actions and states</li>
                                </ul>
                            </div>

                        </div>
                        <br><br>
                        <div class="fragment">
                            <div class="typesetting">
                                <h3 style="text-align: left">What else can you look at?</h3>
                                <ul>
                                    <li><a href="https://spinningup.openai.com/en/latest">Educational resource on RL</a> produced by OpenAI</li>
                                    <li><a href="https://www.davidsilver.uk/teaching/">Intro to RL</a> with David Silver, DeepMind × UCL, 2015</li>
                                    <li><a href="https://www.youtube.com/watch?v=KW9KAWmu4nI">Lecture by S. Nikolenko</a> at Computer Science Club (in Russian)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>