<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>NLP crash intro</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Advanced machine learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>NLP crash intro</h3>
								<br />
								Alex Avdiushenko<br />
								March 18, 2025
							</div>
					</div>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">How to represent the meaning of a word?</h3>

                        <img src="images/wordnet.png" width="70%" style="border-radius: 5%">
                        <p>It is a thesaurus
                            <a href="https://wordnet.princeton.edu/">developed since 2005</a>
                        </p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Problems with WordNet</h3>
                        <div class="column">
                            <ul>
                                <li>New meaning of words, impossible to keep up-do-data</li>
                                <li>Subjective</li>
                                <li>Manually curated</li>
                                <li>Impossible to accurately compute word similarity</li>
                            </ul>
                        </div>
                        <div class="column">
                            <img src="images/WNV_Viz_Synset_Programmer_en.png" width="80%" style="border-radius: 5%;text-align: right">
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Distributional Hypothesis</h3>
                        <p style="text-align: left">
                            A word’s meaning is given by the words that frequently appear close-by.</p><br>
                        <ul class="fragment">
                            <li>“You shall know a word by the company it keeps” (J. R. Firth 1957)</li>
                            <li>One of the most successful ideas of modern statistical NLP!</li>
                        </ul>
                        <br><br>
                        <p class="fragment" style="text-align: left">
                        When a word $w$ appears in a text, its context is the set of words that appear nearby
                        (within a fixed-size window). We use the many contexts of $w$ to build up
                        a representation of $w$.</p>
                        <p class="important r-frame fragment" style="text-align: center">
                            Example: Among all pets, **???** are the best at catching mice.</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Word2Vec</h3>
                        <h4 style="text-align: left">Motivation</h4>
                        <p style="text-align: left">One-hot encoding representations:</p>
                        <ul>
                            <li>Do not reflect the semantic similarity of words</li>
                            <li>Have too high dimensionality</li>
                        </ul>
                        <div class="fragment" style="margin-top: 1.5cm;">
                            <p style="text-align: left">One-hot simplified example:</p>
                            <ul>
                                <li>Gym  $\to  [0, 1, 0, 0, 0, 0]$</li>
                                <li>Sofa $\to  [0, 0, 1, 0, 0, 0]$</li>
                                <li>Rest $\to  [0, 0, 0, 1, 0, 0]$</li>
                            </ul>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Continuous Bag of Words (CBOW) and Skip-gram</h3>
                        <div class="column" style="font-size: large;text-align: center">
                            \[
                            \frac{1}{T}\sum\limits_{t=1}^T \log(p(w_{t}|w_{t-m}, \dots, w_{t-1}, w_{t+1}, \dots, , w_{t+m}))
                            \]
                        </div>
                        <div class="column" style="font-size: large;text-align: center">
                            \[
                            \frac{1}{T}\sum\limits_{t=1}^T \sum\limits_{-m \leq j \leq m, j \neq 0} \log(p(w_{t-j}|w_{t}))
                            \]
                        </div>
                        <div style="text-align: center;">
                            <img src="images/CBOW and skip-gram.jpg" alt="CBOW and Skip-gram" style="width: 85%;border-radius: 5%">
                        </div>
                        <hr style="border: 0.4pt solid black;">
                        <div style="text-align: left;">
                            <em>Mikolov, Corrado, Chen, Dean</em> "Efficient Estimation of Word Representations in Vector Space", 2013
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">CBOW Model</h3>
                        <p style="text-align: left">The probability of word \(w_t\) in a given context:</p>
                            \[C_t = (w_{t-m}, \dots, w_{t-1}, w_{t+1}, \dots, w_{t+m})\]
                            \[p(w_{t} = w|C_t) = \underset{w \in W}{\text{Softmax}} \left< u_w, v^{-t} \right>\]
                        <p style="text-align: left">\(v^{-t} = \frac{1}{2m} \sum\limits_{w \in C_t} v_w\) — the average vector of words from the context \(C_t\)</p>
                        <p style="text-align: left">\(v_w\) — vectors of predicting words,</p>
                        <p style="text-align: left">\(u_w\) — vector of the predicted word, generally \(u_w \neq v_w\)</p>

                        <p style="text-align: left"><strong>Criterion</strong> of maximum log-likelihood,
                            \(U, V \in \mathbb{R}^{|W| \times d}\):</p>
                            <p style="text-align: center;">
                                \(\sum\limits_{t=1}^n \log p(w_t|C_t) \to \max\limits_{U, V}\)
                            </p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Skip-gram: How to Calculate Probabilities</h3>
                            <div class="column" style="width: 50%;">
                                <p>\( p(w_o|w_c) = \frac{\exp[v(w_o)u^T(w_c)]}{\sum\limits_{w=1}^W \exp[v(w)u^T(w_c)]}\)</p>
                                <p style="text-align: left">\(W\) — the set of all dictionary words</p>
                                <p style="text-align: left">\(w_c\) — central word</p>
                                <p style="text-align: left">\(w_o\) — context word</p>
                                <p>\(u(\cdot)\) and \(v(\cdot)\) — parameter vectors (embeddings), which are multiplied scalar-wise</p>
                            </div>
                            <div class="column" style="width: 40%;">
                                <div style="text-align: center;">
                                    <img src="images/skip-gram-p.jpg" alt="Skip-gram" style="width: 80%;border-radius: 5%">
                                </div>
                            </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">What’s the problem with this approach?</h3>
                        <div class="fragment">
                            <p>In the denominator, of course, where all the words of the vocabulary are!</p>
                        </div>
                        <div class="fragment" style="margin-top: 1cm;">
                            <p style="text-align: left"><strong>Mikolov</strong> proposed...</p>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Hierarchical Softmax</h3>
                        <p style="text-align: left">
                            We model the probability more efficiently by building a Huffman tree on the words, and then:</p>
                        <div class="column">
                        \[ 1 - \sigma(x) = \sigma(-x) \]
                        \[ p(w_o|w_c) = \\
                            \prod\limits_{n \in \text{Path}(w_o)} \sigma(d_{nw_o} v(n) u^T(w_c))\]
                        <p style="text-align: left">Here, \(v(n)\) is the trainable vector in the tree node,
                            \(d_{nw_o} = 1\) if \(w_o\) is in the right subtree, \(d_{nw_o} = -1\) otherwise.</p>
                        </div>
                        <div class="column">
                            <img src="images/haffman.jpg" alt="Huffman Tree" style="width: 100%;border-radius: 5%">
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">doc2vec</h3>
                            \[u^\prime (\text{doc}) = \sum\limits_{w \in \text{doc}} \omega_w \cdot u(w)\]
                        <p style="text-align: left">
                            As word weights \(\omega_w\), it makes sense to use
                            TF-IDF (term frequency / inverse document frequency)
                        </p>
                    </section>
                    <section>
                        <h3 style="text-align: left">GloVe (Global Vectors for Word Representation)</h3>
                        <p style="text-align: left"><strong>Motivation:</strong>
                            Unlike Word2Vec, which is a predictive model,
                            GloVe is a count-based model that leverages global statistical information
                            about word occurrences.</p>
                        <div class="fragment">
                        <p style="text-align: left">The model captures word relationships using the following formula:</p>
                            <p style="text-align: center;">
                                \(X_{ij}\) — co-occurrence matrix where \(X_{ij}\) is the frequency of word \(j\) in the context of word \(i\)
                            </p>
                            \[ \log(X_{ij}) = w_i^T \tilde{w}_j + b_i + \tilde{b}_j \]
                            <p style="text-align: left">This equation models the ratio of co-occurrence
                                probabilities to capture semantic relationships between words.</p>
                        </div>
                        <div class="fragment" style="text-align: left;">
                            <hr style="border: 0.4pt solid black;">
                            <em>Pennington, Socher, Manning</em> "GloVe: Global Vectors for Word Representation", 2014
                        </div>
                    </section>
                    <section data-background-color="white">
                        <img src="images/word_emb_scheme.png" alt="word_emb_scheme" width="60%">
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Large Language Model $\to$ ChatGPT</h3>
                    </section>
                    <section data-background-image="images/gpt-assistant-pipeline.jpeg">
                    </section>
                    <section>
                        <h3>Datasets for pretraining stage</h3>
                        <a href="https://ar5iv.labs.arxiv.org/html/2101.00027">
                            <img src="images/pile_chart2.png" alt="pile_chart" width="80%">
                        </a>
                    </section>
                    <section>
                        <p style="text-align: left">
                            Pretraining base LLMs involves training on massive and
                            diverse text datasets to capture a wide range of linguistic patterns,
                            knowledge, and contextual understanding.</p>
                        <ul class="fragment">
                            <li><strong>Common Crawl:</strong> A vast dataset of web pages, capturing diverse topics and writing styles</li>
                            <li><strong>Wikipedia:</strong> A comprehensive and structured dataset of human knowledge across various domains</li>
                            <li><strong>BooksCorpus:</strong> A dataset of published books offering rich narrative structures and varied vocabulary</li>
                            <li><strong>OpenWebText:</strong> A curated dataset of high-quality web content similar to Reddit discussions</li>
                            <li>...</li>
                        </ul>
                    </section>
                    <section>
                        <h3 style="text-align: left">Perplexity in LLMs</h3>
                        <p style="text-align: left">In the context of language models, is defined as
                        <span class="important">the exponential of the average log-likelihood per word
                            in a given test set.</span></p>
                        <p style="text-align: left">It can be interpreted as the weighted average number of choices a model has
                            when predicting the next item in a sequence.
                            <span class="important">A lower perplexity score indicates a better predictive model</span>
                            because it suggests the model has fewer choices, hence less uncertainty.
                        <div class="fragment">
                            \[
                            P(W) = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log p(w_i | w_1, \dots, w_{i-1})\right)
                            \]
                            <ul>
                                <li>Perplexity evaluates the model's uncertainty in predicting the next word</li>
                                <li>High perplexity suggests the model is less certain and might be guessing among many options</li>
                            </ul>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Unexpected abilities begin to emerge</h3>
                        <img src="images/emergent_abilities.gif" alt="emergent_abilities">
                    </section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul style="text-align:left;">
                                    <li><strong>Word Representation:</strong>
                                        Traditional methods like WordNet have limitations.
                                        Approaches like Word2Vec, CBOW, Skip-gram, and GloVe
                                        effectively capture semantic relationships.</li>
                                    <li><strong>Language Models:</strong>
                                        Large language models (LLMs) like BERT and GPT are trained on diverse datasets.
                                        Perplexity is one of the key metrics for evaluating their predictive performance.</li>
                                    <li><strong>Emergent Abilities:</strong> As LLMs scale, they exhibit unexpected capabilities,
                                        enhancing their utility in various NLP tasks.</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">What else can you explore?</p>
                                <ul style="text-align:left;">
                                    <li>Stanford class cs224n:
                                        <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture09-pretraining-updated.pdf">
                                            lecture09 pretraining</a></li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>