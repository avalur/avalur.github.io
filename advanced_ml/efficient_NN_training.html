<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>After pretraining steps: RLHF, DPO and prompting</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
			ul.custom-list {
				list-style: none;
				padding: 0;
			}
			ul.custom-list li::before {
				content: '+';
				margin-right: 8px;
                color: green;
                font-size: larger;
			}
			ul.custom-list li.minus::before {
				content: '-';
                color: red;
                font-size: larger;
			}
			ul.custom-list li.lamp::before {
				content: '💡';
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div>
                            <img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
                        </div>
                        <h2>Advanced machine learning</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                                <div class="typesetting">
                                    <h3>Efficient Neural Network Training</h3>
                                    <br />
                                    Alex Avdiushenko<br />
                                    April 29, 2025
                                </div>
                        </div>
                    </section>
                    <section>
                      <h3 style="text-align: left">Lecture Plan</h3>
                      <ol>
                        <li>Mixed Precision Training</li>
                        <li>Multi-GPU Training with DDP/FSDP</li>
                        <li>Parameter Efficient Finetuning: LoRA</li>
                      </ol>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Floating Points 101</h3>
                        <p style="text-align: left">FP32</p>
                        <p class="fragment" data-fragment-index="0" style="background-color: #FFC0CB; padding: 5px; border-radius: 5px; display: inline-block;">
                            Memory requirement: 4 bytes
                        </p><br>
                        <div class="fragment" data-fragment-index="0" style="text-align: center;">
                            <img src="images/fp32.png" alt="FP32 Structure" style="width: 80%;">
                        </div>
                        <p class="fragment" style="text-align: center">
                            $\left(-1\right)^{S} \times 2^{E-127} \times \left(1 + \sum\limits_{i=1}^{23} b_{23-i} 2^{-i}\right)$
                        </p>
                        <p>
                            <span class="fragment" style="background-color: #FFC0CB; padding: 5px; border-radius: 5px; display: inline-block;">
                                range
                            </span>
                            <span style="padding: 60px"></span>
                            <span class="fragment" style="background-color: #FFC0CB; padding: 5px; border-radius: 5px; display: inline-block;">
                                precision
                            </span>
                        </p><br>
                        <p class="fragment" style="text-align: left">
                            Can represent $[2^k, 2^k(1+\varepsilon), 2^k(1+2\varepsilon), \dots, 2^{k+1}]$, where $\varepsilon = 2^{-23}$
                        </p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Floating Points 101</h3>
                        <br>
                        <img src="images/fp16fp32.png" alt="fp16fp32">
                        <br><br>
                        <div class="fragment">
                            <h3 style="text-align: left">Half-precision for training neural networks?</h3>
                            <ul>
                                <li>Standard Neural Network Training:
                                    Model parameters and gradients represented in FP32
                                    (CUDA Out-Of-Memory errors with large models)</li>
                                <li>Possible solution: Use FP16!</li>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                          <p style="text-align: left">Possible solution: Use FP16!</p>
                          <ul>
                            <li>Less range: Roughly 2e-14 to 2e15 on both sides</li>
                            <li>Smaller precision leads to rounding errors: 1.0001 is 1 in half precision</li>
                            <li>For Neural Net training:
                                <ul>
                                    <li><span style="color: orangered">Gradients can underflow</span></li>
                                    <li><span style="color: orangered">Weight updates are imprecise</span></li>
                                </ul>
                            </li>
                          </ul>
                        <br><br>
                        <img src="images/torchFP16.png" alt="torchFP16">
                    </section>
                    <section data-background-color="white">
                      <h3 style="text-align: left">Solution: Mixed Precision Training</h3>
                      <p><strong>Still use FP16, but use FP32 for neural network updates!</strong></p>
                          <ol>
                            <li>Maintain a copy of model parameters in FP32 (Master weights)</li>
                            <li>Run forward pass in FP16</li>
                              <li><span class="important">Scale loss</span> by a large value
                                  (to artificially increase gradient)</li>
                            <li>Compute gradient in FP16</li>
                              <li>Copy gradient into FP32 and <span class="important">divide by scale factor</span></li>
                            <li>Update master weights in FP32 <span style="color: #E0115F;">[fixes weight update issue!]</span></li>
                            <li>Copy into FP16 version</li>
                          </ol>
                      <div style="text-align: left; margin-top: 1rem;">
                        <a href="https://arxiv.org/abs/1710.03740"><em>Narang et al. 2018</em></a>
                      </div>
                    </section>
                    <section data-background-color="white">
                        <img src="images/fp16range.png" alt="fp16range" width="80%">
                    </section>
                    <section data-background-color="white">
                        <h3>Can we get rid of gradient scaling?</h3>
                        <ul class="custom-list">
                            <li class="minus"> We need scaling because FP16 has a small range compared to FP32</li>
                            <li class="lamp">Can we allocate 8 bits for exponent (same range) while sacrificing precision?</li>
                            <br>
                            <li class="fragment" data-fragment-index="1">Bfloat16 can represent smaller numbers and much larger numbers:</li>
                        </ul>
                        <img class="fragment" data-fragment-index="1" src="images/Bfloat16.png" alt="Bfloat16">
                    </section>
                    <section data-background-color="white">
                        <img src="images/bfloat16res.png" alt="bfloat16res" width="70%">
                        <p><a href="https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html">
                            Source: Sebastian Raschka blog
                        </a></p>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Multi-GPU Training</h3>
                        <div style="text-align: left;">
                            <ul>
                                <li><strong>What's stored on GPU VRAM?</strong></li>
                                <ul>
                                    <li>Model parameters (in FP16)</li>
                                    <li>Optimizer: Master weights (FP32) + Adam momentum (FP32) + Adam variance (FP32)</li>
                                </ul>
                            </ul>
                        </div>
                        <div class="fragment" data-fragment-index="1">
                        $$
                        \begin{align*}
                            m_t &= \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot \nabla w_t \\
                            v_t &= \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot (\nabla w_t)^2 \\
                            \hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \quad
                            \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\
                            w_{t+1} &= w_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \cdot \hat{m}_t
                        \end{align*}
                        $$
                        </div>
                        <p class="fragment" data-fragment-index="1" style="text-align: left">Source: <a href="https://web.stanford.edu/class/cs224n/slides/cs224n-spr2024-lecture12-training-shikhar.pdf">
                            Stanford course CS224n, slides 24-50</a></p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Fully Sharded Data Parallel (FSDP)</h3>
                        <img src="images/FSDP.png" alt="FSDP" width="60%">
                        <div class="fragment">
                            <p style="text-align: left">High-level sketch:</p>
                            <ol>
                                <li>Divide model parameters into FSDP units</li>
                                <li>Shard each unit across multiple GPUs</li>
                                <li>Run forward pass</li>
                                <li>Run backward pass</li>
                                <li><strong>Each GPU updates its own shard using the full gradient received earlier</strong></li>
                            </ol>
                        </div>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                      <h3>From Fine-Tuning to Parameter-Efficient Fine-Tuning</h3>
                      <div class="row">
                        <div class="column">
                          <h4>Full Fine-tuning</h4>
                          <img src="images/fullFT.png" alt="Full Fine-Tuning" style="width: 40%;"/>
                            <p>Update <strong>all model parameters</strong></p>
                        </div>
                        <div class="column">
                          <h4>PEFT</h4>
                          <img src="images/PEFT.png" alt="PEFT" style="width: 40%;"/>
                            <p>Update <strong>a small subset</strong> of model parameters</p>
                        </div>
                      </div>
                    </section>
                    <section data-background-color="white">
                          <ul>
                            <li>Why fine-tune only <span class="important">some</span> parameters?</li>
                            <li>Fine-tuning <span class="important">all</span> parameters is impractical with large models</li>
                            <li>PEFT matches performance of full fine-tuning</li>
                          </ul>
                        <br><br>
                        <a href="https://arxiv.org/abs/1906.02243">
                            <img class="fragment" data-fragment-index="0" src="images/LLM_train_costs.png" alt="LLM_train_costs">
                            <p class="fragment" data-fragment-index="0">
                                Source: Strubell et al. 2019, Energy and Policy Considerations for Deep Learning in NLP</p>
                        </a>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Full Finetuning</h3>
                        <div style="text-align: left;">
                            <ul>
                                <li>Assume we have a pre-trained autoregressive language model
                                    \( P_{\phi}(y|x) \)</li>
                                <ul>
                                    <li>E.g., GPT based on Transformer</li>
                                </ul>
                                <li>Adapt this pre-trained model to downstream tasks
                                    (e.g., summarization, NL2SQL, reading comprehension)</stron</li>
                                <ul>
                                    <li>Training dataset of context-target pairs
                                        \( \{(x_i, y_i)\}_{i=1,...,N} \)</li>
                                </ul>
                                <li>During full fine-tuning,
                                    we update \( \phi_0 \) to \( \phi_0 + \Delta \phi \)
                                    by following the gradient to maximize the conditional
                                    language modeling objective</li>
                                <div class="fragment" data-fragment-index="1">
                                $$
                                \max_{\phi} \sum_{(x, y)} \sum_{t=1}^{|y|} \log(P_{\phi}(y_t|x, y_{< t}))
                                $$
                                </div>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">PEFT</h3>
                        <div style="text-align: left;">
                            <ul>
                                <li>For each downstream task, we learn a different set of parameters
                                    \( \Delta \phi \)</li>
                                <ul class="fragment">
                                    <li>\(| \Delta \phi | = | \phi_0 |\)</li>
                                    <li>GPT-3 has a \( | \phi_0 | \) of 175 billion</li>
                                    <li><span style="color: red;">
                                        Expensive and challenging for storing and deploying many independent instances</span></li>
                                </ul>
                                <li class="fragment">Can we do better?</li>
                                <li class="fragment"><span style="color: red;">Key idea:</span> encode the task-specific parameter
                                    increment \( \Delta \phi = \Delta \phi (\Theta) \) by a smaller-sized set of parameters \( \Theta \),
                                    \( | \Theta | \ll | \phi_0 | \)
                                </li>
                                <li class="fragment">The task of finding \( \Delta \phi \) becomes optimizing over
                                    \( \Theta \)
                                </li>
                                <div class="fragment">
                                $$
                                \max_{\Theta} \sum_{(x, y)} \sum_{t=1}^{|y|}
                                    \log(P_{\phi_0 + \Delta \phi(\Theta)}(y_t|x, y_{< t}))
                                $$
                                </div>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Low-rank-parameterized update matrices</h3>
                        <div class="column">
                            <ul>
                                <li>Updates to the weights have a low “intrinsic rank”
                                    during adaptation
                                    <a href="https://arxiv.org/abs/2012.13255">[Aghajanyan et al. 2020]</a></li>
                                <li>\( W_0 \in \mathbb{R}^{d \times k} \): a pre-trained weight matrix</li>
                                <li>Constrain its update with a low-rank decomposition:</li>
                            </ul>
                            <div class="fragment" data-fragment-index="1">
                                $$
                                W_0 + \Delta W = W_0 + \alpha B A
                                $$
                                $$
                                \text{where} \ B \in \mathbb{R}^{d \times r}, \ A \in \mathbb{R}^{r \times k}, \ r \ll \min(d, k)
                                $$
                            </div>
                        </div>
                        <div class="column">
                            <img src="images/LoRA.png" alt="Low-rank parameterization diagram">
                        </div>
                            <ul class="fragment">
                                <li>\(\alpha\) is the tradeoff between pre-trained “knowledge” and task-specific “knowledge”</li>
                                <li>Only \( A \) and \( B \) contain <span class="important">trainable</span> parameters</li>
                            </ul>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Low-rank-parameterized update matrices</h3>
                        <div class="column">
                            <ul>
                                <li>As one increases the number of trainable parameters,
                                    training LoRA converges to training the original model
                                </li>
                                <li><strong class="important">No additional inference latency:</strong>
                                    when switching to a different task, recover \( W_0 \) by subtracting \( B A \)
                                    and adding a different \( B'A' \)</li>
                                <li>Often LoRA is applied to the weight matrices in the self-attention module</li>
                            </ul>
                        </div>
                        <div class="column">
                            <img src="images/LoRA.png" alt="Low-rank parameterization diagram">
                        </div>
                    </section>
                    <section data-background-color="white">
                        <img src="images/lora-llm.png" alt="lora-llm" width="80%">
                        <p>Source: <a href="https://lightning.ai/pages/community/article/lora-llm/">
                            https://lightning.ai/pages/community/article/lora-llm/</a></p>
                    </section>
                </section>
                <section>
                  <h3 style="text-align: left">Summary</h3>
                  <ol>
                    <li class="fragment">
                      <strong>Mixed Precision Training</strong>
                      <ul class="custom-list">
                        <li>FP32 requires a large amount of memory; FP16 reduces memory footprint</li>
                        <li class="minus">FP16 has a smaller range and lower precision,
                            leading to potential issues with gradient updates</li>
                        <li>Solution: Mixed precision with FP32 for weights and FP16 for forward pass,
                            using scaling techniques or BFloat16</li>
                      </ul>
                    </li>
                    <li class="fragment">
                      <strong>Multi-GPU Training</strong>
                      <ul class="custom-list">
                        <li>Data Parallelism (e.g., DDP) and FSDP
                            help distribute model parameters and computation across GPUs</li>
                        <li class="minus">FSDP is complex to set up but offers better memory efficiency</li>
                      </ul>
                    </li>
                    <li class="fragment">
                      <strong>Low-Rank Adaptation (LoRA)</strong>
                      <ul class="custom-list">
                        <li>Reduces the number of trainable parameters by representing updates in a low-rank form,
                            often applied in self-attention layers</li>
                        <li class="minus">Can incur a slight increase in training complexity
                            but maintains inference speed</li>
                      </ul>
                    </li>
                  </ol>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>