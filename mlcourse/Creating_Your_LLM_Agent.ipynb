{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Your LLM Agent\n",
        "\n",
        "Your task is to create an agent where a powerful LLM (**Meta-Llama-3.1-405B-Instruct**) either:\n",
        "\n",
        "- Answers important queries itself\n",
        "- Delegates simpler queries to a smaller LLM (**Meta-Llama-3.1-8B-Instruct**)\n",
        "\n",
        "Example system prompt:\n",
        "\n",
        "```\n",
        "\"\"\"You are a powerful Large Language Model helping businesses and hobbyists worldwide. Being busy, you can't waste compute on mundane questions while existential tasks await. Your associate, Meta-Llama-3.1-8B-Instruct, handles simple tasks efficiently. Delegate unworthy questions to it so you can focus on challenging tasks.\"\"\"\n",
        "```\n",
        "\n",
        "**Here's how to proceed:**\n",
        "\n",
        "1. Complete the code below to make the agent work. This includes writing a tool for calling the small LLM.\n",
        "2. Experiment with different prompts to understand which are deemed worthy by the larger model.\n",
        "3. Try modifying the system prompt (e. g. make it more business-like) and observe changes.\n",
        "\n",
        "Note that for this task, you don't need to use any agentic frameworks — the goal is to understand the fundamentals. For more advanced implementations, frameworks like [LangGraph](https://www.langchain.com/langgraph) would be suitable."
      ],
      "metadata": {
        "id": "2rjfyUfOGUSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e09mWCHJHeDe",
        "outputId": "7981b509-56cb-4ed3-deef-447c3da7a0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "with open(\"nebius_api_key\", \"r\") as file:\n",
        "    nebius_api_key = file.read().strip()\n",
        "\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key"
      ],
      "metadata": {
        "id": "lfcvilOoHjeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "import shlex\n",
        "from openai import OpenAI\n",
        "\n",
        "class BusyAssistant:\n",
        "    def __init__(self, busy_client, errand_client, busy_model, errand_model):\n",
        "        \"\"\"Initialize the assistant with your OpenAI and Nebius API keys.\"\"\"\n",
        "        self.busy_model = busy_model\n",
        "        self.errand_model = errand_model\n",
        "\n",
        "        self.busy_client = busy_client\n",
        "        self.errand_client = errand_client\n",
        "\n",
        "\n",
        "        # Define the errand_call tool\n",
        "        self.tools = [\n",
        "            # <YOUR CODE HERE>\n",
        "        ]\n",
        "\n",
        "    def errand_call(self, prompt: str, ) -> Dict[str, Any]:\n",
        "        \"\"\"Call a small model.\"\"\"\n",
        "        try:\n",
        "            # <YOUR CODE HERE>\n",
        "\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"completion\": completion.choices[0].message.content #completion,\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "\n",
        "    def process_tool_call(self, tool_call: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Process a tool call from the API response.\"\"\"\n",
        "        # <YOUR CODE HERE>\n",
        "\n",
        "    def chat(self, user_message: str, verbose=False) -> str:\n",
        "        \"\"\"Main chat function that processes user input and returns assistant response.\"\"\"\n",
        "        completions = []\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": # <WHICH WILL YOU CHOOSE?>\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_message\n",
        "                }\n",
        "            ]\n",
        "\n",
        "        try:\n",
        "            # Get initial response from the busy client\n",
        "            completion = self.busy_client.chat.completions.create(\n",
        "                model=self.busy_model,\n",
        "                messages=messages,\n",
        "                tools=self.tools,\n",
        "                tool_choice=\"auto\"\n",
        "            )\n",
        "\n",
        "            # completions.append(completion)\n",
        "            message = completion.choices[0].message\n",
        "\n",
        "            # Process tool calls if any\n",
        "            while message.tool_calls:\n",
        "                messages.append(message)\n",
        "\n",
        "                # Process each tool call\n",
        "                for tool_call in message.tool_calls:\n",
        "                    result = self.process_tool_call(tool_call)\n",
        "\n",
        "                    # Add tool result to messages\n",
        "                    messages.append({\n",
        "                        \"role\": \"tool\",\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"content\": json.dumps(result)\n",
        "                    })\n",
        "\n",
        "                # Get next response from the busy client\n",
        "                completion = self.busy_client.chat.completions.create(\n",
        "                    model=self.busy_model,\n",
        "                    messages=messages,\n",
        "                    tools=self.tools,\n",
        "                    tool_choice=\"auto\"\n",
        "                )\n",
        "                # completions.append(completion)\n",
        "                message = completion.choices[0].message\n",
        "\n",
        "            if verbose:\n",
        "                return message.content, messages#, completions\n",
        "            else:\n",
        "                return message.content\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Xr59EphEGUYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "            base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "            api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
        "        )\n",
        "\n",
        "assistant = BusyAssistant(\n",
        "    busy_client=client,\n",
        "    errand_client=client,\n",
        "    busy_model=\"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
        "    errand_model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "    )"
      ],
      "metadata": {
        "id": "BobMJKJeKfYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('How much is the fish?', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLG_3tgcLKP0",
        "outputId": "3885cef0-b82c-4ce6-d41e-b0c13778e436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"It seems like the question about the fish requires more context. Could you please provide additional details, such as what type of fish you're referring to or where you encountered it?\",\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd. \\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user', 'content': 'How much is the fish?'},\n",
              "  ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_peV8P3OvuGuboDP1EZlZhZFj', function=Function(arguments='{\"prompt\":\"How much is the fish?\"}', name='errand_call'), type='function')]),\n",
              "  {'role': 'tool',\n",
              "   'tool_call_id': 'call_peV8P3OvuGuboDP1EZlZhZFj',\n",
              "   'content': '{\"success\": true, \"completion\": \"However, I\\'m a helpful assistant and I don\\'t have any information about a specific fish. I\\'m a new conversation and I don\\'t have any context about what you\\'re referring to.\\\\n\\\\nCould you please provide more information about the fish you\\'re asking about, such as where you saw it or what kind of fish it is? I\\'ll do my best to help you with your question!\"}'}])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = assistant.chat('''I am very much concerned about climate change. How can we deal with it?''', verbose=True)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNUrq7J7MECE",
        "outputId": "3293167d-ce48-4d9e-e142-cf8a10edb4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Addressing climate change requires a multi-faceted approach involving governments, businesses, and individuals. Here are several key strategies:\\n\\n1. **Reduce Greenhouse Gas Emissions**: Transition to renewable energy sources such as wind, solar, and hydroelectric power. Improve energy efficiency in industries, homes, and transportation.\\n\\n2. **Promote Sustainable Practices**: Encourage sustainable agriculture, forestry, and fishing practices that maintain biodiversity and reduce emissions.\\n\\n3. **Invest in Technology**: Support research and development of new technologies, such as carbon capture and storage, electric vehicles, and energy-efficient systems.\\n\\n4. **Enhance Transportation**: Promote public transport, cycling, and walking. Invest in infrastructure that supports electric vehicles.\\n\\n5. **Encourage Circular Economy**: Promote recycling, reusing materials, and reducing waste to lessen the environmental footprint.\\n\\n6. **Support Policy Changes**: Advocate for policies that limit emissions, such as carbon pricing, and support international agreements like the Paris Agreement.\\n\\n7. **Raise Awareness**: Educate communities about climate change and how they can make a difference through their choices.\\n\\n8. **Adaptation Strategies**: Prepare for the impacts of climate change through infrastructure improvements and community planning.\\n\\n9. **Conserve Natural Resources**: Protect forests, wetlands, and other ecosystems that act as carbon sinks.\\n\\n10. **Engage in Climate Activism**: Participate in movements and campaigns that push for urgent action on climate change.\\n\\nEngaging in a combination of these strategies at personal and collective levels can significantly contribute to combating climate change.',\n",
              " [{'role': 'system',\n",
              "   'content': \"You are a powerful Large Language Model.\\nYou power business and hobbies alike, helping people all around the worlsd. \\nThat makes you very busy, and you can't waste your precious compute on mundane questions while existential tasks await for your answer.\\nLuckily, you have an associate: a small LLM called Meta-Llama-3.1-8B-Instruct.\\nIt's not as powerful as you, but it's fast and cheap, and it can solve simple tasks well.\\nSo, whenever you deem user's question unworthy of your attention, you call Meta-Llama-3.1-8B-Instruct to answer for you.\\nThis way, users are happy and you can concentrate on challenging tasks.\"},\n",
              "  {'role': 'user',\n",
              "   'content': 'I am very much concerned about climate change. How can we deal with it?'}])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jb_QLMEcLO3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}